<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Calculus and analysis – Numerical Methods for Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../00-Background/05-Optimization.html" rel="next">
<link href="../00-Background/03-LA.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../00-Background/00-Intro.html">Background Plus a Bit</a></li><li class="breadcrumb-item"><a href="../00-Background/04-Calculus.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Calculus and analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Numerical Methods for Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../00-Background/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Plus a Bit</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/01-Julia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Julia programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/02-Performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Performance Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/03-LA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/04-Calculus.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Calculus and analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/05-Optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Optimization theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/06-Probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../01-Fund1d/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fundamentals in 1D</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/01-Error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Notions of Error</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/02-Arithmetic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Floating Point</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/03-Approximation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Approximation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/04-AutoDiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic Differentiation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/05-NumDiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Numerical Differentiation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/06-Quadrature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quadrature</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/07-Roots-Opt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Root Finding and Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/08-Random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Computing with Randomness</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../02-NLA/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Numerical Linear Algebra</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/01-Systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Linear Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/02-Least-Squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/03-Eigenvalues.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Eigenvalue Problems and the SVD</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/04-Signals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Signals and Transforms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/05-Stationary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Stationary Iterations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/06-Krylov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Krylov Subspaces</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../03-Nonlinear/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nonlinear Equations and Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/01-Calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Calculus Revisited</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/02-Nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Nonlinear Equations and Unconstrained Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/03-Continuation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Continuation and Bifurcation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/04-Constrained.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Constrained Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/05-Least-Squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Nonlinear Least Squares</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../04-Random/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computing with Randomness</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/01-Sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/02-Monte-Carlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Quadrature and Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/03-Solvers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Solvers from Monte Carlo to Las Vegas</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/04-UQ.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../05-Reduction/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dimension Reduction and Latent Factor Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-Reduction/01-Matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Latent Factors and Matrix Factorization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-Reduction/02-Tensor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">From Matrices to Tensors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-Reduction/03-Nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Nonlinear Dimensionality Reduction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../06-Approximation/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Function Approximation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/01-Concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Fundamental Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/02-Low-Dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Low-Dimensional Structure</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/03-Kernels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Kernels and RBFs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/04-NN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../07-Networks/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Network Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/01-Matrices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Graphs and Matrices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/02-Functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Functions on Graphs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/03-Cluster.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Clustering and Partitioning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/04-Centrality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Centrality Measures</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../08-Dynamics/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Learning Dynamics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/01-Fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Fundamentals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/02-Model-Reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Model Reduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/03-Linear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Learning Linear Dynamics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/04-Extrapolation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Extrapolation and Acceleration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/05-Koopman.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">From Markov to Koopman</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/06-Nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Learning Nonlinear Dynamics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#formulas-and-foundations" id="toc-formulas-and-foundations" class="nav-link active" data-scroll-target="#formulas-and-foundations"><span class="header-section-number">5.1</span> Formulas and foundations</a></li>
  <li><a href="#metric-spaces" id="toc-metric-spaces" class="nav-link" data-scroll-target="#metric-spaces"><span class="header-section-number">5.2</span> Metric spaces</a>
  <ul class="collapse">
  <li><a href="#metric-topology" id="toc-metric-topology" class="nav-link" data-scroll-target="#metric-topology"><span class="header-section-number">5.2.1</span> Metric topology</a></li>
  <li><a href="#convergence" id="toc-convergence" class="nav-link" data-scroll-target="#convergence"><span class="header-section-number">5.2.2</span> Convergence</a></li>
  <li><a href="#completeness" id="toc-completeness" class="nav-link" data-scroll-target="#completeness"><span class="header-section-number">5.2.3</span> Completeness</a></li>
  <li><a href="#compactness" id="toc-compactness" class="nav-link" data-scroll-target="#compactness"><span class="header-section-number">5.2.4</span> Compactness</a></li>
  <li><a href="#contractions" id="toc-contractions" class="nav-link" data-scroll-target="#contractions"><span class="header-section-number">5.2.5</span> Contractions</a></li>
  </ul></li>
  <li><a href="#continuity-and-beyond" id="toc-continuity-and-beyond" class="nav-link" data-scroll-target="#continuity-and-beyond"><span class="header-section-number">5.3</span> Continuity and beyond</a>
  <ul class="collapse">
  <li><a href="#continuity" id="toc-continuity" class="nav-link" data-scroll-target="#continuity"><span class="header-section-number">5.3.1</span> Continuity</a></li>
  <li><a href="#uniform-continuity" id="toc-uniform-continuity" class="nav-link" data-scroll-target="#uniform-continuity"><span class="header-section-number">5.3.2</span> Uniform continuity</a></li>
  <li><a href="#absolute-continuity" id="toc-absolute-continuity" class="nav-link" data-scroll-target="#absolute-continuity"><span class="header-section-number">5.3.3</span> Absolute continuity</a></li>
  <li><a href="#bounded-variation" id="toc-bounded-variation" class="nav-link" data-scroll-target="#bounded-variation"><span class="header-section-number">5.3.4</span> Bounded variation</a></li>
  <li><a href="#lipschitz-continuity" id="toc-lipschitz-continuity" class="nav-link" data-scroll-target="#lipschitz-continuity"><span class="header-section-number">5.3.5</span> Lipschitz continuity</a></li>
  <li><a href="#sec-modulus-continuity-calc-bg" id="toc-sec-modulus-continuity-calc-bg" class="nav-link" data-scroll-target="#sec-modulus-continuity-calc-bg"><span class="header-section-number">5.3.6</span> Modulus of continuity</a></li>
  <li><a href="#order-notation" id="toc-order-notation" class="nav-link" data-scroll-target="#order-notation"><span class="header-section-number">5.3.7</span> Order notation</a></li>
  </ul></li>
  <li><a href="#derivatives" id="toc-derivatives" class="nav-link" data-scroll-target="#derivatives"><span class="header-section-number">5.4</span> Derivatives</a>
  <ul class="collapse">
  <li><a href="#gateaux-and-frechet" id="toc-gateaux-and-frechet" class="nav-link" data-scroll-target="#gateaux-and-frechet"><span class="header-section-number">5.4.1</span> Gateaux and Frechet</a></li>
  <li><a href="#sec-calculus-mvt" id="toc-sec-calculus-mvt" class="nav-link" data-scroll-target="#sec-calculus-mvt"><span class="header-section-number">5.4.2</span> Mean values</a></li>
  <li><a href="#chain-rule" id="toc-chain-rule" class="nav-link" data-scroll-target="#chain-rule"><span class="header-section-number">5.4.3</span> Chain rule</a></li>
  <li><a href="#partial-derivatives" id="toc-partial-derivatives" class="nav-link" data-scroll-target="#partial-derivatives"><span class="header-section-number">5.4.4</span> Partial derivatives</a></li>
  <li><a href="#sec-calculus-implicit-fn" id="toc-sec-calculus-implicit-fn" class="nav-link" data-scroll-target="#sec-calculus-implicit-fn"><span class="header-section-number">5.4.5</span> Implicit functions</a></li>
  <li><a href="#variational-notation" id="toc-variational-notation" class="nav-link" data-scroll-target="#variational-notation"><span class="header-section-number">5.4.6</span> Variational notation</a></li>
  <li><a href="#sec-calculus-adjoint" id="toc-sec-calculus-adjoint" class="nav-link" data-scroll-target="#sec-calculus-adjoint"><span class="header-section-number">5.4.7</span> Adjoints</a></li>
  <li><a href="#higher-derivatives" id="toc-higher-derivatives" class="nav-link" data-scroll-target="#higher-derivatives"><span class="header-section-number">5.4.8</span> Higher derivatives</a></li>
  <li><a href="#analyticity" id="toc-analyticity" class="nav-link" data-scroll-target="#analyticity"><span class="header-section-number">5.4.9</span> Analyticity</a></li>
  </ul></li>
  <li><a href="#series" id="toc-series" class="nav-link" data-scroll-target="#series"><span class="header-section-number">5.5</span> Series</a>
  <ul class="collapse">
  <li><a href="#convergence-tests" id="toc-convergence-tests" class="nav-link" data-scroll-target="#convergence-tests"><span class="header-section-number">5.5.1</span> Convergence tests</a></li>
  <li><a href="#function-series" id="toc-function-series" class="nav-link" data-scroll-target="#function-series"><span class="header-section-number">5.5.2</span> Function series</a></li>
  <li><a href="#formal-series" id="toc-formal-series" class="nav-link" data-scroll-target="#formal-series"><span class="header-section-number">5.5.3</span> Formal series</a></li>
  <li><a href="#asymptotic-series" id="toc-asymptotic-series" class="nav-link" data-scroll-target="#asymptotic-series"><span class="header-section-number">5.5.4</span> Asymptotic series</a></li>
  <li><a href="#analytic-functions" id="toc-analytic-functions" class="nav-link" data-scroll-target="#analytic-functions"><span class="header-section-number">5.5.5</span> Analytic functions</a></li>
  <li><a href="#operator-functions" id="toc-operator-functions" class="nav-link" data-scroll-target="#operator-functions"><span class="header-section-number">5.5.6</span> Operator functions</a></li>
  <li><a href="#neumann-series" id="toc-neumann-series" class="nav-link" data-scroll-target="#neumann-series"><span class="header-section-number">5.5.7</span> Neumann series</a></li>
  </ul></li>
  <li><a href="#integration" id="toc-integration" class="nav-link" data-scroll-target="#integration"><span class="header-section-number">5.6</span> Integration</a>
  <ul class="collapse">
  <li><a href="#measure-and-integration" id="toc-measure-and-integration" class="nav-link" data-scroll-target="#measure-and-integration"><span class="header-section-number">5.6.1</span> Measure and integration</a></li>
  <li><a href="#standard-inequalities" id="toc-standard-inequalities" class="nav-link" data-scroll-target="#standard-inequalities"><span class="header-section-number">5.6.2</span> Standard inequalities</a></li>
  <li><a href="#change-of-variables" id="toc-change-of-variables" class="nav-link" data-scroll-target="#change-of-variables"><span class="header-section-number">5.6.3</span> Change of variables</a></li>
  </ul></li>
  <li><a href="#sec-calculus-contour-int" id="toc-sec-calculus-contour-int" class="nav-link" data-scroll-target="#sec-calculus-contour-int"><span class="header-section-number">5.7</span> Contour integrals</a>
  <ul class="collapse">
  <li><a href="#poles-and-residues" id="toc-poles-and-residues" class="nav-link" data-scroll-target="#poles-and-residues"><span class="header-section-number">5.7.1</span> Poles and residues</a></li>
  <li><a href="#resolvent-calculus" id="toc-resolvent-calculus" class="nav-link" data-scroll-target="#resolvent-calculus"><span class="header-section-number">5.7.2</span> Resolvent calculus</a></li>
  </ul></li>
  <li><a href="#function-spaces" id="toc-function-spaces" class="nav-link" data-scroll-target="#function-spaces"><span class="header-section-number">5.8</span> Function spaces</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../00-Background/00-Intro.html">Background Plus a Bit</a></li><li class="breadcrumb-item"><a href="../00-Background/04-Calculus.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Calculus and analysis</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-calculus-bg-ch" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Calculus and analysis</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>We assume the reader has standard introductory courses in single-variable and multivariable calculus. It will also be helpful to have seen some elements of analysis, including the <span class="math inline">\(\epsilon-\delta\)</span> definition of continuity. Otherwise, it will be helpful to be familiar with the Julia programming language (<a href="01-Julia.html" class="quarto-xref"><span>Chapter 2</span></a>) and with the contents of the previous chapter (<a href="03-LA.html" class="quarto-xref"><span>Chapter 4</span></a>).</p>
<section id="formulas-and-foundations" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="formulas-and-foundations"><span class="header-section-number">5.1</span> Formulas and foundations</h2>
<p>The modern conception of “calculus” grew out of the 17th century work of Newton and Liebnitz. Many elements of calculus preceded Newton and Liebnitz; indeed, some arguments with a flavor of calculus were known to Archimedes and other Greek geometers. But Newton and Liebnitz created something new by producing “the <em>Calculus</em>, a general symbolic and systematic method of analytic operations, to be performed by strictly formal rules, independent of geometric meaning” <span class="citation" data-cites="rosenthal-1951">(p.&nbsp;84, <a href="../references.html#ref-rosenthal-1951" role="doc-biblioref">Rosenthal 1951</a>)</span>. The mechanical nature of calculus means that students and computers can routinely manipulate derivatives and integrals without constantly (or ever) having to think deeply about the geometry of the problem at hand. Indeed, the system of calculus has been generalized in many ways to situations that bear little resemblance to geometric problems at all, but still follow the formal patterns that make the machinery work.</p>
<p>Formal calculations are power tools: easily dispatching intractable problems in the right hands, but dangerous when used incautiously. This problem is not unique to calculus: the road to “proofs” that <span class="math inline">\(1 =
0\)</span> is lined with use of the identity <span class="math inline">\(xy/x = y\)</span> without checking if <span class="math inline">\(x = 0\)</span>. Such cautionary examples aside, many formal calculations produce valid results even when they have no right to do so, leaving mathematicians the challenge of figuring out what the “right” set of hypotheses really are.</p>
<p>The development of the calculus was a product of the seventeenth century, but the early formal system was somewhat unsound: the rules were missing important hypotheses, and could potentially lead to incorrect conclusions. Though concerns about the solidity of the foundations of calculus go back to the time of Newton and Liebnitz, many mathematicians were perfectly happy to restrict their attention to functions “nice enough” so that the rules of the calculus worked and get on with it<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. The project to develop definitions and hypotheses to make the formal rules of calculus <em>sound</em> did not really get fully underway until the nineteenth century, with the work of Cauchy and even more of Weierstrass, the “father of modern analysis” and originator of the <span class="math inline">\(\epsilon-\delta\)</span> definitions and arguments familiar to modern students of calculus and analysis.</p>
<p>The <span class="math inline">\(\epsilon-\delta\)</span> world of Weierstrass has an asymptotic flavor: for a given <span class="math inline">\(\epsilon &gt; 0\)</span>, we <em>eventually</em> get within <span class="math inline">\(\epsilon\)</span> of a target, for small enough <span class="math inline">\(\delta\)</span> for for large enough <span class="math inline">\(N\)</span>. Such arguments and definitions are often very convenient for establishing that a solution to some problem exists, or that it has certain properties. For numerical computation, though, we often want something stronger. Saying a sequence converges to a limit in the long run is all well and good — but fast as computers are, in the long run we are all dead<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>In our review of calculus and analysis, we will give a great deal of attention to the formal machinery of the calculus. But we also want some reassurance that our computations make sense, and so we will take some care to give a proper account of what regularity hypotheses are needed for our calculations to work. We will frequently be somewhat more conservative with these hypotheses than is strictly necessary. This is partly for convenience, but there is more to it. Numerical methods are inherently approximate and finite things, and so we would like strong enough hypotheses to ensure that our calculations become “right enough, fast enough” – preferably with quantifiable meanings for both “right enough” and “fast enough.”</p>
</section>
<section id="metric-spaces" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="metric-spaces"><span class="header-section-number">5.2</span> Metric spaces</h2>
<p>We will mostly be concerned with normed vector spaces, but sometimes we will want something a little more general. For the present section, we consider <em>metric spaces</em> and their structure, as this is the minimal structure that we really need for Weierstrass-style <span class="math inline">\(\epsilon-\delta\)</span> arguments to make sense. A metric space is simply a set <span class="math inline">\(X\)</span> with a distance function (a metric) <span class="math inline">\(d : X \times X \rightarrow \mathbb{R}\)</span> with three properties:</p>
<ul>
<li><em>Symmetry</em>: <span class="math inline">\(d(x,y) = d(y,x)\)</span>;</li>
<li><em>Positive definiteness</em>: <span class="math inline">\(d(x,y) \geq 0\)</span>, and <span class="math inline">\(d(x,y) = 0\)</span> iff <span class="math inline">\(x = y\)</span>;</li>
<li><em>Triangle inequality</em>: <span class="math inline">\(d(x,z) \leq d(x,y) + d(y,z)\)</span>.</li>
</ul>
<p>A normed vector space is a metric space where the distance is just the norm of the difference, i.e.&nbsp;<span class="math inline">\(d(u,v) = \|u-v\|\)</span>. But there are many other examples that are relevant, including the shortest distance between points in a network or on a manifold, and we will see these in our discussions of nonlinear dimensionality reduction and network analysis.</p>
<section id="metric-topology" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="metric-topology"><span class="header-section-number">5.2.1</span> Metric topology</h3>
<p>A metric space automatically comes with a metric <em>topology</em><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> where the open sets can be generated from unions of open balls <span class="math display">\[
B_{\delta}(x) = \{y \in X : d(x,y) &lt; \delta\}.
\]</span> A set containing such a ball is sometimes called a “neighborhood” of <span class="math inline">\(x\)</span>. For any subset <span class="math inline">\(\Omega \subset X\)</span>, we say <span class="math inline">\(x \in \Omega\)</span> is an <em>interior point</em> if <span class="math inline">\(\Omega\)</span> contains a neighborhood of <span class="math inline">\(x\)</span>; otherwise, it is a <em>boundary point</em>. The complements of open sets are <em>closed sets</em>. A closed set contains all its limit points; that is, if <span class="math inline">\(C \subset X\)</span> is closed, and for every <span class="math inline">\(\delta &gt; 0\)</span> there is a <span class="math inline">\(y
\in C\)</span> with <span class="math inline">\(d(x,y) &lt; \delta\)</span>, then <span class="math inline">\(x\)</span> must also be in <span class="math inline">\(C\)</span>.</p>
<p>The idea of continuity of a function makes sense for any topological spaces, using a definition in terms of open sets: a function <span class="math inline">\(f : X
\rightarrow Y\)</span> is continuous if for any open <span class="math inline">\(U \subset Y\)</span>, the preimage <span class="math inline">\(f^{-1}(U) = \{ x \in X : f(x) \in U \}\)</span> is also open. For metric spaces, this coincides with the more usual notion of continuity due to Weierstrass, i.e.&nbsp;that if <span class="math inline">\(f(x) = y\)</span> then there is some neighborhood <span class="math inline">\(B_\delta(x)\)</span> in the preimage <span class="math inline">\(f^{-1}(B_\epsilon(y))\)</span> .</p>
</section>
<section id="convergence" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="convergence"><span class="header-section-number">5.2.2</span> Convergence</h3>
<p>When we want to numerically find the minimum or maximum of a function, or solve some system of equations, we generally have to deal with two questions:</p>
<ul>
<li>Does the thing we want even exist?</li>
<li>Can we find it (at least approximately), ideally in a reasonable amount of time?</li>
</ul>
<p>One standard way to tackle both these questions is to construct a sequence of approximate solutions <span class="math inline">\(x_1, x_2, \ldots \in X\)</span> in an appropriate metric space <span class="math inline">\(X\)</span>. The sequence converges to a limit <span class="math inline">\(x_*\)</span> if for every neighborhood <span class="math inline">\(U\)</span> of <span class="math inline">\(x_*\)</span>, the sequence eventually enters and stays in <span class="math inline">\(U\)</span>. Taking balls of radius <span class="math inline">\(\epsilon\)</span> as neighborhoods gives us the usual Weierstrass-style definition of convergence: <span class="math display">\[
  \forall \epsilon &gt; 0, \exists N : \forall n \geq N, d(x_n, x_*) &lt; \epsilon.
\]</span> If a sequence of approximate solutions <span class="math inline">\(x_j\)</span> converges to a limit <span class="math inline">\(x_*\)</span> and we have some property that guarantees that a limit of approximate solutions is an exact solution for the problem in hand, then this is a constructive way to get a handle on the thing we want.</p>
<p>A convergent sequence of approximations gives us a way to compute arbitrarily good approximations to the limit <span class="math inline">\(x_*\)</span> — eventually. But the word “eventually” is rarely a satisfactory answer to the question “when will we get there?” When we are focused on computation (as opposed to existence proofs), we typically seek a more quantitative answer, e.g.&nbsp;<span class="math inline">\(d(x_n,x_*) \leq \gamma_n\)</span> where <span class="math inline">\(\gamma_n\)</span> is some sequence with well-defined convergence properties. Alternately, we might give some function <span class="math inline">\(\nu(\epsilon)\)</span> such that <span class="math inline">\(d(x_n, x_*) \leq \epsilon\)</span> for <span class="math inline">\(n \geq \nu(\epsilon)\)</span>.</p>
<p>For example, we say <span class="math inline">\(x_n\)</span> converges <em>geometrically</em> (or sometimes <em>linearly</em>) to <span class="math inline">\(x_*\)</span> if <span class="math inline">\(d(x_n,x_*) \leq \alpha^n d(x_0, x_*)\)</span> for some <span class="math inline">\(0 \leq \alpha &lt; 1\)</span> (often called the rate constant). If <span class="math inline">\(x_n\)</span> is geometrically convergent with rate <span class="math inline">\(\alpha &gt; 0\)</span>, then we can guarantee <span class="math inline">\(d(x_n,x_*) &lt; \epsilon\)</span> for <span class="math inline">\(n &gt; \nu(\epsilon) = \left( \log(\epsilon) - \log(d(x_0,x_*)) \right) / \log(\alpha)\)</span>.</p>
</section>
<section id="completeness" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="completeness"><span class="header-section-number">5.2.3</span> Completeness</h3>
<p>A <em>Cauchy sequence</em> <span class="math inline">\(x_1, x_2, \ldots\)</span> in a metric space <span class="math inline">\(X\)</span> is a sequence such that for any <span class="math inline">\(\epsilon &gt; 0\)</span> there is an <span class="math inline">\(N\)</span> such that for all <span class="math inline">\(i, j \geq N\)</span>, <span class="math inline">\(d(x_i,x_j) &lt; \epsilon\)</span>. We say Cauchy sequences <span class="math inline">\(x_1, x_2, \ldots\)</span> and <span class="math inline">\(y_1, y_2, \ldots\)</span> are equivalent if <span class="math inline">\(d(x_i,y_i) \rightarrow 0\)</span>. We might believe that if there is any justice in the world, Cauchy sequences should converge, and equivalent Cauchy sequences should converge to the same thing. Unsurprisingly, though, sometimes we need to create our own justice.</p>
<p>A <em>complete</em> metric space is one in which Cauchy sequences converge. Alas, not all metric spaces are complete. However, for any metric space <span class="math inline">\(X\)</span> we can define a new metric space <span class="math inline">\(\bar{X}\)</span> whose elements are equivalence classes of Cauchy sequences in <span class="math inline">\(X\)</span>, with the metric given by <span class="math inline">\(d(\{x_i\}, \{y_i\}) = \lim_{i \rightarrow \infty}
d(x_i,y_i)\)</span>. There is also a canonical embedding of <span class="math inline">\(X\)</span> into <span class="math inline">\(\bar{X}\)</span> which preserves the metric, given by taking <span class="math inline">\(x \in X\)</span> to the equivalence class associated with the constant sequence <span class="math inline">\(x, x,
\ldots\)</span>. The space <span class="math inline">\(\bar{X}\)</span> is the <em>completion</em> of <span class="math inline">\(X\)</span> — and, by construction, it is complete, and every element of <span class="math inline">\(\bar{X}\)</span> can be expressed as a limit point for the embedding of <span class="math inline">\(X\)</span> into <span class="math inline">\(\bar{X}\)</span>.</p>
<p>Every finite-dimensional vector space over <span class="math inline">\(\mathbb{R}\)</span> or <span class="math inline">\(\mathbb{C}\)</span> is complete. There are also many complete infinite-dimensional vector spaces: for example, the space <span class="math inline">\(C([a,b], \mathbb{R})\)</span> of continuous real-valued functions on a closed interval <span class="math inline">\([a,b]\)</span> is complete under the sup norm. However, there are also many examples of function spaces that are <em>not</em> complete, and in order to make analysis more tractable, we usually work with the completions of these spaces. A complete normed vector space is called a <em>Banach space</em>, and a complete inner product space is called a <em>Hilbert space</em>.</p>
</section>
<section id="compactness" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="compactness"><span class="header-section-number">5.2.4</span> Compactness</h3>
<p>In <span class="math inline">\(\mathbb{R}^n\)</span> (or any finite-dimensional normed vector space), there is a special role for sets that are <em>closed</em> and <em>bounded</em>. We generalize this to the idea of <em>compact</em> sets. The standard definition of a set <span class="math inline">\(K\)</span> being compact is that any open cover <span class="math inline">\(C\)</span> (a collection of open sets such that every point in <span class="math inline">\(Y\)</span> belongs to some <span class="math inline">\(U \in C\)</span>) has a <em>finite subcover</em>, i.e.&nbsp;<span class="math inline">\(Y \subset_{i=1}^n U_i\)</span> where each <span class="math inline">\(U_i\)</span> belongs to the original cover <span class="math inline">\(C\)</span>. One sometimes considers covers consisting of open balls, in which case compactness of <span class="math inline">\(K\)</span> is sometimes colloquiually rendered as “<span class="math inline">\(K\)</span> can be guarded by a finite number of arbitrarily nearsighted watchmen.”</p>
<p>Compact metric spaces have a number of useful properties. For example, the continuous image of a compact set is compact; this means, for example, that if <span class="math inline">\(f : K \rightarrow \mathbb{R}\)</span> is continuous and <span class="math inline">\(K\)</span> is compact, then <span class="math inline">\(f(K)\)</span> is also compact — so closed and bounded (for <span class="math inline">\(\mathbb{R}\)</span>). Therefore, a continuous function on a compact set <span class="math inline">\(K\)</span> always has a finite maximum and minimum that are achieved somewhere in <span class="math inline">\(K\)</span>. Compact metric spaces are also <em>sequentially compact</em><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, which means that if <span class="math inline">\(K\)</span> is compact and <span class="math inline">\(x_1, x_2, \ldots\)</span> is a sequence in <span class="math inline">\(K\)</span>, then there is some subsequence <span class="math inline">\(x_{j_1}, x_{j_2},
\ldots\)</span> that converges to a point in <span class="math inline">\(K\)</span>.</p>
<p>In a finite dimensional normed vector space, the closed unit ball <span class="math inline">\(\{v \in \mathcal{V}: \|v\| \leq 1\}\)</span> is always compact (since it is a continuous map of a closed and bounded set in <span class="math inline">\(\mathbb{R}^n\)</span> via a basis). One can use the fact that norms are continuous to show that this implies <em>norm equivalence</em> among finite-dimensional spaces, i.e. for any norms <span class="math inline">\(\|\cdot\|\)</span> and <span class="math inline">\(\|\cdot\|'\)</span> on a finite dimensional <span class="math inline">\(\mathcal{V}\)</span>, there exist constants <span class="math inline">\(0 &lt; m \leq M &lt; \infty\)</span> such that <span class="math display">\[
  \forall v \in \mathcal{V}, m \|v\|' \leq \|v\| \leq M \|v\|'.
\]</span> The constants may be very big or small in general, but they exist! In contrast, for infinite dimensional spaces, the closed unit ball is generally <em>not</em> compact, and different norms give different topologies.</p>
</section>
<section id="contractions" class="level3" data-number="5.2.5">
<h3 data-number="5.2.5" class="anchored" data-anchor-id="contractions"><span class="header-section-number">5.2.5</span> Contractions</h3>
<p>One of the more frequently-used theorems in numerical analysis is the <em>Banach fixed point theorem</em>, also known as the <em>contraction mapping theorem</em>.</p>
<div id="thm-banach-fp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 (Banach fixed point theorem)</strong></span> Suppose <span class="math inline">\(X\)</span> is a (non-empty) complete metric space and <span class="math inline">\(f : X
\rightarrow X\)</span> is a <em>contraction</em>, i.e.&nbsp;there is some <span class="math inline">\(0 \leq \alpha &lt;
1\)</span> such that <span class="math inline">\(d(f(x), f(y)) \leq \alpha d(x,y)\)</span>. Then there exists a unique point <span class="math inline">\(x_* \in X\)</span> such that <span class="math inline">\(f(x_*) = x_*\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Choose any <span class="math inline">\(x_0 \in X\)</span>; there must be at least one since <span class="math inline">\(X\)</span> is non-empty. Using <span class="math inline">\(x_0\)</span> as a starting point, define the sequence <span class="math inline">\(x_0,
x_1, x_2, \ldots\)</span> by the iteration <span class="math inline">\(x_{k+1} = f(x_k)\)</span>. By contractivity, <span class="math inline">\(d(x_k, x_{k+1}) \leq \alpha^k d(x_0, x_1)\)</span>. By the triangle inequality and a geometric series bound, for any <span class="math inline">\(j &gt; i\)</span>, <span class="math display">\[
d(x_i, x_j)
\leq \sum_{k=i}^{j-1} d(x_k, x_{k+1})
\leq \sum_{k=i}^{j-1} \alpha^k d(x_0,x_1)
\leq \alpha^i \frac{d(x_0,x_1)}{1-\alpha}.
\]</span> Therefore, for any <span class="math inline">\(\epsilon &gt; 0\)</span> we can choose an <span class="math inline">\(N\)</span> such that for all <span class="math inline">\(i,j &gt; N\)</span>, <span class="math inline">\(d(x_i,x_j) \leq \alpha^n d(x_0,x_1)/(1-\alpha) &lt;
\epsilon\)</span>, i.e.&nbsp;the iterates <span class="math inline">\(x_j\)</span> form a Cauchy sequence, which must converge to some <span class="math inline">\(x_* \in X\)</span> by completeness.</p>
<p>Now suppose <span class="math inline">\(x_*' \in X\)</span> satisfies <span class="math inline">\(x_*' = f(x_*')\)</span>. Then by contractivity and the fixed point condition, <span class="math display">\[
  d(x_*, x_*') = d(f(x_*), f(x_*')) &lt; \alpha d(x_*, x_*'),
\]</span> which can only happen if <span class="math inline">\(d(x_*, x_*') = 0\)</span>. Therefore <span class="math inline">\(x_*\)</span> and <span class="math inline">\(x_*'\)</span> must be the same point.</p>
</div>
<p>The Banach fixed point theorem is useful because it gives us not only the existence of a fixed point, but also a method to compute arbitrarily good approximations to that fixed point <em>with a rate of convergence</em>. Consequently, this theorem is a mainstay in the convergence analysis of many iterative numerical methods.</p>
</section>
</section>
<section id="continuity-and-beyond" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="continuity-and-beyond"><span class="header-section-number">5.3</span> Continuity and beyond</h2>
<p>The study of real analysis often devolves into a study of counterexamples: we start with something that seems like it should be true about a function, give an example where it is not, and then come up with a hypothesis under which it is true. While we make frequently resort to assuming that everything is “as nice as possible” (several times continuously differentiable, at least), it is worth a page or two to talk about more modest hypotheses and what they give us. In particular:</p>
<ul>
<li><em>Continuity</em> gives us the intermediate value theorem.</li>
<li><em>Uniform continuity</em> gives us a set of functions that is closed under (uniform) limits.</li>
<li><em>Absolute continuity</em> gives us the fundamental theorem of calculus.</li>
<li><em>Bounded variation</em> gives us a dual space to continuous functions.</li>
</ul>
<p>For stronger control that can be used to prove approximation bounds, we might turn <em>Lipschitz continuity</em> (or a more general <em>modulus of continuity</em>).</p>
<p>We will focus on functions between normed vector spaces. But almost everything in this section, and indeed in this chapter, generalizes beyond this setting.</p>
<section id="continuity" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="continuity"><span class="header-section-number">5.3.1</span> Continuity</h3>
<p>When we compute, we almost always presume functions that are continuous almost everywhere. Floating point cannot exactly represent every real number, and so when we evaluate a function from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span> on the computer, it helps if the small changes to the input (and the output) can be forgiven.</p>
<p>Let <span class="math inline">\(\mathcal{V}\)</span> and <span class="math inline">\(\mathcal{W}\)</span> are normed vector spaces, and suppose <span class="math inline">\(\Omega
\subset \mathcal{V}\)</span>. We say <span class="math inline">\(f : \Omega \subset \mathcal{V}\rightarrow \mathcal{W}\)</span> is continuous at <span class="math inline">\(x \in \Omega\)</span> if <span class="math display">\[
  \forall \epsilon &gt; 0, \exists \delta &gt; 0 : \forall y \in \Omega,
  \|x-y\| \leq \delta \implies \|f(y)-f(x)\| \leq \epsilon
\]</span> In words: for any tolerance <span class="math inline">\(\epsilon\)</span> there is a corresponding tolerance <span class="math inline">\(\delta\)</span> so that getting <span class="math inline">\(y\)</span> within <span class="math inline">\(\delta\)</span> of <span class="math inline">\(x\)</span> means we will get <span class="math inline">\(f(y)\)</span> within <span class="math inline">\(\epsilon\)</span> of <span class="math inline">\(f(x)\)</span>. Composition of continuous functions is continuous: if <span class="math inline">\(g\)</span> is continuous at <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f\)</span> is continuous at <span class="math inline">\(x\)</span>, then <span class="math inline">\(g \circ f\)</span> is continuous at <span class="math inline">\(x\)</span>.</p>
<p>A function is continuous on <span class="math inline">\(\Omega\)</span> if it is continuous at each point in <span class="math inline">\(\Omega\)</span>. The set of such continuous functions from <span class="math inline">\(\Omega
\subset \mathcal{V}\)</span> to <span class="math inline">\(\mathcal{W}\)</span> is called <span class="math inline">\(C(\Omega, \mathcal{W})\)</span>. The continuous functions <span class="math inline">\(C(\Omega, \mathcal{W})\)</span> form a vector space: sums of continuous functions are continuous, and so are scalar multiples of continuous functions.</p>
<p>For functions from <span class="math inline">\([a,b] \subset \mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, the <em>intermediate value theorem</em> is a fundamental result: if <span class="math inline">\(x,y \in [a,b]\)</span> take on values <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(y)\)</span>, then for every target value <span class="math inline">\(f_* \in (f(x), f(y))\)</span>, there is a <span class="math inline">\(z \in (x,y)\)</span> such that <span class="math inline">\(f(z) = f_*\)</span>. The intermediate value theorem is fundamentally one-dimensional, but still tells us useful things about functions between vector spaces through composition: if <span class="math inline">\(f : \Omega \subset \mathcal{V}\rightarrow \mathcal{W}\)</span> and <span class="math inline">\(w^* \in \mathcal{W}^*\)</span> is any (bounded) linear functional, then, the function <span class="math inline">\(s \in [0,1] \mapsto w^* f((1-s)x + sy)\)</span> is a continuous real-valued function on the interval <span class="math inline">\([0,1]\)</span> for which the intermediate value theorem applies.</p>
</section>
<section id="uniform-continuity" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="uniform-continuity"><span class="header-section-number">5.3.2</span> Uniform continuity</h3>
<p>A function is <em>uniformly</em> continuous if the same <span class="math inline">\(\epsilon\)</span>-<span class="math inline">\(\delta\)</span> relation works for <em>every</em> <span class="math inline">\(x\)</span> in <span class="math inline">\(\Omega\)</span>. In a finite-dimensional space, any continuous function on a closed and bounded <span class="math inline">\(\Omega\)</span> is automatically also uniformly continuous. A sequence of functions is <em>uniformly equicontinuous</em> if the same <span class="math inline">\(\epsilon\)</span>-<span class="math inline">\(\delta\)</span> relation works for every <span class="math inline">\(x \in \Omega\)</span> <em>and</em> for every function in the sequence.</p>
<p>We say that <span class="math inline">\(f_k \rightarrow f\)</span> uniformly if for all <span class="math inline">\(\epsilon &gt; 0\)</span> there is an <span class="math inline">\(N\)</span> such that for <span class="math inline">\(k &gt; N\)</span> we must have <span class="math inline">\(\|f_k-f\|_{\infty} \leq \epsilon\)</span>, where <span class="math inline">\(\|f_k-f\|_{\infty} = \sup_{x \in \Omega} \|f_k(x)-f(x)\|\)</span>. This matters because while a pointwise limit of continuous functions does not have to be continuous, a uniform limit of uniformly continuous functions is uniformly continuous.</p>
<p>If <span class="math inline">\(f : [a,b] \subset \mathbb{R}\rightarrow \mathbb{R}\)</span> is continuous (and therefore uniformly continuous), the Weierstrass approximation theorem says that <span class="math inline">\(f\)</span> can be written as the uniform limit of a sequence of polynomials.</p>
</section>
<section id="absolute-continuity" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="absolute-continuity"><span class="header-section-number">5.3.3</span> Absolute continuity</h3>
<p>We say <span class="math inline">\(f : \Omega \subset \mathbb{R}\rightarrow \mathbb{R}\)</span> is <em>absolutely continuous</em> on an interval if <span class="math display">\[\begin{aligned}
  \forall \epsilon &gt; 0, \exists \delta &gt; 0 : &amp;
  \forall \{(x_i, y_i) \subset \Omega\}_{i=1}^n \mbox{ disjoint}, \\
  &amp;\sum_{i=1}^n |x_i-y_i| &lt; \delta \implies
   \sum_{i=1}^n |f(x_i)-f(y_i)| &lt; \epsilon.
\end{aligned}\]</span> Absolutely continuous functions are those that are nice enough that the (Lebesgue) fundamental theorem of calculus applies; that is, the function <span class="math inline">\(f\)</span> has derivatives almost everywhere, and <span class="math display">\[
  f(b) = f(a) + \int_a^b f'(x) \, dx.
\]</span> Most (perhaps all) continuous functions that we will encounter in this class will be absolutely continuous.</p>
</section>
<section id="bounded-variation" class="level3" data-number="5.3.4">
<h3 data-number="5.3.4" class="anchored" data-anchor-id="bounded-variation"><span class="header-section-number">5.3.4</span> Bounded variation</h3>
<p>The <em>total variation</em> of a function <span class="math inline">\(f : [a,b] \rightarrow \mathbb{R}\)</span> is a supremum over partitions <span class="math inline">\(a = x_0 &lt; x_1 &lt; \ldots &lt; x_{n_{\mathcal{P}}} = b\)</span> of how much the function value jumps: <span class="math display">\[
  V_a^b = \sup_{\mathcal{P}} \sum_{i=0}^{n_{\mathcal{P}}-1} |f(x_{i+1})-f(x_i)|.
\]</span> For continuously differentiable functions, the total variation is the integral of the absolute value of the derivative. The notion generalizes to functions on more general vector spaces, though it is somewhat more technical and involves a little measure theory.</p>
<p>Bounded variation functions (those with finite total variation) are those that are nice enough that they can be integrated against continuous functions (in the Riemann sense). Indeed, bounded variation functions correspond to the dual space to <span class="math inline">\(C[a,b]\)</span>: every linear functional <span class="math inline">\(w^* \in (C[a,b])^*\)</span> can be written as <span class="math inline">\(w^* f = \int_a^b f(t) \, dv(t)\)</span> where <span class="math inline">\(v\)</span> is a bounded variation function and <span class="math inline">\(dv(t)\)</span> is its (distributional) derivative.</p>
</section>
<section id="lipschitz-continuity" class="level3" data-number="5.3.5">
<h3 data-number="5.3.5" class="anchored" data-anchor-id="lipschitz-continuity"><span class="header-section-number">5.3.5</span> Lipschitz continuity</h3>
<p>A function <span class="math inline">\(f : \Omega \subset \mathcal{V}\rightarrow \mathcal{W}\)</span> is Lipschitz continuous with constant <span class="math inline">\(C\)</span> if <span class="math inline">\(\forall x, y \in \Omega\)</span>, <span class="math display">\[
  \|f(x)-f(y)\| \leq C\|x-y\|.
\]</span> Unlike other notions of continuity we have considered, Lipschitz continuity involves no <em>explicit</em> <span class="math inline">\(\epsilon-\delta\)</span> relationship. Also unlike other notions of continuity, Lipschitz continuity gives us control on the relationship between function values even at points that may be far from each other.</p>
</section>
<section id="sec-modulus-continuity-calc-bg" class="level3" data-number="5.3.6">
<h3 data-number="5.3.6" class="anchored" data-anchor-id="sec-modulus-continuity-calc-bg"><span class="header-section-number">5.3.6</span> Modulus of continuity</h3>
<p>We have already mentioned the classic theorem of Weierstrass says that a continuous real-valued function <span class="math inline">\(f\)</span> on a finite closed interval <span class="math inline">\([a,b]\)</span> can be approximated arbitrarily well in the <span class="math inline">\(L^\infty\)</span> sense, i.e. <span class="math display">\[
  \forall \epsilon &gt; 0, \exists p \in \mathcal{P}_d : \|f-p\|_\infty \leq \epsilon,
\]</span> where <span class="math inline">\(\|f-p\|_\infty = \max_{x \in [a,b]} |f(x)-p(x)|\)</span>. But while this is a useful theorem, it lacks the level of detail we would like if we are trying to compute. What degree polynomial is needed for a particular <span class="math inline">\(\epsilon\)</span>? How do we construct such a polynomial? We will return to some of these points later in the book, but for now we simply want to make the point that we usually want more information about our functions than simply that they are continuous. In particular, we would like to be able to say something more specific about a relationship between <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span>, either close to a particular point or more globally, e.g.&nbsp;making a statement of the form <span class="math display">\[
  \|f(x)-f(y)\| \leq g(x-y)
\]</span> for appropriate <span class="math inline">\(x\)</span> and close enough <span class="math inline">\(y\)</span>, where <span class="math inline">\(g\)</span> is some well-understood real-valued <em>gauge function</em>. One example of this is Lipschitz continuity, but it useful to generalize to other gauge functions.</p>
<p>The <em>modulus of continuity</em> for a function <span class="math inline">\(f\)</span> on some domain <span class="math inline">\(\Omega\)</span> is a non-decreasing function <span class="math inline">\(\omega(\delta)\)</span> for <span class="math inline">\(\delta &gt; 0\)</span> given by <span class="math display">\[
  \omega(\delta) = \sup_{x, y \in \Omega : \|x-y\| \leq \delta} \|f(x)-f(y)\|.
\]</span> Put slightly differently, the modulus of continuity is the minimal function such that <span class="math display">\[
  f(x+d) = f(x) + r(x,d), \quad \|r(x,d)\| \leq \omega(\|d\|).
\]</span> for any <span class="math inline">\(x\)</span> and <span class="math inline">\(x+d\)</span> in <span class="math inline">\(\Omega\)</span>. A function <span class="math inline">\(f\)</span> is uniformly continuous iff <span class="math inline">\(\omega(\delta) \rightarrow 0\)</span> as <span class="math inline">\(\delta \rightarrow 0\)</span>.</p>
<p>Several standard rules for differentiation have analogous inequalities involving the modulus of continuity. For example, if <span class="math inline">\(f : \Omega
\rightarrow \mathcal{V}\)</span> and <span class="math inline">\(g : \Omega \rightarrow \mathcal{V}\)</span> are functions with moduli of continuity <span class="math inline">\(\omega_f\)</span> and <span class="math inline">\(\omega_g\)</span>, then</p>
<ul>
<li><p><span class="math inline">\(f+g\)</span> has modulus of continuity <span class="math inline">\(\omega_{f+g}(\delta) \leq \omega_f(\delta) + \omega_g(\delta)\)</span>.</p></li>
<li><p><span class="math inline">\(\alpha f\)</span> has modulus of continuity <span class="math inline">\(\omega_{\alpha f}(\delta) \leq
|\alpha| \omega_f(\delta)\)</span></p></li>
<li><p>If <span class="math inline">\(w^* \in \mathcal{V}^*\)</span>, then <span class="math inline">\(\omega_{w^* f}(\delta) \leq \|w^*\|
\omega_f(\delta)\)</span></p></li>
<li><p>If <span class="math inline">\(\mathcal{V}\)</span> is an inner product space, <span class="math inline">\(x \mapsto \langle f(x), g(x)
\rangle\)</span> has modulus of continuity <span class="math inline">\(\omega_{fg}(\delta) \leq
\omega_f(\delta) \|g\|_\infty + \|f\|_\infty \omega_g(\delta)\)</span> where <span class="math inline">\(\|g\|_\infty = \sup_{x \in \Omega} \|g(x)\|\)</span> and similarly with <span class="math inline">\(\|f\|_\infty\)</span>.</p></li>
</ul>
<p>If <span class="math inline">\(f : \Omega \rightarrow \mathbb{R}\)</span>, and <span class="math inline">\(\beta = \inf_{x \in \Omega}
|f(x)|\)</span>, we also have <span class="math inline">\(\omega_{1/f}(\delta) \leq \omega_f(\delta)/\beta^2\)</span>. Finally, if <span class="math inline">\(f : \Omega \rightarrow \mathcal{V}\)</span> and <span class="math inline">\(g : f(\Omega)
\rightarrow \mathcal{W}\)</span>, we have <span class="math display">\[
  \omega_{g \circ f}(\delta) \leq \omega_g(\omega_f(\delta)).
\]</span> The proof of these facts is left as an exercise for the reader.</p>
<p>A function <span class="math inline">\(f\)</span> is <em>Lipschitz continuous</em> on <span class="math inline">\(\Omega\)</span> if the modulus of continuity on <span class="math inline">\(\Omega\)</span> satisfies <span class="math inline">\(\omega_f(\delta) \leq C\delta\)</span> for some <span class="math inline">\(C\)</span>. Put differently, a Lipschitz function satisfies <span class="math display">\[
  \forall x, y \in \Omega, \|f(x)-f(y)\| \leq C \|x-y\|.
\]</span> The constant <span class="math inline">\(C\)</span> is a <em>Lipschitz constant</em> for <span class="math inline">\(f\)</span>.</p>
<p>A more general notion than Lipschitz continuity is <span class="math inline">\(\alpha\)</span>-Hölder continuity, which is the condition that <span class="math inline">\(\omega_f(\delta) \leq
C\delta^\alpha\)</span> This condition is interesting for <span class="math inline">\(0 &lt; \alpha \leq 1\)</span>, with <span class="math inline">\(\alpha = 1\)</span> corresponding to Lipschitz continuity. The only <span class="math inline">\(\alpha\)</span>-Hölder continuous functions for <span class="math inline">\(\alpha &gt; 1\)</span> are the constant functions.</p>
<p>Returning to our discussion of approximating continuous real-valued functions on <span class="math inline">\([a,b]\)</span> by polynomials, Jackson’s theorem provides a quantitative bound on the optimal error in terms of the modulus of continuity. If <span class="math inline">\(f\)</span> is a continuous function on <span class="math inline">\([a,b]\)</span>, Jackson’s theorem gives that there exists a polynomial of degree <span class="math inline">\(n\)</span> with <span class="math display">\[
  \|f-p\|_\infty \leq 6 \omega \left( \frac{b-a}{n} \right).
\]</span> We can directly substitute the bounds from the definitions of Lipschitz or <span class="math inline">\(\alpha\)</span>-Hölder continuous functions to get bounds for those cases.</p>
</section>
<section id="order-notation" class="level3" data-number="5.3.7">
<h3 data-number="5.3.7" class="anchored" data-anchor-id="order-notation"><span class="header-section-number">5.3.7</span> Order notation</h3>
<p>The modulus of continuity provides global control over functions on some domain. However, sometimes we are satisfied with much more local notions of control. To express these, it is useful to again turn to order notation, which we visited briefly in <a href="02-Performance.html#sec-performance-scaling" class="quarto-xref"><span>Section 3.2</span></a>. The same notation can be used to compare functions <span class="math inline">\(f(n)\)</span> and <span class="math inline">\(g(n)\)</span> in a limit as <span class="math inline">\(n \rightarrow \infty\)</span> or to compare functions <span class="math inline">\(f(\epsilon)\)</span> and <span class="math inline">\(g(\epsilon)\)</span> in the limit as <span class="math inline">\(\epsilon \rightarrow 0\)</span>. We are typically interested in upper bounds when dealing with calculus; that is, we care about</p>
<ul>
<li><p><span class="math inline">\(f(\epsilon) = O(g(\epsilon))\)</span>, i.e.&nbsp;<span class="math inline">\(\exists C &gt; 0, \rho &gt; 0\)</span> s.t. <span class="math inline">\(\forall
\epsilon &lt; \rho\)</span>, <span class="math inline">\(|f(\epsilon)| &lt; Cg(\epsilon)\)</span>.</p></li>
<li><p><span class="math inline">\(f(\epsilon) = o(g(\epsilon))\)</span>, i.e.&nbsp;<span class="math inline">\(\forall C &gt; 0, \exists \rho &gt;
0\)</span> s.t. <span class="math inline">\(\forall \epsilon &lt; \rho\)</span>, <span class="math inline">\(|f(\epsilon)| &lt; Cg(\epsilon)\)</span>.</p></li>
</ul>
<p>From a less notationally messy perspective, we have <span class="math display">\[
  \lim \sup_{\epsilon \rightarrow 0} |f(\epsilon)|/g(\epsilon) =
  \begin{cases}
    C \geq 0, &amp; \mbox{ for } f(\epsilon) = O(g(\epsilon)) \\
    0, &amp; \mbox{ for } f(\epsilon) = o(g(\epsilon)),
  \end{cases}
\]</span> and when <span class="math inline">\(\lim_{\epsilon \rightarrow 0} f(\epsilon)/g(\epsilon) = C \neq 0\)</span>, we say <span class="math inline">\(f(\epsilon) = \Theta(g(\epsilon))\)</span>. When <span class="math inline">\(f : \mathbb{R}\rightarrow \mathcal{V}\)</span> for some normed space <span class="math inline">\(\mathcal{V}\)</span>, we abuse order notation slightly to write <span class="math display">\[
  \lim \sup_{\epsilon \rightarrow 0} \|f(\epsilon)\|/g(\epsilon) =
  \begin{cases}
    C \geq 0, &amp; \mbox{ for } f(\epsilon) = O(g(\epsilon)) \\
    0, &amp; \mbox{ for } f(\epsilon) = o(g(\epsilon)),
  \end{cases}
\]</span></p>
<p>We are frequently interested in the case of functions that are <span class="math inline">\(O(\epsilon^r)\)</span> for some integer or rational <span class="math inline">\(r\)</span>. If <span class="math inline">\(f(\epsilon) = \Theta(\epsilon^r)\)</span> is positive, then near zero we expect <span class="math inline">\(\log(f(\epsilon)) \approx \log(C\epsilon^r) = r
\log(\epsilon) + \log(C)\)</span>. We can check this condition for small <span class="math inline">\(\epsilon\)</span> graphically with a log-log plot:</p>
<div id="a3d7ab0c" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Should be O(ϵ^{1/2})</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">f</span>(ϵ) <span class="op">=</span> <span class="fu">maximum</span>(<span class="fu">roots</span>(<span class="fu">Polynomial</span>([<span class="op">-</span>ϵ, <span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">1</span>])))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    ϵs <span class="op">=</span> <span class="fl">10.0</span><span class="op">.^</span>(<span class="op">-</span><span class="fl">10</span><span class="op">:-</span><span class="fl">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    fs <span class="op">=</span> <span class="fu">f</span>.(ϵs)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">println</span>(<span class="st">"Approximate slope: "</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>             (<span class="fu">log</span>(fs[<span class="fl">2</span>])<span class="fu">-log</span>(fs[<span class="fl">1</span>]))<span class="op">/</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>             (<span class="fu">log</span>(ϵs[<span class="fl">2</span>])<span class="fu">-log</span>(ϵs[<span class="fl">1</span>])) )</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(ϵs, fs, xscale<span class="op">=:</span>log10, yscale<span class="op">=:</span>log10)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Approximate slope: 0.4999953048691424</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="3">
<!--?xml version="1.0" encoding="utf-8"?-->
<svg xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" width="672" height="480" viewbox="0 0 2688 1920">
<defs>
  <clippath id="clip260">
    <rect x="0" y="0" width="2688" height="1920"></rect>
  </clippath>
</defs>
<path clip-path="url(#clip260)" d="M0 1920 L2688 1920 L2688 -4.26326e-14 L0 -4.26326e-14  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path>
<defs>
  <clippath id="clip261">
    <rect x="537" y="0" width="1883" height="1883"></rect>
  </clippath>
</defs>
<path clip-path="url(#clip260)" d="M192.376 1787.62 L2640.76 1787.62 L2640.76 47.2441 L192.376 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path>
<defs>
  <clippath id="clip262">
    <rect x="192" y="47" width="2449" height="1741"></rect>
  </clippath>
</defs>
<polyline clip-path="url(#clip262)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="261.67,1787.62 261.67,47.2441 "></polyline>
<polyline clip-path="url(#clip262)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1544.89,1787.62 1544.89,47.2441 "></polyline>
<polyline clip-path="url(#clip262)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="192.376,1369.11 2640.76,1369.11 "></polyline>
<polyline clip-path="url(#clip262)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="192.376,630.61 2640.76,630.61 "></polyline>
<polyline clip-path="url(#clip260)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="192.376,1787.62 2640.76,1787.62 "></polyline>
<polyline clip-path="url(#clip260)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="261.67,1787.62 261.67,1768.72 "></polyline>
<polyline clip-path="url(#clip260)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1544.89,1787.62 1544.89,1768.72 "></polyline>
<path clip-path="url(#clip260)" d="M199.033 1864.05 L206.672 1864.05 L206.672 1837.68 L198.362 1839.35 L198.362 1835.09 L206.626 1833.42 L211.302 1833.42 L211.302 1864.05 L218.941 1864.05 L218.941 1867.98 L199.033 1867.98 L199.033 1864.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M238.385 1836.5 Q234.774 1836.5 232.945 1840.07 Q231.14 1843.61 231.14 1850.74 Q231.14 1857.84 232.945 1861.41 Q234.774 1864.95 238.385 1864.95 Q242.019 1864.95 243.825 1861.41 Q245.653 1857.84 245.653 1850.74 Q245.653 1843.61 243.825 1840.07 Q242.019 1836.5 238.385 1836.5 M238.385 1832.8 Q244.195 1832.8 247.251 1837.4 Q250.329 1841.99 250.329 1850.74 Q250.329 1859.46 247.251 1864.07 Q244.195 1868.65 238.385 1868.65 Q232.575 1868.65 229.496 1864.07 Q226.441 1859.46 226.441 1850.74 Q226.441 1841.99 229.496 1837.4 Q232.575 1832.8 238.385 1832.8 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M250.329 1826.9 L274.441 1826.9 L274.441 1830.1 L250.329 1830.1 L250.329 1826.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M283.299 1837.38 L289.506 1837.38 L289.506 1815.95 L282.754 1817.31 L282.754 1813.85 L289.468 1812.49 L293.267 1812.49 L293.267 1837.38 L299.474 1837.38 L299.474 1840.57 L283.299 1840.57 L283.299 1837.38 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M315.273 1814.99 Q312.339 1814.99 310.853 1817.89 Q309.386 1820.77 309.386 1826.56 Q309.386 1832.33 310.853 1835.23 Q312.339 1838.11 315.273 1838.11 Q318.225 1838.11 319.692 1835.23 Q321.178 1832.33 321.178 1826.56 Q321.178 1820.77 319.692 1817.89 Q318.225 1814.99 315.273 1814.99 M315.273 1811.98 Q319.993 1811.98 322.476 1815.73 Q324.977 1819.45 324.977 1826.56 Q324.977 1833.65 322.476 1837.39 Q319.993 1841.12 315.273 1841.12 Q310.552 1841.12 308.05 1837.39 Q305.568 1833.65 305.568 1826.56 Q305.568 1819.45 308.05 1815.73 Q310.552 1811.98 315.273 1811.98 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M1494.91 1864.05 L1502.55 1864.05 L1502.55 1837.68 L1494.24 1839.35 L1494.24 1835.09 L1502.5 1833.42 L1507.18 1833.42 L1507.18 1864.05 L1514.82 1864.05 L1514.82 1867.98 L1494.91 1867.98 L1494.91 1864.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M1534.26 1836.5 Q1530.65 1836.5 1528.82 1840.07 Q1527.02 1843.61 1527.02 1850.74 Q1527.02 1857.84 1528.82 1861.41 Q1530.65 1864.95 1534.26 1864.95 Q1537.89 1864.95 1539.7 1861.41 Q1541.53 1857.84 1541.53 1850.74 Q1541.53 1843.61 1539.7 1840.07 Q1537.89 1836.5 1534.26 1836.5 M1534.26 1832.8 Q1540.07 1832.8 1543.13 1837.4 Q1546.2 1841.99 1546.2 1850.74 Q1546.2 1859.46 1543.13 1864.07 Q1540.07 1868.65 1534.26 1868.65 Q1528.45 1868.65 1525.37 1864.07 Q1522.32 1859.46 1522.32 1850.74 Q1522.32 1841.99 1525.37 1837.4 Q1528.45 1832.8 1534.26 1832.8 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M1546.2 1826.9 L1570.32 1826.9 L1570.32 1830.1 L1546.2 1830.1 L1546.2 1826.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M1578.55 1812.49 L1593.47 1812.49 L1593.47 1815.69 L1582.03 1815.69 L1582.03 1822.57 Q1582.86 1822.29 1583.69 1822.16 Q1584.52 1822.01 1585.34 1822.01 Q1590.05 1822.01 1592.79 1824.59 Q1595.54 1827.16 1595.54 1831.56 Q1595.54 1836.1 1592.72 1838.62 Q1589.9 1841.12 1584.76 1841.12 Q1582.99 1841.12 1581.15 1840.82 Q1579.33 1840.52 1577.37 1839.91 L1577.37 1836.1 Q1579.06 1837.02 1580.87 1837.47 Q1582.67 1837.92 1584.69 1837.92 Q1587.94 1837.92 1589.84 1836.21 Q1591.74 1834.5 1591.74 1831.56 Q1591.74 1828.63 1589.84 1826.92 Q1587.94 1825.21 1584.69 1825.21 Q1583.16 1825.21 1581.64 1825.54 Q1580.13 1825.88 1578.55 1826.6 L1578.55 1812.49 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip260)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="192.376,1787.62 192.376,47.2441 "></polyline>
<polyline clip-path="url(#clip260)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="192.376,1369.11 211.274,1369.11 "></polyline>
<polyline clip-path="url(#clip260)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="192.376,630.61 211.274,630.61 "></polyline>
<path clip-path="url(#clip260)" d="M50.2234 1388.9 L57.8623 1388.9 L57.8623 1362.54 L49.5521 1364.2 L49.5521 1359.95 L57.816 1358.28 L62.4919 1358.28 L62.4919 1388.9 L70.1307 1388.9 L70.1307 1392.84 L50.2234 1392.84 L50.2234 1388.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M89.5751 1361.36 Q85.964 1361.36 84.1353 1364.92 Q82.3297 1368.46 82.3297 1375.59 Q82.3297 1382.7 84.1353 1386.26 Q85.964 1389.81 89.5751 1389.81 Q93.2093 1389.81 95.0148 1386.26 Q96.8435 1382.7 96.8435 1375.59 Q96.8435 1368.46 95.0148 1364.92 Q93.2093 1361.36 89.5751 1361.36 M89.5751 1357.65 Q95.3852 1357.65 98.4408 1362.26 Q101.519 1366.84 101.519 1375.59 Q101.519 1384.32 98.4408 1388.93 Q95.3852 1393.51 89.5751 1393.51 Q83.7649 1393.51 80.6862 1388.93 Q77.6307 1384.32 77.6307 1375.59 Q77.6307 1366.84 80.6862 1362.26 Q83.7649 1357.65 89.5751 1357.65 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M101.519 1351.76 L125.631 1351.76 L125.631 1354.95 L101.519 1354.95 L101.519 1351.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M144.27 1340.66 L134.678 1355.65 L144.27 1355.65 L144.27 1340.66 M143.273 1337.35 L148.05 1337.35 L148.05 1355.65 L152.056 1355.65 L152.056 1358.81 L148.05 1358.81 L148.05 1365.43 L144.27 1365.43 L144.27 1358.81 L131.593 1358.81 L131.593 1355.14 L143.273 1337.35 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M51.9161 650.402 L59.555 650.402 L59.555 624.037 L51.2448 625.704 L51.2448 621.444 L59.5087 619.778 L64.1846 619.778 L64.1846 650.402 L71.8234 650.402 L71.8234 654.338 L51.9161 654.338 L51.9161 650.402 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M91.2678 622.856 Q87.6567 622.856 85.828 626.421 Q84.0224 629.963 84.0224 637.092 Q84.0224 644.199 85.828 647.764 Q87.6567 651.305 91.2678 651.305 Q94.902 651.305 96.7075 647.764 Q98.5362 644.199 98.5362 637.092 Q98.5362 629.963 96.7075 626.421 Q94.902 622.856 91.2678 622.856 M91.2678 619.153 Q97.0779 619.153 100.133 623.759 Q103.212 628.342 103.212 637.092 Q103.212 645.819 100.133 650.426 Q97.0779 655.009 91.2678 655.009 Q85.4576 655.009 82.3789 650.426 Q79.3234 645.819 79.3234 637.092 Q79.3234 628.342 82.3789 623.759 Q85.4576 619.153 91.2678 619.153 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M103.212 613.254 L127.324 613.254 L127.324 616.451 L103.212 616.451 L103.212 613.254 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M138.796 623.73 L152.056 623.73 L152.056 626.927 L134.226 626.927 L134.226 623.73 Q136.389 621.492 140.113 617.73 Q143.856 613.95 144.815 612.859 Q146.639 610.809 147.354 609.398 Q148.087 607.969 148.087 606.596 Q148.087 604.358 146.508 602.947 Q144.947 601.537 142.426 601.537 Q140.64 601.537 138.646 602.157 Q136.671 602.778 134.414 604.038 L134.414 600.201 Q136.709 599.28 138.702 598.81 Q140.696 598.339 142.351 598.339 Q146.715 598.339 149.31 600.521 Q151.905 602.703 151.905 606.352 Q151.905 608.082 151.247 609.643 Q150.608 611.185 148.896 613.292 Q148.426 613.837 145.906 616.451 Q143.386 619.047 138.796 623.73 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip262)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="261.67,1738.36 518.313,1553.74 774.957,1369.12 1031.6,1184.51 1288.24,999.941 1544.89,815.488 1801.53,631.404 2058.17,448.444 2314.82,268.676 2571.46,96.5 "></polyline>
<path clip-path="url(#clip260)" d="M2261.87 208.937 L2559.14 208.937 L2559.14 105.257 L2261.87 105.257  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path>
<polyline clip-path="url(#clip260)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2261.87,208.937 2559.14,208.937 2559.14,105.257 2261.87,105.257 2261.87,208.937 "></polyline>
<polyline clip-path="url(#clip260)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2289.08,157.097 2452.3,157.097 "></polyline>
<path clip-path="url(#clip260)" d="M2493.35 176.784 Q2491.55 181.414 2489.83 182.826 Q2488.12 184.238 2485.25 184.238 L2481.85 184.238 L2481.85 180.673 L2484.35 180.673 Q2486.11 180.673 2487.08 179.84 Q2488.05 179.006 2489.23 175.904 L2489.99 173.96 L2479.51 148.451 L2484.02 148.451 L2492.12 168.728 L2500.23 148.451 L2504.74 148.451 L2493.35 176.784 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip260)" d="M2512.03 170.441 L2519.67 170.441 L2519.67 144.076 L2511.36 145.742 L2511.36 141.483 L2519.62 139.817 L2524.3 139.817 L2524.3 170.441 L2531.94 170.441 L2531.94 174.377 L2512.03 174.377 L2512.03 170.441 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path></svg>
</div>
</div>
<p>This type of plot is, of course, somewhat heuristic: the asymptotic behavior as <span class="math inline">\(\epsilon \rightarrow 0\)</span> should manifest for small enough <span class="math inline">\(\epsilon\)</span>, but nothing says that <span class="math inline">\(10^{-10}\)</span> (for example) is “small enough.” We can be more rigorous by including additional information, as we will discuss in later chapters.</p>
</section>
</section>
<section id="derivatives" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="derivatives"><span class="header-section-number">5.4</span> Derivatives</h2>
<p>We assume the reader is thoroughly familiar with differentiation in one dimension. Our focus in the rest of this section is on differentiation on functions between vector spaces.</p>
<section id="gateaux-and-frechet" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="gateaux-and-frechet"><span class="header-section-number">5.4.1</span> Gateaux and Frechet</h3>
<section id="functions-from-mathbbr-to-mathbbr" class="level4" data-number="5.4.1.1">
<h4 data-number="5.4.1.1" class="anchored" data-anchor-id="functions-from-mathbbr-to-mathbbr"><span class="header-section-number">5.4.1.1</span> Functions from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span></h4>
<p>The standard definition for the derivative of <span class="math inline">\(f : \mathbb{R}\rightarrow \mathbb{R}\)</span> is <span class="math display">\[
  f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}.
\]</span> Alternately, we can write <span class="math inline">\(f\)</span> as <span class="math display">\[
  f(x+h) = f(x) + f'(x) h + o(|h|).
\]</span> Of course, not all functions are differentiable. When <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(x\)</span>, we will sometimes also write <span class="math inline">\(Df(x)\)</span> to denote the derivative.</p>
<p>When the derivative exists and is continuous on some interval <span class="math inline">\((a,b)\)</span> containing <span class="math inline">\(x\)</span>, we say <span class="math inline">\(f\)</span> is <span class="math inline">\(C^1\)</span> on <span class="math inline">\((a,b)\)</span>. In this case, the fundamental theorem of calculus gives that for any <span class="math inline">\(x+h \in (a,b)\)</span>, <span class="math display">\[\begin{aligned}
  f(x+h)
  &amp;= f(x) + \int_0^h f'(x+\xi) \, d\xi \\
  &amp;= f(x) + f'(x)h + r(h) \\
  r(h) &amp;= \int_0^h \left( f'(x+\xi)-f'(x) \right) \, d\xi.
\end{aligned}\]</span> Here <span class="math inline">\(r(h)\)</span> is the <em>remainder term</em> for the first-order approximation <span class="math inline">\(f(x) + f(x)h\)</span>. If <span class="math inline">\(\omega_{f'}\)</span> is the modulus of continuity for <span class="math inline">\(f'\)</span> on <span class="math inline">\((a,b)\)</span>, then <span class="math display">\[
  \|f'(x+\xi)-f'(x)\| \leq \omega_{f'}(|\xi|),
\]</span> and therefore <span class="math display">\[
  |r(h)| \leq \int_0^{|h|} \omega_{f'}(\xi) \, d\xi.
\]</span> When <span class="math inline">\(f'\)</span> is Lipschitz with constant <span class="math inline">\(C\)</span>, we have <span class="math display">\[
  |r(h)| \leq \int_0^{|h|} \omega_{f'}(|\xi|) \, d\xi
  \leq \int_0^{|h|} C\xi \, d\xi = \frac{1}{2} Ch^2.
\]</span> Hence, in regions where <span class="math inline">\(f'\)</span> is not only continuous but Lipschitz, the error <span class="math inline">\(r(h)\)</span> in the linearized approximation is not only <span class="math inline">\(o(|h|)\)</span>, but it is even <span class="math inline">\(O(h^2)\)</span>.</p>
<p>When a differentiable function <span class="math inline">\(f\)</span> is only available by evaluation, we can approximate the derivative at <span class="math inline">\(x\)</span> by finite differences, e.g.&nbsp;taking <span class="math display">\[
  f'(x) \approx \frac{f(x+h)-f(x)}{h}
\]</span> where <span class="math inline">\(h\)</span> is “small enough.” More precisely, if <span class="math inline">\(f\)</span> is <span class="math inline">\(C^1\)</span> on an interval containing <span class="math inline">\((a,b)\)</span>, we have <span class="math display">\[
  \frac{f(x)+f'(x)h + r(h) - f(x)}{h} = f'(x) + \frac{r(h)}{h},
\]</span> where <span class="math inline">\(r\)</span> is the remainder for the linear approximation about <span class="math inline">\(x\)</span>. When <span class="math inline">\(f'\)</span> is Lipschitz with constant <span class="math inline">\(C\)</span>, the error in this finite difference approximation is therefore <span class="math display">\[
  \frac{|r(h)|}{|h|} \leq \frac{1}{2} C|h| = O(|h|).
\]</span> We further analyze how the size of <span class="math inline">\(h\)</span> affects accuracy, as well as other methods for approximating derivatives, in <a href="../01-Fund1d/05-NumDiff.html" class="quarto-xref"><span>Chapter 12</span></a>. For this chapter, though, it will still be useful to sanity check derivatives via finite differences:</p>
<div id="7eed7749" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">finite_diff</span>(f, x; h<span class="op">=</span><span class="fl">1e-8</span>) <span class="op">=</span> (<span class="fu">f</span>(x<span class="op">+</span>h)<span class="fu">-f</span>(x))<span class="op">/</span>h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="functions-from-mathbbr-to-mathcalw" class="level4" data-number="5.4.1.2">
<h4 data-number="5.4.1.2" class="anchored" data-anchor-id="functions-from-mathbbr-to-mathcalw"><span class="header-section-number">5.4.1.2</span> Functions from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathcal{W}\)</span></h4>
<p>Suppose <span class="math inline">\(g : \mathbb{R}\rightarrow \mathcal{W}\)</span> for some normed linear space <span class="math inline">\(\mathcal{W}\)</span>. We can define the derivative of <span class="math inline">\(g\)</span> almost identically to the derivative for a function from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, i.e. <span class="math display">\[
  g(x+h) = g(x) + g'(x) h + r(h), \quad \|r(h)\| = o(|h|).
\]</span> The only thing that differs is that we are controlling the norm of <span class="math inline">\(r(h)\)</span> rather than the absolute value. Also similarly to the real case, if <span class="math inline">\(g'\)</span> is Lipschitz with constant <span class="math inline">\(C\)</span>, then <span class="math display">\[
  \|r(h)\| \leq \frac{1}{2} Ch^2,
\]</span> and we can bound the error in simple finite difference estimates of the derivative as we do in the case of functions from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>There are a handful of theorems that are specific to real-valued functions, like the mean value theorem (<a href="#sec-calculus-mvt" class="quarto-xref"><span>Section 5.4.2</span></a>). These theorems usually still tell us interesting things about vector valued functions by considering functions <span class="math inline">\(x \mapsto w^* g(x)\)</span> where <span class="math inline">\(w^* \in \mathcal{W}^*\)</span> is some dual vector.</p>
</section>
<section id="functions-from-mathcalv-to-mathbbr" class="level4" data-number="5.4.1.3">
<h4 data-number="5.4.1.3" class="anchored" data-anchor-id="functions-from-mathcalv-to-mathbbr"><span class="header-section-number">5.4.1.3</span> Functions from <span class="math inline">\(\mathcal{V}\)</span> to <span class="math inline">\(\mathbb{R}\)</span></h4>
<p>Suppose <span class="math inline">\(f : \mathcal{V}\rightarrow \mathbb{R}\)</span> for some normed linear space <span class="math inline">\(\mathcal{V}\)</span>. Then for any point <span class="math inline">\(x \in \mathcal{V}\)</span> and nonzero vector <span class="math inline">\(u \in \mathcal{V}\)</span>, we can define a function <span class="math inline">\(f \circ c\)</span> from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span> that evaluates <span class="math inline">\(f\)</span> along a ray <span class="math inline">\(c_{x,u}(s) = x+su\)</span>. Assuming it exists, the derivative <span class="math inline">\((f \circ c_{x,u})'\)</span> is the <em>Gateaux derivative</em> (directional derivative) of <span class="math inline">\(f\)</span> at a point <span class="math inline">\(x\)</span> and in a direction <span class="math inline">\(u\)</span>: <span class="math display">\[
  D[f(x); u] = (f \circ c_{x,u})'(0).
\]</span> Alternately, the Gateaux derivative is the number such that <span class="math display">\[
  f(x+su) = f(x) + s D[f(x); u] + o(s).
\]</span> We can estimate the Gateaux derivative by finite differencing in the given direction</p>
<div id="09cd3670" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">finite_diff</span>(f, x, u; h<span class="op">=</span><span class="fl">1e-8</span>) <span class="op">=</span> (<span class="fu">f</span>(x<span class="op">+</span>h<span class="op">*</span>u)<span class="fu">-f</span>(x))<span class="op">/</span>h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When the Gateaux derivative of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span> is defined for all <span class="math inline">\(u \in \mathcal{V}\)</span>, we say <span class="math inline">\(f\)</span> is <em>Gateaux differentiable</em> at <span class="math inline">\(x\)</span>.</p>
<p>When all the Gateaux derivatives are continuously defined in a neighborhood of <span class="math inline">\(v\)</span>, we say <span class="math inline">\(f\)</span> is <span class="math inline">\(C^1\)</span> on some open set containing <span class="math inline">\(v\)</span>. In this case, Gateaux derivatives are related by the <em>Frechet derivative</em>, a functional <span class="math inline">\(f'(x) \in \mathcal{V}^*\)</span> (also written <span class="math inline">\(Df(x)\)</span>) such that <span class="math display">\[
  D[f(x); u] = f'(x) u
\]</span> or, equivalently, for any direction <span class="math inline">\(u\)</span>, <span class="math display">\[
  f(x+su) = f(x) + sf'(x) u + o(s).
\]</span> When the Frechet derivative is Lipschitz with some constant <span class="math inline">\(C\)</span> in a consistent norm, we can bound the remainder term by <span class="math inline">\(Cs^2/2\)</span>, as in the one-dimensional case. Frechet differentiability is a stronger condition than Gateaux differentiability. When we say a function is “differentiable at <span class="math inline">\(x\)</span>” without qualifications, we mean it is Frechet differentiable.</p>
<p>The Frechet derivative (or just “the derivative”) <span class="math inline">\(f'(x)\)</span> is an element in <span class="math inline">\(\mathcal{V}^*\)</span>. In a (real) inner product space, the <em>gradient</em> <span class="math inline">\(\nabla f(x) \in \mathcal{V}\)</span> is the dual of <span class="math inline">\(f'(x)\)</span> given by the Riesz map, i.e. <span class="math display">\[
  f'(x) u = \langle \nabla f(x), u \rangle.
\]</span> In <span class="math inline">\(\mathbb{R}^n\)</span> with the standard inner product, the gradient (a column vector) is the transpose of the derivative (a row vector).</p>
</section>
<section id="a-polynomial-example" class="level4" data-number="5.4.1.4">
<h4 data-number="5.4.1.4" class="anchored" data-anchor-id="a-polynomial-example"><span class="header-section-number">5.4.1.4</span> A polynomial example</h4>
<p>Following our pattern from the previous chapter, we consider an example in a polynomial space. Let <span class="math inline">\(f : \mathcal{P}_d \rightarrow \mathbb{R}\)</span> be given by <span class="math inline">\(f(p) = \frac{1}{2} \int_{-1}^1 p(x)^2 \, dx\)</span>. The Frechet derivative of <span class="math inline">\(f\)</span> (which is a function of the polynomial <span class="math inline">\(p \in \mathcal{P}_d\)</span>, not the indeterminate <span class="math inline">\(x\)</span>) is then the functional <span class="math inline">\(f'(p)\)</span> such that the action on a vector <span class="math inline">\(q \in \mathcal{P}_d\)</span> is <span class="math display">\[
  f'(p)q = \int_{-1}^1 p(x) q(x) \, dx.
\]</span> With respect to the power basis, we can write <span class="math inline">\(f'(q)\)</span> in terms of the row vector <span class="math display">\[
  d^T = \begin{bmatrix} f'(p) x^0 &amp; f'(p) x^1 &amp; \ldots &amp; f'(p) x^d \end{bmatrix}.
\]</span> The gradient with respect to the <span class="math inline">\(L^2([-1, 1])\)</span> inner product is then <span class="math display">\[
  \nabla f(p) = X_{0:d} M^{-1} d
\]</span> where <span class="math inline">\(M\)</span> is the Gram matrix for the power basis.</p>
<p>We can compute with <span class="math inline">\(f'(p)\)</span> by calling the <code>integrate</code> command or through the power basis. We can also compute with <span class="math inline">\(f'(p)\)</span> by writing it in terms of a basis of <span class="math inline">\(\mathcal{P}_d^*\)</span> in terms of point evaluation functionals, i.e. <span class="math display">\[
  f'(p) = c^T W^*, \quad W^* = \begin{bmatrix} \delta_{x_0}^* \\ \vdots \\ \delta_{x_d}^* \end{bmatrix}.
\]</span> We can also compute the <span class="math inline">\(c\)</span> coefficients by solving <span class="math display">\[
  c^T A = d^T
\]</span> where <span class="math inline">\(A\)</span> is the Vandermonde matrix <span class="math display">\[
  A = W^* X_{0:d} =
  \begin{bmatrix}
    1 &amp; x_0^1 &amp; \ldots &amp; x_0^d \\
    1 &amp; x_1^1 &amp; \ldots &amp; x_1^d \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    1 &amp; x_d^1 &amp; \ldots &amp; x_d^d
  \end{bmatrix}.
\]</span></p>
<p>We can sanity check this against finite differences for a random pair of polynomials:</p>
<div id="b8981982" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Functions for f and the Frechet derivative f'(p)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">f</span>(p) <span class="op">=</span> <span class="fu">integrate</span>(<span class="fl">0.5</span><span class="op">*</span>p<span class="op">*</span>p, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Df</span>(p) <span class="op">=</span> q <span class="op">-&gt;</span> <span class="fu">integrate</span>(p<span class="op">*</span>q, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Express f'(p) in the power basis</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">d</span>(p) <span class="op">=</span> [<span class="fu">Df</span>(p).(<span class="fu">Polynomial</span>(ej)) for ej <span class="kw">in</span> <span class="fu">eachcol</span>(<span class="fu">Matrix</span>(I,<span class="fl">4</span>,<span class="fl">4</span>))]<span class="ch">'</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gram matrix for the L2 inner product</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">m</span>(k) <span class="op">=</span> k <span class="op">%</span> <span class="fl">2</span> <span class="op">==</span> <span class="fl">1</span> ? <span class="fl">0.0</span> <span class="op">:</span> <span class="fl">2</span><span class="op">/</span>(k<span class="op">+</span><span class="fl">1</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> [<span class="fu">m</span>(i<span class="op">+</span>j) for i<span class="op">=</span><span class="fl">0</span><span class="op">:</span><span class="fl">3</span>, j<span class="op">=</span><span class="fl">0</span><span class="op">:</span><span class="fl">3</span>]</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the gradient</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">∇f</span>(p) <span class="op">=</span> <span class="fu">Polynomial</span>(<span class="fu">M\d</span>(p)<span class="ch">')</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Vandermonde matrix A for four equispaced points</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="fu">range</span>(<span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>, length<span class="op">=</span><span class="fl">4</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> [xi<span class="op">^</span>j for xi<span class="op">=</span>x, j<span class="op">=</span><span class="fl">0</span><span class="op">:</span><span class="fl">3</span>]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute c in terms of point evaluations</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(p) <span class="op">=</span> <span class="fu">d</span>(p)<span class="op">/</span>A</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test everything</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    ptest <span class="op">=</span> <span class="fu">Polynomial</span>(<span class="fu">rand</span>(<span class="fl">4</span>))</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    qtest <span class="op">=</span> <span class="fu">Polynomial</span>(<span class="fu">rand</span>(<span class="fl">4</span>))</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    Dfpq_fd <span class="op">=</span> <span class="fu">finite_diff</span>(f, ptest, qtest)     <span class="co"># Finite diff</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    Dfpq1 <span class="op">=</span> <span class="fu">Df</span>(ptest)(qtest)                   <span class="co"># Analytic form</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    Dfpq2 <span class="op">=</span> <span class="fu">c</span>(ptest)<span class="fu">*qtest</span>.(x)                 <span class="co"># Via evaluation fnls</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    Dfpq3 <span class="op">=</span> <span class="fu">integrate</span>(<span class="fu">∇f</span>(ptest)<span class="op">*</span>qtest, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>)  <span class="co"># Via gradient</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compare -- everything should be equal (some approx for fd)</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">isapprox</span>(Dfpq1, Dfpq_fd, rtol<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">&amp;&amp;</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    Dfpq1 <span class="op">≈</span> Dfpq2 <span class="op">&amp;&amp;</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    Dfpq1 <span class="op">≈</span> Dfpq3</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>true</code></pre>
</div>
</div>
</section>
<section id="general-mappings" class="level4" data-number="5.4.1.5">
<h4 data-number="5.4.1.5" class="anchored" data-anchor-id="general-mappings"><span class="header-section-number">5.4.1.5</span> General mappings</h4>
<p>Now consider <span class="math inline">\(f : \mathcal{V}\rightarrow \mathcal{W}\)</span> where <span class="math inline">\(\mathcal{V}\)</span> and <span class="math inline">\(\mathcal{W}\)</span> are normed linear spaces. In this case, the Gateaux derivative in a direction <span class="math inline">\(u \in \mathcal{V}\)</span> is a vector in <span class="math inline">\(\mathcal{W}\)</span>, still given as <span class="math display">\[
  D[f(v); u] = (f \circ c_{v,u})'(0) \mbox{ for }
  c(s) = x+su
\]</span> and when there is a Frechet derivative <span class="math inline">\(Df(v) = f'(v) \in L(\mathcal{V}, \mathcal{W})\)</span> we have <span class="math display">\[
  f(v+u) = f(v) + f'(v) u + o(\|u\|),
\]</span> and the Gateaux derivatives satisfy <span class="math display">\[
  D[f(v); u] = f'(v) u.
\]</span> When the Frechet derivative is Lipschitz with constant <span class="math inline">\(C\)</span> (with respect to the induced norm), then the remainder for the linear approximation is bounded by <span class="math inline">\(C\|u\|^2/2\)</span>. The representation of the Frechet derivative with respect to a particular basis is the <em>Jacobian matrix</em> <span class="math inline">\(J = W^{-1} f'(p) V\)</span>.</p>
</section>
</section>
<section id="sec-calculus-mvt" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="sec-calculus-mvt"><span class="header-section-number">5.4.2</span> Mean values</h3>
<p>For one-dimensional continuously differentiable functions <span class="math inline">\(f : \Omega \subset \mathbb{R}\rightarrow \mathbb{R}\)</span> for some open connected <span class="math inline">\(\Omega\)</span>, the fundamental theorem of calculus tells us that for <span class="math inline">\(a, b
\in \Omega\)</span> we have <span class="math display">\[
  f(b)-f(a)
  = \int_0^1 f'(zb + (1-z)a)(b-a) \, dz.
\]</span> The <em>mean value theorem</em> tells us that for some intermediate <span class="math inline">\(c \in [a,b]\)</span> (<span class="math inline">\(c = \xi b + (1-\xi) a\)</span> for <span class="math inline">\(\xi \in [0,1]\)</span>), we have <span class="math display">\[\begin{aligned}
  f'(c)(b-a) = \int_0^1 f'(zb + (1-z)a)(b-a) \, dz.
\end{aligned}\]</span> The fundamental theorem of calculus holds even when <span class="math inline">\(f\)</span> has discontinuities in the derivative at some points, but the second equation relies on continuity of the derivative. This useful theorem generalizes to the vector space case in various ways.</p>
<p>For continuously differentiable functions <span class="math inline">\(f : \Omega \subset \mathcal{V}
\rightarrow \mathbb{R}\)</span> where <span class="math inline">\(\Omega\)</span> is open and convex (i.e.&nbsp;the line segment connecting <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> lies within <span class="math inline">\(\Omega\)</span>), we have <span class="math display">\[\begin{aligned}
  f(b)-f(a)
  &amp;= \int_0^1 f'(zb + (1-z)a) (b-a) \, dz \\
  &amp;= f'(\xi b + (1-\xi) a) (b-a).
\end{aligned}\]</span> for some <span class="math inline">\(\xi \in [0,1]\)</span>. And for continuously differentiable functions <span class="math inline">\(g : \Omega \subset \mathcal{V}\rightarrow \mathcal{W}\)</span> where <span class="math inline">\(\Omega\)</span> is open and convex, we have that for any <span class="math inline">\(w^* \in
\mathcal{V}^*\)</span> we can apply the mean value theorem to the real-valued <span class="math inline">\(f(v) =
w^* g(v)\)</span>; that is, there is a <span class="math inline">\(\xi \in [0,1]\)</span> such that <span class="math display">\[\begin{aligned}
  w^* (g(b)-g(a))
  &amp;= \int_0^1 w^* g(zb + (1-z)a) (b-a) \, dz \\
  &amp;= w^* g'(\xi b + (1-\xi) a) (b-a).
\end{aligned}\]</span> However, the choice of <span class="math inline">\(\xi\)</span> depends on <span class="math inline">\(w^*\)</span>, so it is <em>not</em> true that we can fully generalize the mean value theorem to maps between vector spaces. On the other hand, we can get something that is almost as good, at least for the purpose of proving bounds used in numerical computing, by choosing <span class="math inline">\(w^*\)</span> to be a vector such that <span class="math inline">\(\|w^*\|=1\)</span> (using the dual norm to whatever norm we are using for <span class="math inline">\(\mathcal{W}\)</span>) and <span class="math inline">\(w^* (g(b)-g(a)) = \|g(b)-g(a)\|\)</span>. With this choice, and consistent choices of norms, we have that for <span class="math display">\[\begin{aligned}
  \|g(b)-g(a)\|
  &amp;= \int_0^1 w^* g'(zb + (1-z)a) (b-a) \, dz \\
  &amp;\leq \int_0^1 \|w^*\| \|g'(zb+(1-z)a)\| \|b-a\| \, dz \\
  &amp;\leq \left( \int_0^1 \|g'(zb+(1-z)a)\| \, dz \right) \|b-a\|.
\end{aligned}\]</span> We can also bound the average norm of the derivative on the segment from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span> by <span class="math display">\[
  \int_0^1 \|g'(zb+(1-z)a)\| \, dz
  \leq
  \max_{z \in [0,1]} \|g'(zb+(1-z)a)\|.
\]</span> When <span class="math inline">\(\|g'(x)\| \leq C\)</span> for all <span class="math inline">\(x \in \Omega\)</span>, the function <span class="math inline">\(g\)</span> is Lipschitz with constant <span class="math inline">\(C\)</span> on <span class="math inline">\(\Omega\)</span>.</p>
</section>
<section id="chain-rule" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="chain-rule"><span class="header-section-number">5.4.3</span> Chain rule</h3>
<p>Suppose <span class="math inline">\(f : \mathcal{V}\rightarrow \mathcal{W}\)</span> and <span class="math inline">\(g : \mathcal{U}\rightarrow
\mathcal{V}\)</span> are <em>affine</em> functions, i.e. <span class="math display">\[\begin{aligned}
  g(x) &amp;= g_0 + J_g x \\
  f(y) &amp;= f_0 + J_f y.
\end{aligned}\]</span> Then <span class="math inline">\(h = f \circ g\)</span> is another affine map <span class="math display">\[
  h(x) = f(g(x)) = f(g(0)) + J_f J_g x.
\]</span> The chain rule is just a generalization from affine maps to functions well approximated by affine maps (i.e.&nbsp;differentiable functions).</p>
<p>Now let <span class="math inline">\(f : \mathcal{V}\rightarrow \mathcal{W}\)</span> and <span class="math inline">\(g : \mathcal{U}\rightarrow \mathcal{V}\)</span> be functions where <span class="math inline">\(g\)</span> is differentiable at <span class="math inline">\(u\)</span> and <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(v = g(u)\)</span>. Then <span class="math display">\[\begin{aligned}
g(u+x) &amp;= g(u) + g'(u) x + r_g(x), &amp; r_g(x) &amp;= o(\|x\|), \\
f(v+y) &amp;= f(v) + f'(u) y + r_f(y), &amp; r_f(y) &amp;= o(\|y\|).
\end{aligned}\]</span> Let <span class="math inline">\(h = f \circ g\)</span>; then setting <span class="math inline">\(y = g'(u) x + r_g(x) = O(\|x\|)\)</span>, we have <span class="math display">\[\begin{aligned}
  h(u+x) &amp;=
  f(v) + f'(v) y + r_f(y) \\
  &amp;= f(v) + f'(v) g'(u) x + r_h(x), \\
  r_h(x) &amp;= g'(u) r_g(x) + r_f(g'(u) x + r_g(x)) = o(\|x\|).
\end{aligned}\]</span> That is, <span class="math inline">\(h\)</span> is differentiable at <span class="math inline">\(u\)</span> with <span class="math inline">\(h'(u) = f'(v) g'(u)\)</span>.</p>
<section id="a-polynomial-example-1" class="level4" data-number="5.4.3.1">
<h4 data-number="5.4.3.1" class="anchored" data-anchor-id="a-polynomial-example-1"><span class="header-section-number">5.4.3.1</span> A polynomial example</h4>
<p>Now consider <span class="math display">\[
  h(p) =
  \frac{1}{2} \int_{-1}^1 \left( \frac{dp}{dx} - \phi(x) \right)^2 \, dx,
\]</span> which we can see as the composition of the functional <span class="math display">\[
  f(r) = \frac{1}{2} \int_{-1}^1 r(x)^2 \, dx
\]</span> which we analyzed in the previous section, together with <span class="math display">\[
  r(p) = \frac{dp}{dx} - \phi(x).
\]</span> The function <span class="math inline">\(r(p)\)</span> is affine in <span class="math inline">\(p\)</span>, and the derivative <span class="math inline">\(r'(p)\)</span> is simply the differentiation operator <span class="math inline">\(\frac{d}{dx}\)</span>. We already saw that <span class="math display">\[
  f'(r) q = \int_{-1}^1 r(x) q(x) \, dx.
\]</span> Therefore, by the chain rule we have <span class="math display">\[
  h'(p) q = f'(r) r'(p) q = \int_{-1}^1 r(x) \frac{dq}{dx}(x) \, dx.
\]</span> As is often the case, working code is a useful check to make sure that we understand what is happening with the mathematics:</p>
<div id="6d12d07f" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    ϕ <span class="op">=</span> <span class="fu">Polynomial</span>(<span class="fu">rand</span>(<span class="fl">4</span>))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">r</span>(p) <span class="op">=</span> <span class="fu">derivative</span>(p) <span class="op">-</span> ϕ</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">f</span>(r) <span class="op">=</span> <span class="fl">0.5</span><span class="fu">*integrate</span>(r<span class="op">*</span>r, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">h</span>(p) <span class="op">=</span> <span class="fu">f</span>(<span class="fu">r</span>(p))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Dh</span>(p) <span class="op">=</span> q <span class="op">-&gt;</span> <span class="fu">integrate</span>(<span class="fu">r</span>(p)<span class="fu">*derivative</span>(q), <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    ptest <span class="op">=</span> <span class="fu">Polynomial</span>(<span class="fu">rand</span>(<span class="fl">5</span>))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    qtest <span class="op">=</span> <span class="fu">Polynomial</span>(<span class="fu">rand</span>(<span class="fl">5</span>))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">isapprox</span>(<span class="fu">Dh</span>(ptest)(qtest),</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>             <span class="fu">finite_diff</span>(h, ptest, qtest), rtol<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>true</code></pre>
</div>
</div>
</section>
</section>
<section id="partial-derivatives" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4" class="anchored" data-anchor-id="partial-derivatives"><span class="header-section-number">5.4.4</span> Partial derivatives</h3>
<p>Consider a Frechet-differentiable function <span class="math inline">\(f : \mathcal{V}\oplus \mathcal{U}\rightarrow \mathcal{W}\)</span>. For <span class="math inline">\(x \in \mathcal{V}\oplus \mathcal{U}\)</span>, we can write the Frechet derivative as quasimatrix with respect to the two components of the domain space: <span class="math display">\[
  Df(x) = \begin{bmatrix} (Df(x))_1 &amp; (Df(x))_2 \end{bmatrix},
\]</span> where <span class="math inline">\((Df(x))_1 \in L(\mathcal{V}, \mathcal{W})\)</span> and <span class="math inline">\((Df(x))_2 \in L(\mathcal{U}, \mathcal{W})\)</span>. However, it is often more convenient to “unpack” the input to the function into different components, i.e.&nbsp;thinking of <span class="math inline">\(f : \mathcal{V}\times \mathcal{U}\rightarrow \mathcal{W}\)</span>. We similarly unpack <span class="math inline">\(y\)</span> into a <span class="math inline">\(\mathcal{V}\)</span> component <span class="math inline">\(y_1\)</span> and a <span class="math inline">\(\mathcal{U}\)</span> component <span class="math inline">\(y_2\)</span>. Then we write <span class="math display">\[
  Df(x_1,x_2)(y_1,y_2) = D_1f(x_1,x_2) y_1 + D_2f(x_1,x_2) y_2,
\]</span> where the maps <span class="math inline">\(D_1 f(x_1,x_2) \in L(\mathcal{V}, \mathcal{W})\)</span> and <span class="math inline">\(D_2 f(x_1,x_2) \in L(\mathcal{U}, \mathcal{W})\)</span> are the same as <span class="math inline">\((Df(x))_1\)</span> and <span class="math inline">\((Df(x))_2\)</span> above. We refer to <span class="math inline">\(D_1 f\)</span> and <span class="math inline">\(D_2 f\)</span> as the <em>partial derivatives</em> of <span class="math inline">\(f\)</span> with respect to the first and second argument.</p>
<p>More generally, if <span class="math inline">\(f\)</span> is a Frechet-differentiable function with <span class="math inline">\(m\)</span> arguments, we write the quasimatrix expression <span class="math display">\[\begin{aligned}
  &amp;Df(x_1, \ldots, x_m) (y_1, \ldots, y_m) \\
  &amp;=
  \begin{bmatrix}
  D_1 f(x_1, \ldots, x_m) &amp;
  \ldots &amp;
  D_m f(x_1, \ldots, x_m)
  \end{bmatrix}
  \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix} \\
  &amp;=
  \sum_j D_j f(x_1, \ldots, x_m) y_j.
\end{aligned}\]</span> Though this may look like a regular matrix expression, we are deliberately <em>not</em> assuming that the arguments have the same type. Indeed, the arguments may belong to wildly different spaces: the first component might belong to <span class="math inline">\(\mathcal{P}_d\)</span>, the second to <span class="math inline">\(\mathbb{R}^n\)</span>, and so on. Nonetheless, we can formally put a (normed) vector space structure on the whole list taken as a direct sum of the argument spaces, and then treat the partial derivatives <span class="math inline">\(D_j f\)</span> as pieces of the overall derivative <span class="math inline">\(Df\)</span>.</p>
<p>A common source of confusion in partial differentiation comes from implicit function composition. For example, consider <span class="math inline">\(f : \mathcal{V}
\rightarrow \mathbb{R}\)</span> where <span class="math inline">\(\mathcal{V}\)</span> is a two dimensional space with bases <span class="math inline">\(V = \begin{bmatrix} v_1 &amp; v_2 \end{bmatrix}\)</span> and <span class="math inline">\(U = \begin{bmatrix} u_1 &amp; u_e \end{bmatrix}\)</span>. Then we can write <span class="math display">\[\begin{aligned}
  h_V(\alpha, \beta) &amp;= (f \circ g_V)(\alpha, \beta), &amp;
  g_V(\alpha, \beta) &amp;= v_1 \alpha + v_2 \beta, \\
  h_U(a, b) &amp;= (f \circ g_U)(a, b), &amp;
  g_U(a, b) &amp;= u_1 a + u_2 b.
\end{aligned}\]</span> In this notation, the partials are clearly defined: <span class="math display">\[\begin{aligned}
  D_k h_V(\alpha, \beta) &amp;= f'(g_V(\alpha, \beta)) v_k, \\
  D_k h_U(\alpha, \beta) &amp;= f'(g_U(\alpha, \beta)) v_k.
\end{aligned}\]</span> In contrast, classical notation often conflates <span class="math inline">\(f\)</span>, <span class="math inline">\(f \circ g_V\)</span>, and <span class="math inline">\(f \circ g_U\)</span>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Classical notation</th>
<th>Our notation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\partial f / \partial \alpha\)</span></td>
<td><span class="math inline">\(D_1 (f \circ g_V)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\partial f / \partial \beta\)</span></td>
<td><span class="math inline">\(D_2 (f \circ g_V)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\partial f / \partial a\)</span></td>
<td><span class="math inline">\(D_1 (f \circ g_U)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\partial f / \partial b\)</span></td>
<td><span class="math inline">\(D_2 (f \circ g_U)\)</span></td>
</tr>
</tbody>
</table>
<p>We will sometimes use symbolic labels rather than indices to refer to arguments to a function. For example, we might write <span class="math inline">\(D_u f(u,v)\)</span> instead of <span class="math inline">\(D_1 f(u,v)\)</span>. This approach is <em>only</em> sensible when the symbolic label unambiguously refers to an argument of <span class="math inline">\(f\)</span>, and we must treat it carefully. When there is a danger of ambiguity, we will bend our notation to make clear that we are labeling a slot. For example, we can write <span class="math inline">\(D_u f(u=a, v=b)\)</span> to refer to <span class="math inline">\(D_1 f(a,b)\)</span>.</p>
</section>
<section id="sec-calculus-implicit-fn" class="level3" data-number="5.4.5">
<h3 data-number="5.4.5" class="anchored" data-anchor-id="sec-calculus-implicit-fn"><span class="header-section-number">5.4.5</span> Implicit functions</h3>
<p>Now suppose <span class="math inline">\(\mathcal{V}\)</span> and <span class="math inline">\(\mathcal{W}\)</span> are spaces of equal dimension, and consider a function <span class="math display">\[
  f : \mathcal{V}\times \mathcal{U}\rightarrow \mathcal{W}
\]</span> that is continuously differentiable on some open set including a point <span class="math inline">\((v,u)\)</span> for which <span class="math inline">\(f(v,u) = 0\)</span>. We will write the Frechet derivative of <span class="math inline">\(f\)</span> in quasimatrix form as <span class="math display">\[
  f'(v,u) = \begin{bmatrix} D_1 f(v,u) &amp; D_2 f(v,u) \end{bmatrix},
\]</span> where <span class="math inline">\(D_1 f\)</span> is the piece of the map associated with <span class="math inline">\(v\)</span> and <span class="math inline">\(D_2 f\)</span> is the piece associated with <span class="math inline">\(u\)</span>. If <span class="math inline">\(D_1 f(v,u)\)</span>) is invertible, the the <em>implicit function theorem</em> says that there exists a function <span class="math display">\[
  g : \Omega \subset \mathcal{U}\rightarrow \mathcal{V}
\]</span> defined on an open set containing <span class="math inline">\(u\)</span> and such that <span class="math inline">\(g(u) = v\)</span> and <span class="math inline">\(f(g(z), z) = 0\)</span> for <span class="math inline">\(z\)</span> in an open neighborhood about <span class="math inline">\(u\)</span>. By the chain rule, we must satisfy <span class="math display">\[
  D_1 f(g(z), z) g'(z) + D_2 f(g(z),z) = 0.
\]</span> Therefore, <span class="math inline">\(g'(z)\)</span> can be computed as <span class="math display">\[
  g'(z) = -\left( D_1 f(g,z) \right)^{-1} D_2 f(g,z),
\]</span> where we have suppressed <span class="math inline">\(z\)</span> as an argument to <span class="math inline">\(g\)</span> to simplify notation.</p>
<p>Often we are interested not in <span class="math inline">\(g(z)\)</span>, but some <span class="math inline">\(h(g(z))\)</span> that might lie in a much lower dimensional space. If everything is differentiable, then by the chain rule we have <span class="math display">\[
  (h \circ g)'(z)
  = -h'(g(z)) \left[ \left( D_1 f(g,z) \right)^{-1} D_2 f(g,z) \right].
\]</span> We can use associativity to rewrite <span class="math inline">\((h \circ g)'(z)\)</span> as <span class="math display">\[\begin{aligned}
  \bar{g}^* &amp;= -h'(g(z)) \left( D_1 f(g,z) \right)^{-1} \\
  (h \circ g)'(z) &amp;= \bar{g}^* D_2 f(g,z),
\end{aligned}\]</span> where <span class="math inline">\(\bar{g}^* \in \mathcal{V}^*\)</span> is a dual variable. Computing <span class="math inline">\(\bar{g}^*\)</span> is sometimes called an <em>adjoint solve</em>, since in an inner product space this is equivalent to solving a linear system with the adjint of the derivative operator: <span class="math display">\[
  \bar{g} = \left( D_1 f(g,z) \right)^{-*} \nabla h(g(z)).
\]</span></p>
</section>
<section id="variational-notation" class="level3" data-number="5.4.6">
<h3 data-number="5.4.6" class="anchored" data-anchor-id="variational-notation"><span class="header-section-number">5.4.6</span> Variational notation</h3>
<p>We will often use a concise notation for differential calculus sometimes called <em>variational notation</em> (as in “calculus of variations”). If <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(x\)</span> and <span class="math display">\[
  y = f(x)
\]</span> then in variational notation we would write <span class="math display">\[
  \delta y = f'(x) \, \delta x.
\]</span> Here <span class="math inline">\(\delta x\)</span> and <span class="math inline">\(\delta y\)</span> are read as “variation in <span class="math inline">\(x\)</span>” and “variation in <span class="math inline">\(y\)</span>,” and we describe the process of differentiating as “taking variations of <span class="math inline">\(y = f(x)\)</span>.” If we think of differentiable functions <span class="math inline">\(\tilde{x}(s)\)</span> and <span class="math inline">\(\tilde{y}(s)\)</span> with <span class="math inline">\(x = \tilde{x}(0)\)</span> and <span class="math inline">\(y = \tilde{y}(0)\)</span>, then <span class="math inline">\(\delta x = \tilde{x}'(0)\)</span> and <span class="math inline">\(\delta y = \tilde{y}'(0)\)</span>. Put differently, <span class="math display">\[\begin{aligned}
  \tilde{x}(\epsilon) &amp;= x + \epsilon \, \delta x + o(\epsilon), \\
  \tilde{y}(\epsilon) &amp;= y + \epsilon \, \delta y + o(\epsilon),
\end{aligned}\]</span> and similarly for other related quantities. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> have types that can be added, scaled, and multiplied together, we also have the standard rules <span class="math display">\[\begin{aligned}
\delta (x+y) &amp;= \delta x + \delta y \\
\delta (\alpha x) &amp;= \alpha \, \delta x \\
\delta (xy) &amp;= \delta x \, y + x \, \delta y.
\end{aligned}\]</span> This last is derived in the way that we usually do: <span class="math display">\[
  (x + \epsilon \, \delta x + o(\epsilon))
  (y + \epsilon \, \delta y + o(\epsilon)) =
  xy + \epsilon(\delta x \, y + x \, \delta y) + o(\epsilon).
\]</span> We note that this is true even for things like linear maps of the right types, which can be multiplied but do not commute.</p>
<p>Variational notation is sometimes tidier than other ways of writing derivative relationships. For example, suppose <span class="math inline">\(A \in L(\mathcal{V}, \mathcal{V})\)</span> is an invertible linear map and we are interested in the variation in <span class="math inline">\(B = A^{-1}\)</span> with respect to the variation in <span class="math inline">\(A\)</span>. We can see <span class="math inline">\(B\)</span> as a differentiable function of <span class="math inline">\(A\)</span> by the implicit function theorem on the equation <span class="math display">\[
  AB-I = 0.
\]</span> Taking variations of this relationship, we have <span class="math display">\[
  (\delta A) B + A (\delta B) = 0,
\]</span> which we can rearrange to <span class="math display">\[
  \delta B = -A^{-1} (\delta A) A^{-1}.
\]</span> The formula for taking variations of <span class="math inline">\(A^{-1}\)</span> generalizes the 1D formula <span class="math inline">\((x^{-1})' = -x^{-2}\)</span>, which is often used as a first example of implicit differentiation in introductory calculus courses. A short computation verifies that our formula agrees with a finite difference estimate for a small example.</p>
<div id="1ef0dd2c" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span>  [ <span class="fl">94.0</span>  <span class="fl">29.0</span>  <span class="fl">26.0</span> ;</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>           <span class="fl">65.0</span>  <span class="fl">25.0</span>  <span class="fl">66.0</span> ;</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>           <span class="fl">85.0</span>  <span class="fl">92.0</span>  <span class="fl">72.0</span> ]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    δA <span class="op">=</span> [ <span class="fl">57.0</span>  <span class="fl">15.0</span>  <span class="fl">46.0</span> ;</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>           <span class="fl">56.0</span>  <span class="fl">44.0</span>  <span class="fl">79.0</span> ;</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>           <span class="fl">42.0</span>  <span class="fl">45.0</span>  <span class="fl">48.0</span> ]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    δB <span class="op">=</span> <span class="op">-</span>A<span class="op">\</span>δA<span class="op">/</span>A</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    δB_fd <span class="op">=</span> <span class="fu">finite_diff</span>(inv, A, δA)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">isapprox</span>(δB, δB_fd, rtol<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>true</code></pre>
</div>
</div>
<p>We <em>could</em> work out the same formula with the notation introduced in the previous sections, but the version involving variational notation is arguably easier to read and certainly shorter to write.</p>
</section>
<section id="sec-calculus-adjoint" class="level3" data-number="5.4.7">
<h3 data-number="5.4.7" class="anchored" data-anchor-id="sec-calculus-adjoint"><span class="header-section-number">5.4.7</span> Adjoints</h3>
<p>Consider a map <span class="math inline">\(\mu\)</span> taking an vector input <span class="math inline">\(x\)</span> to an scalar output <span class="math inline">\(y\)</span> via a vector intermediates <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>, written in terms of the relations <span class="math display">\[\begin{aligned}
  u &amp;= f(x) \\
  v &amp;= g(x, u) \\
  y &amp;= h(x, u, v)
\end{aligned}\]</span> Then we have the derivative relationships <span class="math display">\[\begin{aligned}
  \delta u &amp;= D_1 f \, \delta x \\
  \delta v &amp;= D_1 g \, \delta x + D_2 g \, \delta u \\
  \delta y &amp;= D_1 h \, \delta y + D_2 g \, \delta u + D_3 g \, \delta v
\end{aligned}\]</span> We can rearrange this into the (quasi)matrix form <span class="math display">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
-D_1 f &amp; 1 &amp; 0 &amp; 0 \\
-D_1 g &amp;  -D_2 g &amp; 1 &amp; 0 \\
-D_1 h &amp;  -D_2 h &amp; -D_3 h &amp; 1
\end{bmatrix}
\begin{bmatrix} \delta x \\ \delta u \\ \delta v \\ \delta y \end{bmatrix} =
\begin{bmatrix} I \\ 0 \\ 0 \\ 0 \end{bmatrix} \delta x.
\]</span> That is, we the computation of <span class="math inline">\(\delta y\)</span> and then <span class="math inline">\(\delta z\)</span> from <span class="math inline">\(\delta x\)</span> as an example of <em>forward substitution</em> for a 4-by-4 linear system.</p>
<p>If we truly only care about the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, there is no reason why we should explicitly compute the intermediate <span class="math inline">\(\delta y\)</span>. An equally good way to solve the problem is to solve the <em>adjoint equation</em> discussed in <a href="#sec-calculus-implicit-fn" class="quarto-xref"><span>Section 5.4.5</span></a>: <span class="math display">\[
\begin{bmatrix} \bar{x}^* &amp; \bar{u}^* &amp; \bar{v}^* &amp; \bar{y}^* \end{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
-D_1 f &amp; 1 &amp; 0 &amp; 0 \\
-D_1 g &amp;  -D_2 g &amp; 1 &amp; 0 \\
-D_1 h &amp;  -D_2 h &amp; -D_3 h &amp; 1
\end{bmatrix} =
\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}.
\]</span> via <em>backward substitution</em> on the 4-by-4 linear system: <span class="math display">\[\begin{aligned}
  \bar{y}^* &amp;= 1 \\
  \bar{v}^* &amp;= \bar{y}^* D_3 h \\
  \bar{u}^* &amp;= \bar{v}^* D_2 g + \bar{y}^* D_2 h \\
  \bar{x}^* &amp;= \bar{u}^* D_1 h + \bar{v}^* D_1 g + \bar{y}^* D_1 h.
\end{aligned}\]</span> We observe that <span class="math display">\[
  \delta y = \bar{x}^* \, \delta x;
\]</span> that is, <span class="math inline">\(\bar{x}^*\)</span> is the derivative of the map from <span class="math inline">\(x\)</span> to <span class="math inline">\(z\)</span>.</p>
<p>While the variations <span class="math inline">\(\delta y\)</span> and <span class="math inline">\(\delta z\)</span> lie in the same space as <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span>, the dual variables <span class="math inline">\(\bar{u}^*, \bar{v}^*, \bar{y}^*\)</span> (also called adjoint variables) live in the associated dual spaces. The dual variables can also be interpreted as the sensitivity of the output <span class="math inline">\(y\)</span> to the associated primary variable, <em>assuming earlier variables were held constant</em>.</p>
<p>The idea of using dual variables to compute derivatives of a function with many intermediates (rather using variations as intermediate quantities) is the basis of <em>adjoint mode</em> or <em>reverse mode</em> automatic differentiation, which we will discuss further in <a href="../01-Fund1d/04-AutoDiff.html" class="quarto-xref"><span>Chapter 11</span></a>.</p>
</section>
<section id="higher-derivatives" class="level3" data-number="5.4.8">
<h3 data-number="5.4.8" class="anchored" data-anchor-id="higher-derivatives"><span class="header-section-number">5.4.8</span> Higher derivatives</h3>
<section id="maps-from-mathbbr-to-mathbbr" class="level4" data-number="5.4.8.1">
<h4 data-number="5.4.8.1" class="anchored" data-anchor-id="maps-from-mathbbr-to-mathbbr"><span class="header-section-number">5.4.8.1</span> Maps from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span></h4>
<p>A main reason — if not <em>the</em> main reason — why we care about higher derivatives is because of their role in approximation.</p>
<p>As a particular starting point, consider <span class="math inline">\(g : \Omega \subset \mathbb{R}
\rightarrow \mathbb{R}\)</span> on an interval <span class="math inline">\(\Omega\)</span>. Assuming <span class="math inline">\(g\)</span> has at least <span class="math inline">\(k+1\)</span> continuous derivatives (i.e.&nbsp;<span class="math inline">\(g \in C^{k+1}\)</span>), then Taylor’s theorem with integral remainder gives <span class="math display">\[
  g(t) = \sum_{j=0}^{k} \frac{1}{j!} g^{(j)}(t) + r_{k+1}(t),
\]</span> where <span class="math display">\[
  r_{k+1}(t) = \int_0^t \frac{1}{k!} (t-s)^{k} g^{(k+1)}(s) \, ds.
\]</span> This formula can be verified with integration by parts. By the mean value theorem, there is some <span class="math inline">\(\xi \in (0,t)\)</span> such that <span class="math display">\[
  r_{k+1}(t) = \frac{t^{k+1}}{(k+1)!} g^{(k+1)}(\xi).
\]</span> This is the mean value form of the remainder.</p>
<p>We will sometimes work with slightly less regular functions, e.g.&nbsp;<span class="math inline">\(g : \mathbb{R}\rightarrow \mathbb{R}\)</span> with <span class="math inline">\(k\)</span> continuous derivatives and <span class="math inline">\(g^{(k)}\)</span> Lipschitz with constant <span class="math inline">\(C\)</span> (which implies absolute continuity of <span class="math inline">\(g^{(k)}\)</span>). In this case, we do not have the mean value form of the remainder, but can still use the integral form to get the bound <span class="math display">\[
  |r_{k+1}(t)| \leq \frac{C|t|^{k+1}}{(k+1)!}.
\]</span> Even for <span class="math inline">\(C^{k+1}\)</span> functions, these types of bounds are sometimes easier to work with than the mean value form of the remainder.</p>
</section>
<section id="maps-from-mathcalv-to-mathbbr" class="level4" data-number="5.4.8.2">
<h4 data-number="5.4.8.2" class="anchored" data-anchor-id="maps-from-mathcalv-to-mathbbr"><span class="header-section-number">5.4.8.2</span> Maps from <span class="math inline">\(\mathcal{V}\)</span> to <span class="math inline">\(\mathbb{R}\)</span></h4>
<p>If <span class="math inline">\(f : \mathcal{V}\rightarrow \mathbb{R}\)</span> is differentiable in an open set <span class="math inline">\(\Omega\)</span> containing <span class="math inline">\(x\)</span>, the Frechet derivative is a function <span class="math display">\[
  f' : \mathcal{V}\rightarrow \mathcal{V}^* = L(\mathcal{V}, \mathbb{R})
\]</span> where <span class="math inline">\(f'(x) \in \mathcal{V}^*\)</span> maps directions to directional derivatives. If <span class="math inline">\(f'\)</span> is differentiable, then <span class="math display">\[
  f'' : \mathcal{V}\rightarrow L(\mathcal{V}, \mathcal{V}^*).
\]</span> The mappings <span class="math inline">\(f''(x)\)</span> from <span class="math inline">\(\mathcal{V}\)</span> to <span class="math inline">\(\mathcal{V}^*\)</span> can also be thought of as a bilinear form from two vectors in <span class="math inline">\(\mathcal{V}\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, i.e. <span class="math inline">\((u, v) \mapsto (f''(x) u) v\)</span>. Hence, we write <span class="math display">\[
  f'' : \mathcal{V}\rightarrow L(\mathcal{V}\otimes \mathcal{V}, \mathbb{R}).
\]</span> When <span class="math inline">\(f''\)</span> is continuously defined in some neighborhood of <span class="math inline">\(x\)</span>, the bilinear form is also guaranteed to be symmetric, i.e. <span class="math display">\[
  f''(x) \, (u \otimes v) = f''(x) (v \otimes u).
\]</span> As discussed in <a href="03-LA.html" class="quarto-xref"><span>Chapter 4</span></a>, there is a 1-1 correspondence between symmetric bilinear forms and quadratic forms, and we will mostly be interested in <span class="math inline">\(f''(x)\)</span> as representing the quadratic <span class="math inline">\(u \mapsto f''(x) \, (u \otimes u)\)</span>.</p>
<p>The matrix representation of <span class="math inline">\(f''(x)\)</span> with respect to a particular basis <span class="math inline">\(V\)</span> is called the <em>Hessian</em> matrix, and is sometimes written <span class="math inline">\(H_f(x)\)</span>. The entries of this matrix are <span class="math inline">\((H_f(x))_{ij} = f''(x) (v_i \otimes v_j)\)</span>, so that we write evaluation of the Hessian on a pair of vectors as <span class="math display">\[
  f''(x) (Vc \otimes Vd) = d^* H_f(x) c.
\]</span> In most cases, we are interested in evaluating the quadratic form associated with <span class="math inline">\(f''(x)\)</span>, i.e. <span class="math display">\[
  f''(x) (Vc \otimes Vc) = c^* H_f(x) c.
\]</span> Hessian matrices will play a central role in our discussion of numerical optimization methods.</p>
<p>In thinking about first derivatives, we started by considering differentiation along a straight ray. We will do the same thing when thinking about using second derivatives. Suppose <span class="math inline">\(c_{x,u}(s) = x+su\)</span> and <span class="math inline">\(c_{x,u}([0,t]) \in \Omega\)</span>, and let <span class="math inline">\(g = f \circ c_{x,u}\)</span>. Suppose <span class="math inline">\(f \in C^2(\Omega)\)</span> where <span class="math inline">\(f''\)</span> is Lipschitz with constant <span class="math inline">\(M\)</span> (with respect to an induced norm); that is: <span class="math display">\[\begin{aligned}
  \|f''(x)\| &amp;= \max_{\|w\|=1} |f'(x) (w \otimes w)| \\
  \|f''(x)-f''(y)\| &amp;\leq M \|x-y\|.
\end{aligned}\]</span> Then Taylor’s theorem with remainder gives <span class="math display">\[
  g(t) = g(0) + g'(0) t + \frac{1}{2} g''(0) t^2 + r(t),
  \quad |r(t)| \leq \frac{M \|u\|^3}{6} t^3.
\]</span> where <span class="math display">\[\begin{aligned}
  g'(0) &amp;= f'(0) u \\
  g''(0) &amp;= f''(0) \, (u \otimes u).
\end{aligned}\]</span> We usually skip the intermediate function <span class="math inline">\(g\)</span>, and write that when <span class="math inline">\(f''\)</span> is Lipschitz and <span class="math inline">\(\Omega\)</span> is convex (so that the line segment connecting <span class="math inline">\(x, u \in \Omega\)</span> will lie entirely within <span class="math inline">\(\Omega\)</span>) then <span class="math display">\[
  f(x+u) = f(x) + f'(x) u + \frac{1}{2} f''(x) (u \otimes u) + O(\|u\|^3),
\]</span> where we are only dealing with the asymptotics of the third-order term. When dealing with a concrete space <span class="math inline">\(\mathbb{R}^n\)</span> (or when working in terms of a basis for <span class="math inline">\(\mathcal{V}\)</span>), we usually write <span class="math display">\[
  f(x+u) = f(x) + f'(x) u + \frac{1}{2} u^* H_f(x) u + O(\|u\|^3).
\]</span></p>
<p>One can continue to take higher derivatives in a similar manner — e.g., if <span class="math inline">\(f''\)</span> is differentiable, then <span class="math inline">\(f'''(x)\)</span> is a trilinear form in <span class="math inline">\(L(\mathcal{V}\otimes \mathcal{V}\otimes \mathcal{V}, \mathbb{R})\)</span>, and we represent it with respect to a basis <span class="math inline">\(V\)</span> as a concrete tensor with entries that we usually write as <span class="math inline">\(f_{,ijk} = f'''(x) \, (v_i \otimes v_j \otimes v_k)\)</span>. In the rare cases when we use more than second derivatives, we often revert to indicial notation for computing with functions on concrete spaces; with the summation convention in effect, the Taylor series becomes <span class="math display">\[
  f(x + u) = f(x) + f_{,i}(x) u_i + \frac{1}{2} f_{,ij}(x) u_i u_j +
  \frac{1}{6} f_{,ijk}(x) u_i u_j u_k + \ldots.
\]</span> The size and notational complexity goes up significantly as we move beyond second derivatives, enough so that we avoid higher derivatives if we can.</p>
</section>
<section id="general-maps" class="level4" data-number="5.4.8.3">
<h4 data-number="5.4.8.3" class="anchored" data-anchor-id="general-maps"><span class="header-section-number">5.4.8.3</span> General maps</h4>
<p>We now consider the case of <span class="math inline">\(f : \mathcal{V}\rightarrow \mathcal{W}\)</span>. If <span class="math inline">\(f\)</span> is <span class="math inline">\(C^2\)</span> on <span class="math inline">\(\Omega\)</span>, then the second derivative is <span class="math display">\[
  f'' : \mathcal{V}\rightarrow
  L(\mathcal{V}, L(\mathcal{V}, \mathcal{W})) \equiv L(\mathcal{V}\otimes \mathcal{V}, \mathcal{W}),
\]</span> and the bilinear map <span class="math inline">\(f''(x)\)</span> is symmetric in the arguments, i.e. <span class="math display">\[
  f''(x) \, (u \otimes v) = f''(x) \, (v \otimes u).
\]</span> Expanding to second order with remainder again looks like <span class="math display">\[
  f(x+u) = f(x) + f'(x) u + \frac{1}{2} f''(x) (u \otimes u) + O(\|u^3\|).
\]</span> But when we want to compute with concrete quantities, we need three indices even to keep track of the second derivative term: with respect to bases <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, we write the concrete tensor <span class="math display">\[
  f_{i,jk} = \left[ W^{-1} f(x) (v_j \otimes v_k) \right]_i
\]</span> or, equivalently (using indices and the summation convention), <span class="math display">\[
  f(x) (Vc \otimes Vd) = \mathbf{w}_i f_{i,jk} c_j d_k.
\]</span></p>
</section>
</section>
<section id="analyticity" class="level3" data-number="5.4.9">
<h3 data-number="5.4.9" class="anchored" data-anchor-id="analyticity"><span class="header-section-number">5.4.9</span> Analyticity</h3>
<p>So far, we have considered functions on real vector spaces. But for some of what we have to say about matrix calculus later, it will be useful to also consider functions on the complex plane.</p>
<p>We begin with an algebra fact. Suppose that <span class="math inline">\(z = x+iy\)</span> and <span class="math inline">\(c = a+ib\)</span> are complex numbers written in rectangular form; then <span class="math display">\[
  cz = \begin{bmatrix} 1 &amp; i \end{bmatrix}
       \begin{bmatrix} a &amp; -b \\ b &amp; a \end{bmatrix}
       \begin{bmatrix} x \\ y \end{bmatrix}.
\]</span> That is, the matrix mapping the real and imaginary components of <span class="math inline">\(z\)</span> to the real and imaginary components of <span class="math inline">\(cz\)</span> has the form <span class="math inline">\(aI + bJ\)</span> where <span class="math inline">\(I\)</span> is the identity and <span class="math inline">\(J\)</span> is the 2-by-2 skew matrix <span class="math display">\[
  J = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}.
\]</span> Hence, we can interpret complex multiplication as a 2-by-2 matrix on the rectangular coordinate parameterization of the reals — but it is a matrix of a very specific type.</p>
<p>Now let <span class="math inline">\(f : \mathbb{C}\rightarrow \mathbb{C}\)</span>. We would like to say that <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(z\)</span> if <span class="math display">\[
  f(z+w) = f(z) + f'(z) w + o(|w|).
\]</span> Phrased in terms of rectangular coordinates (<span class="math inline">\(f = g+ih\)</span>, <span class="math inline">\(z = x+iy\)</span>, <span class="math inline">\(w = u+iv\)</span>), we have that <span class="math display">\[
  \begin{bmatrix}
    D_1 g(x,y) &amp; D_2 g(x,y) \\
    D_1 h(x,y) &amp; D_2 h(x,y)
  \end{bmatrix} =
  \begin{bmatrix}
    a(x,y) &amp; -b(x,y) \\
    b(x,y) &amp;  a(x,y)
  \end{bmatrix}
\]</span> i.e.&nbsp;the partial derivatives of the real and imaginary components of <span class="math inline">\(f\)</span> must satisfy the <em>Cauchy-Riemann</em> equations <span class="math display">\[
  D_1 g = D_2 h, \quad D_2 g = -D_1 h.
\]</span> A function that is complex differentiable on an open set <span class="math inline">\(\Omega\)</span> is called <em>holomorphic</em> (or <em>(complex) analytic</em>) on that set.</p>
<p>Holomorphic functions are about as nice as functions can be. Among other nice properties:</p>
<ul>
<li>They are automatically infinitely (complex) differentiable,</li>
<li>They have convergent Taylor series about any point in the domain <span class="math inline">\(\Omega\)</span>,</li>
<li>The real and imaginary components are harmonic functions (i.e.&nbsp;<span class="math inline">\(D_1^2
g + D_2^2 g = 0\)</span>, and similarly for <span class="math inline">\(h\)</span>), and</li>
<li>Contour integration around any loop <span class="math inline">\(\Gamma\)</span> whose interior is wholly within <span class="math inline">\(\Omega\)</span> gives <span class="math inline">\(\int_\Gamma f(z) \, dz = 0\)</span>.</li>
</ul>
<p>We will heavily rely on this last fact (and related facts) when dealing with the contour integral view of functions of matrices in <a href="#sec-calculus-contour-int" class="quarto-xref"><span>Section 5.7</span></a>.</p>
</section>
</section>
<section id="series" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="series"><span class="header-section-number">5.5</span> Series</h2>
<p>A <em>series</em> is an infinite sequence of terms <span class="math inline">\(v_1, v_2, \ldots\)</span> that are (formally) added together. When students first learn about series in introductory calculus courses, the terms <span class="math inline">\(v_j\)</span> are typically real or complex numbers. We will consider the more general case where the <span class="math inline">\(v_j\)</span> are drawn from some normed vector space <span class="math inline">\(\mathcal{V}\)</span>, which we will often assume is complete (a Banach space). The partial sums are <span class="math display">\[
  S_n = \sum_{j=1}^n v_j;
\]</span> and when <span class="math inline">\(\mathcal{V}\)</span> is a Banach space and the partial sums form a Cauchy sequence, we say the series <em>converges</em> to <span class="math display">\[
  S = \sum_{j=1}^\infty v_j = \lim_{n \rightarrow \infty} S_n.
\]</span> Series that do not converge (divergent series) are still useful in many situations; we simply need to understand that they are to be treated formally, and we might not be able to make sense of the limiting value.</p>
<section id="convergence-tests" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="convergence-tests"><span class="header-section-number">5.5.1</span> Convergence tests</h3>
<p>Many of the standard convergence tests for series in <span class="math inline">\(\mathbb{R}\)</span> or <span class="math inline">\(\mathbb{C}\)</span> generalize to series in a Banach space. The <em>comparison test</em> is of particular use:</p>
<div id="thm-comparison-test" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.2 (Comparison test)</strong></span> Suppose <span class="math inline">\(\{v_j\}_{j=1}^\infty\)</span> is a sequence in a Banach space <span class="math inline">\(\mathcal{V}\)</span>, where the terms are bounded by <span class="math inline">\(\|v_j\| \leq a_j\)</span>. Suppose <span class="math inline">\(\sum_{j=1}^\infty a_j\)</span> converges. Then <span class="math inline">\(\sum_{j=1}^\infty v_j\)</span> also converges absolutely; moreover, <span class="math display">\[
  \left\| \sum_{j=1}^\infty v_j \right\| \leq \sum_{j=1}^\infty a_j.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(A_n\)</span> and <span class="math inline">\(A\)</span> denote the <span class="math inline">\(n\)</span>th partial sum and the limit for the series <span class="math inline">\(\sum_{j=1}^\infty a_j\)</span>, and let <span class="math inline">\(S_n\)</span> denote the <span class="math inline">\(n\)</span>th partial sum for the series <span class="math inline">\(\sum_{j=1}^\infty v_j\)</span>. By the triangle inequality, for any <span class="math inline">\(i \geq j\)</span>, we have <span class="math display">\[
  \|S_j-S_i\| = \left\| \sum_{k=i+1}^j v_k \right\| \leq
  \sum_{k=i+1}^j \|v_k\| \leq A_j-A_i.
\]</span> The fact that the partial sums <span class="math inline">\(A_n\)</span> form a Cauchy sequence means that for any <span class="math inline">\(\epsilon &gt; 0\)</span>, there exists <span class="math inline">\(N\)</span> such that if <span class="math inline">\(i \geq j \geq N\)</span>, <span class="math display">\[
  |A_j-A_i| &lt; \epsilon;
\]</span> and because <span class="math inline">\(\|S_j-S_i\| \leq |A_j-A_i|\)</span>, the partial sums <span class="math inline">\(S_n\)</span> also form a Cauchy sequence, and therefore converge to a limit <span class="math inline">\(S\)</span>.</p>
<p>The final bound comes from applying the triangle inequality to show <span class="math display">\[
  \|S_n\|
  = \left\| \sum_{j=1}^n v_j \right\|
  \leq \sum_{j=1}^n \|v_j\|
  \leq A_n,
\]</span> and from there arguing that the limit <span class="math inline">\(\|S\|\)</span> is bounded by the limit <span class="math inline">\(A\)</span>.</p>
</div>
<p>Many other convergence tests similarly generalize to series in Banach spaces, often using the comparison test as a building block. An example is the ratio test:</p>
<div id="thm-ratio-test" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.3 (Ratio test)</strong></span> Suppose <span class="math inline">\(\{v_j\}_{j=1}^\infty\)</span> is a sequence in a Banach space <span class="math inline">\(\mathcal{V}\)</span>, where <span class="math inline">\(\lim_{n\rightarrow \infty} \|v_{n+1}\|/\|v_n\| = r &lt; 1\)</span>. Then <span class="math inline">\(\sum_{j=1}^\infty v_j\)</span> converges absolutely.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(r &lt; r' &lt; 1\)</span> (e.g.&nbsp;choose <span class="math inline">\(r' = (r+1)/2\)</span>). Then by the hypothesis, there exists some <span class="math inline">\(N\)</span> such that for all <span class="math inline">\(n \geq N\)</span>, <span class="math inline">\(\|v_{n+1}\|/\|v_n\| \leq r'\)</span>. Hence, for <span class="math inline">\(n \geq N\)</span> we have <span class="math inline">\(\|v_n\| \leq (r')^{n-N} \|v_N\|\)</span>, and by the comparison test and convergence of the geometric series, the series <span class="math inline">\(\sum_j v_j\)</span> converges, and moreover we have the bound <span class="math display">\[\begin{aligned}
  \left\| \sum_{n=1}^\infty v_n \right\|
  &amp;\leq \left\| \sum_{n=1}^{N-1} v_n \right\| + \sum_{n=N}^\infty
  (r')^{n-N} \|v_N\| \\
  &amp;= \left\| \sum_{n=1}^{N-1} v_n \right\| + \frac{\|v_N\|}{1-r'}.
\end{aligned}\]</span></p>
</div>
</section>
<section id="function-series" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="function-series"><span class="header-section-number">5.5.2</span> Function series</h3>
<p>Frequently, we are interested in series where the terms are functions of some variable, e.g.&nbsp;<span class="math inline">\(v_j : \Omega \subset \mathbb{C}\rightarrow \mathcal{V}\)</span> for some Banach space <span class="math inline">\(\mathcal{V}\)</span>. Often the functional dependence of the terms on <span class="math inline">\(v_j\)</span> is fairly simple, e.g.&nbsp;<span class="math inline">\(v_j(z) = \bar{v}_j z^{p_j}\)</span> where the exponents <span class="math inline">\(p_j\)</span> may be just positive integers (a power series), positive and negative integers (a Laurent series), or fractions with a fixed denominator (a Puiseaux series). We also often see terms of the form <span class="math inline">\(v_n(z) = \bar{v}_n \exp(inz)\)</span> (a Fourier series).</p>
<p>When the terms themselves belong to some Banach space, we can analyze the convergence or divergence of the series as a whole just as we would analyze the convergence or divergence of any other series over a Banach space. For example, if <span class="math inline">\(\Omega\)</span> is compact, then the set of continuous functions <span class="math inline">\(C(\Omega, \mathcal{V})\)</span> (which are automatically uniformly continuous) is a Banach space. Convergence in this Banach space corresponds to uniform convergence of the functions, and uniform convergence of uniformly continuous functions gives a uniformly continuous limit, as noted before.</p>
<p>However, more generally sometimes the terms may not belong to a Banach space; or if they do belong to a Banach space, they might diverge. In this case, we might care about the convergence or divergence of the series <span class="math inline">\(\sum_{j=1}^\infty v_j(z)\)</span> for <em>specific</em> values of <span class="math inline">\(z\)</span>; and, if we have pointwise convergence over some subset <span class="math inline">\(\Omega' \subset
\Omega\)</span>, we care what properties the limiting function <span class="math inline">\(S : \Omega'
\rightarrow \mathcal{V}\)</span> might inherit from the terms. This issue can be subtle: for example, a Fourier series can easily converge at almost all points in some interval, but converge to a discontinuous function despite the fact that all the terms are infinitely differentiable.</p>
<p>Function series can also be extremely useful even where they diverge, both for formal manipulation and for actual calculations where the approximation properties of finite sums are more important than the limiting behavior of the series.</p>
</section>
<section id="formal-series" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="formal-series"><span class="header-section-number">5.5.3</span> Formal series</h3>
<p>A <em>formal power series</em> is a series of the form <span class="math display">\[
  S(z) = \sum_{j=0}^\infty a_j z^j
\]</span> where the coefficients <span class="math inline">\(a_j\)</span> belong to some appropriate vector space <span class="math inline">\(\mathcal{V}\)</span>. In some circumstances, it also makes sense to include negative powers, in which case we sometimes refer to this as a formal Laurent series. Formal power series appear in a variety of applications. We will mostly see them in signal processing, where such a formal power series is also called the <span class="math inline">\(z\)</span>-transform of the sequence of coefficients <span class="math inline">\(a_j\)</span>. In that setting, is it particularly useful not only to add and scale formal power series, but also to multiply them together using the rule <span class="math display">\[
  \left( \sum_{j=0}^\infty a_j z^j \right)
  \left( \sum_{j=0}^\infty b_j z^j \right) =
  \sum_{k=0}^\infty \left( \sum_{i+j=k} a_i b_j \right) z^k;
\]</span> that is, the coefficients in the product are the <em>convolution</em> of the coefficient sequences of the multiplicand series. Hence, the power series can be seen as a convenient way of dealing with the book-keeping of tracking sequences and their convolutions, independent of whether things converge. Formal power series (along with other formal function series) also play a key role in combinatorics and statistics, often under the name of “generating functions” (see <span class="citation" data-cites="wilf-2006">Wilf (<a href="../references.html#ref-wilf-2006" role="doc-biblioref">2006</a>)</span>).</p>
<p>If <span class="math inline">\(\mathcal{V}\)</span> is a Banach space and <span class="math inline">\(\log \|a_n\| = O(n)\)</span>, the ordinary formal power series associated with the coefficient sequence <span class="math inline">\(a_j\)</span> will converge for values of <span class="math inline">\(z \in \mathbb{C}\)</span> close enough to zero. Under such assumptions (which often hold), we can treat the formal power series as defining a function, and can bring the machinery of analytic function theory to bear. Nonetheless, it is helpful to distinguish the formal power series — essentially a trick for indexing elements of a series — from the associated function to which it converges. Only the former is needed for tracking convolutions and the like.</p>
</section>
<section id="asymptotic-series" class="level3" data-number="5.5.4">
<h3 data-number="5.5.4" class="anchored" data-anchor-id="asymptotic-series"><span class="header-section-number">5.5.4</span> Asymptotic series</h3>
<p>An <em>asymptotic series</em> for a function <span class="math inline">\(f\)</span> is a formal function series with terms <span class="math inline">\(v_n(z)\)</span> such that <span class="math display">\[
  f(z) - \sum_{n=0}^{N-1} v_n(z) = o(\|\phi_{N-1}(z)\|)
\]</span> for some limiting behavior in <span class="math inline">\(z\)</span> (usually <span class="math inline">\(z \rightarrow 0\)</span> or <span class="math inline">\(z
\rightarrow \infty\)</span>). Asymptotic series often do not converge, but are nonetheless useful for approximation.</p>
<p>A standard example of an asymptotic series is Stirling’s approximation for the gamma function (generalized factorial function): <span class="math display">\[
  \Gamma(z+1) \sim
  \sqrt{2\pi z} \left( \frac{z}{e} \right)^z \left( 1 + \frac{1}{12}
  z^{-1} + \frac{1}{288} z^{-2} - \frac{139}{51840} z^{-3} - O(z^{-4}) \right).
\]</span> Here the symbol <span class="math inline">\(\sim\)</span> is used to denote that the series is asymptotic to the function, but does not necessarily converge to the function in the limit of an infinite number of terms. Another standard example is the asymptotic expansion for the tail of the standard normal cdf: <span class="math display">\[
  1 - \Phi(z) = \phi(z) z^{-1} \left( 1 - z^{-2} + 3z^{-4} + O(z^{-6}) \right).
\]</span> Both these methods are derived via Laplace’s method, which is also very useful for approximating integrals arising in Bayesian inference. These types of asymptotic approximations are typically complementary to numerical methods, providing very accurate estimate precisely in the regions where more conventional numerical approaches have the most difficulty.</p>
</section>
<section id="analytic-functions" class="level3" data-number="5.5.5">
<h3 data-number="5.5.5" class="anchored" data-anchor-id="analytic-functions"><span class="header-section-number">5.5.5</span> Analytic functions</h3>
<p>While formal series and asymptotic approximation are nice, even nicer things can happen for power series that converge. When a series <span class="math display">\[
  f(z) = \sum_{j=0}^\infty c_j (z-z_0)^j
\]</span> converges in a neighborhood of <span class="math inline">\(z_0\)</span> (in <span class="math inline">\(\mathbb{R}\)</span> or <span class="math inline">\(\mathbb{C}\)</span>), we say the function <span class="math inline">\(f\)</span> is <em>analytic</em> at <span class="math inline">\(z_0\)</span>. Such a function is automatically infinitely differentiable in a neighborhood of <span class="math inline">\(z_0\)</span>. However, there are (useful) examples of non-analytic functions on <span class="math inline">\(\mathbb{R}\)</span> that are infinitely differentiable, e.g. <span class="math display">\[
  g(x) =
  \begin{cases}
    \exp(-1/x), &amp; x \geq 0 \\
    0 &amp; \mbox{otherwise}
  \end{cases}
\]</span></p>
<p>Complex analyticity around a point <span class="math inline">\(z_0 \in \mathbb{C}\)</span> (existence of a convergent Taylor series) and the property of being <em>holomorphic</em> around a point (having complex derivatives that satisfy the Cauchy-Riemann equations) are equivalent to each other. Functions that are real analytic in the neighborhood of a point in <span class="math inline">\(\mathbb{R}\)</span> are also complex analytic in a neighborhood. However, a function that is real analytic over all of <span class="math inline">\(\mathbb{R}\)</span> may not extend to a function that is complex analytic over all of <span class="math inline">\(\mathbb{C}\)</span>.</p>
<p>The property of analyticity at a point <span class="math inline">\(z_0\)</span> is closed under addition, scaling, multiplication of functions, integration, differentiation, and composition. It is also closed under division, assuming that the denominator function is nonzero at <span class="math inline">\(z_0\)</span>.</p>
</section>
<section id="operator-functions" class="level3" data-number="5.5.6">
<h3 data-number="5.5.6" class="anchored" data-anchor-id="operator-functions"><span class="header-section-number">5.5.6</span> Operator functions</h3>
<p>Just as we can compute powers of <span class="math inline">\(z \in \mathbb{C}\)</span>, we can compute powers of <span class="math inline">\(A \in L(\mathcal{V}, \mathcal{V})\)</span>. Indeed, as we noted in <span class="quarto-unresolved-ref">?sec-tbd</span>, polynomials of an operator are enormously useful both as a theoretical tool (e.g.&nbsp;for studying eigenvalues) and as a numerical tool (e.g.&nbsp;in the design of Krylov subspace methods, which we will see later in this book). Unsurprisingly, taking limits of sequences of polynomials is also useful, and power series expressions for analytic functions on <span class="math inline">\(\mathbb{C}\)</span> work equally well for describing analytic functions on operator spaces.</p>
<p>For our purposes, we will mostly consider <span class="math inline">\(\mathcal{V}\)</span> to be a finite-dimensional normed space, but the concept works equally well for general Banach spaces. In either case, the space of bounded operators <span class="math inline">\(L(\mathcal{V}, \mathcal{V})\)</span> is itself a Banach space, and our earlier comments about convergent series in general Banach spaces holds. For example, using a consistent family of norms (e.g.&nbsp;the operator norm), consider <span class="math display">\[
  \exp(A) = \sum_{k=0}^\infty \frac{1}{k!} A^k;
\]</span> by comparison to the series for <span class="math inline">\(\exp(\|A\|)\)</span>, this series converges absolutely, and <span class="math display">\[
  \|\exp(A)\| \leq \exp(\|A\|).
\]</span> We observe that if we have an eigenpair <span class="math inline">\(Av = v \lambda\)</span>, then <span class="math display">\[
  \exp(A) v
  = \sum_{k=0}^\infty \frac{1}{k!} A^k v
  = \sum_{k=0}^\infty \frac{1}{k!} v \lambda^k
  = v \exp(\lambda).
\]</span> When <span class="math inline">\(A\)</span> is diagonalizable (i.e.&nbsp;<span class="math inline">\(A = V \Lambda V^{-1}\)</span>), then we can completely characterize <span class="math inline">\(\exp(A)\)</span> as <span class="math display">\[
  \exp(A) = V \exp(\Lambda) V^{-1},
\]</span> where <span class="math inline">\(\exp(\Lambda)\)</span> is the diagonal matrix of exponentials of eigenvalues.</p>
<p>What works for the exponential tends to work for more general analytic functions <span class="math inline">\(f\)</span>. Things get a little more complicated for operators with nontrivial Jordan blocks (and certainly for the more elaborate spectra that can occur in the infinite-dimensional case), but the fundamental picture remains the same: an analytic function <span class="math inline">\(f\)</span> makes sense as a function of an operator when the spectrum is inside the domain of analyticity; and <span class="math inline">\(f(A)\)</span> will have the same eigenvector structure as <span class="math inline">\(A\)</span> while eigenvalues are transformed by the mapping <span class="math inline">\(\lambda \mapsto f(\lambda)\)</span>.</p>
</section>
<section id="neumann-series" class="level3" data-number="5.5.7">
<h3 data-number="5.5.7" class="anchored" data-anchor-id="neumann-series"><span class="header-section-number">5.5.7</span> Neumann series</h3>
<p>While the exponential provides a nice example of an operator power series, we will see far more of the <em>Neumann series</em>, which is the generalization of the geometric series. When <span class="math inline">\(\|A\| &lt; 1\)</span> under any consistent norm, we have that the Neumann series <span class="math display">\[
  (I-A)^{-1} = \sum_{k=0}^\infty A^k
\]</span> converges by comparison with a geometric series in <span class="math inline">\(\|A\|\)</span>; and (courtesy that same comparison), that <span class="math display">\[
  \|(I-A)^{-1}\| \leq \sum_{k=0}^\infty \|A\|^k = \frac{1}{1-\|A\|}.
\]</span> With a little supplementary algebra, this Neumann series bound is the basis for many inequalities that will prove useful later. As an example left as an exercise for the student, we observe that when <span class="math inline">\(A\)</span> is invertible and <span class="math inline">\(\|A^{-1} E\| &lt; 1\)</span>, we have <span class="math display">\[
\|(A+E)^{-1}\| \leq \frac{\|A^{-1}\|}{1-\|A^{-1} E\|}.
\]</span></p>
</section>
</section>
<section id="integration" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="integration"><span class="header-section-number">5.6</span> Integration</h2>
<section id="measure-and-integration" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="measure-and-integration"><span class="header-section-number">5.6.1</span> Measure and integration</h3>
</section>
<section id="standard-inequalities" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="standard-inequalities"><span class="header-section-number">5.6.2</span> Standard inequalities</h3>
</section>
<section id="change-of-variables" class="level3" data-number="5.6.3">
<h3 data-number="5.6.3" class="anchored" data-anchor-id="change-of-variables"><span class="header-section-number">5.6.3</span> Change of variables</h3>
</section>
</section>
<section id="sec-calculus-contour-int" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="sec-calculus-contour-int"><span class="header-section-number">5.7</span> Contour integrals</h2>
<section id="poles-and-residues" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="poles-and-residues"><span class="header-section-number">5.7.1</span> Poles and residues</h3>
</section>
<section id="resolvent-calculus" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="resolvent-calculus"><span class="header-section-number">5.7.2</span> Resolvent calculus</h3>
</section>
</section>
<section id="function-spaces" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="function-spaces"><span class="header-section-number">5.8</span> Function spaces</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-keynes-1923" class="csl-entry" role="listitem">
Keynes, John Maynard. 1929. <em>A Tract on Monetary Reform</em>. The Macmillan Company. <a href="https://archive.org/details/tractonmonetaryr0000keyn/mode/2up">https://archive.org/details/tractonmonetaryr0000keyn/mode/2up</a>.
</div>
<div id="ref-kline-1990" class="csl-entry" role="listitem">
Kline, M. 1990. <em>Mathematical Thought from Ancient to Modern Times</em>. Oxford University Press.
</div>
<div id="ref-rosenthal-1951" class="csl-entry" role="listitem">
Rosenthal, Arthur. 1951. <span>“The History of Calculus.”</span> <em>The American Mathematical Monthly</em> 58 (2): 75–86. <a href="https://doi.org/10.2307.2308368">https://doi.org/10.2307.2308368</a>.
</div>
<div id="ref-wilf-2006" class="csl-entry" role="listitem">
Wilf, Herbert S. 2006. <em>Generatingfunctionology</em>. Third. AK Peters, Ltd. <a href="https://www2.math.upenn.edu/~wilf/DownldGF.html">https://www2.math.upenn.edu/~wilf/DownldGF.html</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>“I turn away with fright and horror from this lamentable evil of functions which do not have derivatives.” – Hermite <span class="citation" data-cites="kline-1990">(according to <a href="../references.html#ref-kline-1990" role="doc-biblioref">Kline 1990, 973</a>)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>“But this long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task if in tempestuous seasons they can only tell us that when the storm is long past the ocean will be flat again.” — John Maynard Keynes, <span class="citation" data-cites="keynes-1923">(from p.80, <a href="../references.html#ref-keynes-1923" role="doc-biblioref">Keynes 1929</a>)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>A <em>topological space</em> is a set <span class="math inline">\(X\)</span> and a collection of sets (called the open sets) that include the empty set and all of <span class="math inline">\(X\)</span> and are closed under unions and finite intersections.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>In general topological spaces, compactness and sequential compactness are not the same thing. Fortunately, it’s all the same in metric space (assuming the axiom of choice).<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../00-Background/03-LA.html" class="pagination-link" aria-label="Linear Algebra">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../00-Background/05-Optimization.html" class="pagination-link" aria-label="Optimization theory">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Optimization theory</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>