<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Performance Basics – Numerical Methods for Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../00-Background/03-LA.html" rel="next">
<link href="../00-Background/01-Julia.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../00-Background/00-Intro.html">Background Plus a Bit</a></li><li class="breadcrumb-item"><a href="../00-Background/02-Performance.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Performance Basics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Numerical Methods for Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../00-Background/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Plus a Bit</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/01-Julia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Julia programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/02-Performance.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Performance Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/03-LA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/04-Calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Calculus and analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/05-Optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Optimization theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-Background/06-Probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../01-Fund1d/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fundamentals in 1D</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/01-Error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Notions of Error</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/02-Arithmetic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Floating Point</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/03-Approximation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Approximation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/04-AutoDiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic Differentiation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/05-NumDiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Numerical Differentiation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/06-Quadrature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quadrature</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/07-Roots-Opt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Root Finding and Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-Fund1d/08-Random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Computing with Randomness</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../02-NLA/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Numerical Linear Algebra</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/01-Systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Linear Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/02-Least-Squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/03-Eigenvalues.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Eigenvalue Problems and the SVD</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/04-Signals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Signals and Transforms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/05-Stationary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Stationary Iterations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-NLA/06-Krylov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Krylov Subspaces</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../03-Nonlinear/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nonlinear Equations and Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/01-Calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Calculus Revisited</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/02-Nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Nonlinear Equations and Unconstrained Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/03-Continuation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Continuation and Bifurcation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/04-Constrained.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Constrained Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-Nonlinear/05-Least-Squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Nonlinear Least Squares</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../04-Random/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computing with Randomness</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/01-Sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/02-Monte-Carlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Quadrature and Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/03-Solvers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Solvers from Monte Carlo to Las Vegas</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-Random/04-UQ.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../05-Reduction/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dimension Reduction and Latent Factor Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-Reduction/01-Matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Latent Factors and Matrix Factorization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-Reduction/02-Tensor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">From Matrices to Tensors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-Reduction/03-Nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Nonlinear Dimensionality Reduction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../06-Approximation/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Function Approximation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/01-Concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Fundamental Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/02-Low-Dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Low-Dimensional Structure</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/03-Kernels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Kernels and RBFs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-Approximation/04-NN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../07-Networks/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Network Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/01-Matrices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Graphs and Matrices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/02-Functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Functions on Graphs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/03-Cluster.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Clustering and Partitioning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Networks/04-Centrality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Centrality Measures</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../08-Dynamics/00-Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Learning Dynamics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/01-Fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Fundamentals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/02-Model-Reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Model Reduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/03-Linear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Learning Linear Dynamics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/04-Extrapolation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Extrapolation and Acceleration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/05-Koopman.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">From Markov to Koopman</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Dynamics/06-Nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Learning Nonlinear Dynamics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#time-to-what" id="toc-time-to-what" class="nav-link active" data-scroll-target="#time-to-what"><span class="header-section-number">3.1</span> Time to what?</a></li>
  <li><a href="#sec-performance-scaling" id="toc-sec-performance-scaling" class="nav-link" data-scroll-target="#sec-performance-scaling"><span class="header-section-number">3.2</span> Scaling analysis</a></li>
  <li><a href="#sec-performance-arch" id="toc-sec-performance-arch" class="nav-link" data-scroll-target="#sec-performance-arch"><span class="header-section-number">3.3</span> Architecture basics</a>
  <ul class="collapse">
  <li><a href="#memory-matters" id="toc-memory-matters" class="nav-link" data-scroll-target="#memory-matters"><span class="header-section-number">3.3.1</span> Memory matters</a></li>
  <li><a href="#instruction-level-parallelism" id="toc-instruction-level-parallelism" class="nav-link" data-scroll-target="#instruction-level-parallelism"><span class="header-section-number">3.3.2</span> Instruction-level parallelism</a></li>
  </ul></li>
  <li><a href="#sec-performance-model" id="toc-sec-performance-model" class="nav-link" data-scroll-target="#sec-performance-model"><span class="header-section-number">3.4</span> Performance modeling</a>
  <ul class="collapse">
  <li><a href="#applications-benchmarks-and-kernels" id="toc-applications-benchmarks-and-kernels" class="nav-link" data-scroll-target="#applications-benchmarks-and-kernels"><span class="header-section-number">3.4.1</span> Applications, benchmarks, and kernels</a></li>
  <li><a href="#model-composition" id="toc-model-composition" class="nav-link" data-scroll-target="#model-composition"><span class="header-section-number">3.4.2</span> Model composition</a></li>
  <li><a href="#modeling-with-kernels" id="toc-modeling-with-kernels" class="nav-link" data-scroll-target="#modeling-with-kernels"><span class="header-section-number">3.4.3</span> Modeling with kernels</a></li>
  <li><a href="#the-roofline-model" id="toc-the-roofline-model" class="nav-link" data-scroll-target="#the-roofline-model"><span class="header-section-number">3.4.4</span> The Roofline model</a></li>
  <li><a href="#amdahls-law" id="toc-amdahls-law" class="nav-link" data-scroll-target="#amdahls-law"><span class="header-section-number">3.4.5</span> Amdahl’s law</a></li>
  <li><a href="#gustafsons-law" id="toc-gustafsons-law" class="nav-link" data-scroll-target="#gustafsons-law"><span class="header-section-number">3.4.6</span> Gustafson’s law</a></li>
  </ul></li>
  <li><a href="#sec-performance-principles" id="toc-sec-performance-principles" class="nav-link" data-scroll-target="#sec-performance-principles"><span class="header-section-number">3.5</span> Performance principles</a>
  <ul class="collapse">
  <li><a href="#think-before-you-write" id="toc-think-before-you-write" class="nav-link" data-scroll-target="#think-before-you-write"><span class="header-section-number">3.5.1</span> Think before you write</a></li>
  <li><a href="#time-before-you-tune" id="toc-time-before-you-tune" class="nav-link" data-scroll-target="#time-before-you-tune"><span class="header-section-number">3.5.2</span> Time before you tune</a></li>
  <li><a href="#shoulders-of-giants" id="toc-shoulders-of-giants" class="nav-link" data-scroll-target="#shoulders-of-giants"><span class="header-section-number">3.5.3</span> Shoulders of giants</a></li>
  <li><a href="#help-tools-help-you" id="toc-help-tools-help-you" class="nav-link" data-scroll-target="#help-tools-help-you"><span class="header-section-number">3.5.4</span> Help tools help you</a></li>
  <li><a href="#tune-data-structures" id="toc-tune-data-structures" class="nav-link" data-scroll-target="#tune-data-structures"><span class="header-section-number">3.5.5</span> Tune data structures</a></li>
  </ul></li>
  <li><a href="#sec-performance-julia" id="toc-sec-performance-julia" class="nav-link" data-scroll-target="#sec-performance-julia"><span class="header-section-number">3.6</span> Performance in Julia</a>
  <ul class="collapse">
  <li><a href="#measurement-tools" id="toc-measurement-tools" class="nav-link" data-scroll-target="#measurement-tools"><span class="header-section-number">3.6.1</span> Measurement tools</a></li>
  <li><a href="#avoiding-boxing" id="toc-avoiding-boxing" class="nav-link" data-scroll-target="#avoiding-boxing"><span class="header-section-number">3.6.2</span> Avoiding boxing</a></li>
  <li><a href="#temporary-issues" id="toc-temporary-issues" class="nav-link" data-scroll-target="#temporary-issues"><span class="header-section-number">3.6.3</span> Temporary issues</a></li>
  <li><a href="#performance-annotations" id="toc-performance-annotations" class="nav-link" data-scroll-target="#performance-annotations"><span class="header-section-number">3.6.4</span> Performance annotations</a></li>
  </ul></li>
  <li><a href="#sec-performance-misconceptions" id="toc-sec-performance-misconceptions" class="nav-link" data-scroll-target="#sec-performance-misconceptions"><span class="header-section-number">3.7</span> Misconceptions and deceptions</a>
  <ul class="collapse">
  <li><a href="#incorrect-mental-models" id="toc-incorrect-mental-models" class="nav-link" data-scroll-target="#incorrect-mental-models"><span class="header-section-number">3.7.1</span> Incorrect mental models</a></li>
  <li><a href="#deceptions-and-self-deceptions" id="toc-deceptions-and-self-deceptions" class="nav-link" data-scroll-target="#deceptions-and-self-deceptions"><span class="header-section-number">3.7.2</span> Deceptions and self-deceptions</a></li>
  <li><a href="#rules-for-presenting-performance-results" id="toc-rules-for-presenting-performance-results" class="nav-link" data-scroll-target="#rules-for-presenting-performance-results"><span class="header-section-number">3.7.3</span> Rules for presenting performance results</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../00-Background/00-Intro.html">Background Plus a Bit</a></li><li class="breadcrumb-item"><a href="../00-Background/02-Performance.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Performance Basics</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-performance-ch" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Performance Basics</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“Are we there yet?”<br>
— small children everywhere</p>
</blockquote>
<p>Numerical code should be right enough and fast enough. Correctness comes first, but we are also impatient, and want our codes to run fast. We do not recommend our readers should become reckless speed demons: improvements in running time should be weighed against the time required to implement and maintain the code, and for most of us it is unwise to get into the business of rewriting our codes every year to eke every jot of speed out of the newest processor. But certain details of how we implement our methods can make order-of-magnitude differences in how fast our codes run on most modern processors, and a little knowledge of those details (along with a few tools) can go a long way toward keeping us happy with both the performance and the tidiness of our codes.</p>
<section id="time-to-what" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="time-to-what"><span class="header-section-number">3.1</span> Time to what?</h2>
<p>The key metric in performance of numerical methods is <em>usually</em> the wall clock time to a solution. In addition to wall clock time, one might want to understand how much memory, disk space, network bandwidth, or power are used in a given computation. However, all these measures may depend on implementation details, on details of the problem being solved, and on the system used.</p>
<p>We would certainly like code that we have optimized to run fast on problems other than our test problems; and ideally, we would like our performance to be portable (or at least “transportable” with small changes) to new systems. To accomplish this, it is useful to have both performance <em>models</em> that we can use to generalize beyond a single instance and performance <em>experiments</em> to validate our models and to fit any free parameters. Sometimes we use proxy measures for wall clock time, such as the number of arithmetic operations in a code, with the suggestion that these proxies relate to wall clock time by a simple (usually linear) model. Such proxy measures should be treated with caution unless backed by data. We discuss this in <a href="#sec-performance-scaling" class="quarto-xref"><span>Section 3.2</span></a>, <a href="#sec-performance-model" class="quarto-xref"><span>Section 3.4</span></a>, and <a href="#sec-performance-misconceptions" class="quarto-xref"><span>Section 3.7</span></a>.</p>
<p>The notion of “time to solution” has some additional subtleties. Some algorithms run to completion, then return a result more-or-less instantaneously at the end. In many cases, though, a computation will produce intermediate results. In this case, there are usually two quantities of interest in characterizing the performance:</p>
<ul>
<li>The time to first result (response time)</li>
<li>Rate at which results are subsequently produced (throughput)</li>
</ul>
<p>For problems involving information retrieval or communication, we usually refer to these as the <em>latency</em> and <em>bandwidth</em>. But the idea of an initial response time and subsequent throughput rate applies more broadly.</p>
<p>When we think about concepts of latency and throughput, we are measuring performance not by a single number (time to completion), but by a curve of utility versus time. When we think about a <em>fixed</em> latency and throughput, we are implicitly defining a piecewise linear model of utility versus time: there’s an initial period of zero utility, followed by a period of linearly increasing utility with constant slope (until completion). The piecewise linear model is attractive in its simplicity, but more complex models are sometimes useful. For example, to understand the performance of an iterative solver, the right way to measure performance might be in terms of approximation error versus time.</p>
<p>We are also often interested in situations where we <em>incrementally recompute</em> something based on new data. In this case, we might have initial setup costs that need only be run once, and thereafter are not required again (or are only required periodically). In this case, there is a tradeoff: the setup costs may be expensive, but what we care about is the setup cost <em>together with</em> the incremental costs. In this case, we say the setup costs are <em>amortized</em> over the computation.</p>
<p>While it is not our main topic in this chapter, it is also worthwhile to pay attention to resources that we care about that fall outside our computation. This might include things like input/output costs – reading data in and writing results out can be surprisingly expensive! It can also include things like the number of times that we require human attention, or the amount of experimental data required to adequately fit a model.</p>
</section>
<section id="sec-performance-scaling" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-performance-scaling"><span class="header-section-number">3.2</span> Scaling analysis</h2>
<p>For most problem classes, there is some natural measure of the size of the problem. This could be the number of data points we measure, the size of a linear system to be solved, etc. Scaling analysis for algorithms has to do with the way the cost changes as the size parameter <span class="math inline">\(n\)</span> grows, along with other parameters such as the number of processors.</p>
<p>We usually write scaling analyis using order notation (aka “big O” notation), which refers to <em>sets</em> of functions with certain growth rates; we say <span class="math inline">\(f(n)\)</span> is</p>
<ul>
<li><span class="math inline">\(O(g(n))\)</span> if <span class="math inline">\(f\)</span> grows no faster than <span class="math inline">\(g\)</span>: there is an integer <span class="math inline">\(N\)</span> and positive constant <span class="math inline">\(C\)</span> such that for all <span class="math inline">\(n \geq N\)</span>, <span class="math inline">\(f(n) \leq C
g(n)\)</span>.</li>
<li><span class="math inline">\(o(g(n))\)</span> if <span class="math inline">\(f\)</span> grows more slowly than <span class="math inline">\(g\)</span>: for any positive <span class="math inline">\(C\)</span>, there is an integer <span class="math inline">\(N\)</span> such that for all <span class="math inline">\(n &gt; N\)</span>, <span class="math inline">\(f(n) &lt; C g(n)\)</span>.</li>
<li><span class="math inline">\(\Omega(g(n))\)</span> if <span class="math inline">\(f\)</span> grows no more slowly than <span class="math inline">\(g\)</span>: there is an integer <span class="math inline">\(N\)</span> and positive constant <span class="math inline">\(C\)</span> such that for all <span class="math inline">\(n \geq N\)</span>, <span class="math inline">\(f(n) \geq C g(n)\)</span>.</li>
<li><span class="math inline">\(\omega(g(n))\)</span> if <span class="math inline">\(f\)</span> grows strictly faster than <span class="math inline">\(g\)</span>: for any positive <span class="math inline">\(C\)</span>, there is an integer <span class="math inline">\(N\)</span> such that for all <span class="math inline">\(n &gt; N\)</span>, <span class="math inline">\(f(n) &lt; C g(n)\)</span>.</li>
<li><span class="math inline">\(\Theta(g(n))\)</span> if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> grow at the same rate: there is an integer <span class="math inline">\(N\)</span> and positive constants <span class="math inline">\(c\)</span> and <span class="math inline">\(C\)</span> such that for all <span class="math inline">\(n
\geq N\)</span>, <span class="math inline">\(c g(n) \leq f(n) \leq C g(n)\)</span>.</li>
</ul>
<p>Even though order notation defines sets of functions, we usually abuse notation and write things like <span class="math inline">\(f(n) = O(g(n))\)</span> rather than the more proper <span class="math inline">\(f(n) \in O(g(n))\)</span>. We will see the same notation in a different context (<span class="math inline">\(x\)</span> going to zero rather than <span class="math inline">\(n\)</span> going to infinity) in <a href="04-Calculus.html" class="quarto-xref"><span>Chapter 5</span></a>.</p>
<p>As a concrete example, that will recur frequently, let us consider the complexity of the <em>Basic Linear Algebra Subroutines</em> (BLAS), a collection of standard linear algebra operations often used as building blocks for higher-level algorithms. Typical examples are dot products, matrix-vector products, and matrix-matrix products. For the case of square matrices, these cost <span class="math inline">\(O(n^1)\)</span>, <span class="math inline">\(O(n^2)\)</span>, and <span class="math inline">\(O(n^3)\)</span> time, respectively. In the language of the BLAS standard, we call these level 1, level 2, and level 3 BLAS calls. However, while this captures the correct scaling, it is nowhere near the full story: a fast implementation of the standard matrix-matrix multiply<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> can be orders of magnitude faster than the standard three nested loops.</p>
<p>Order notation is convenient, and we will use it often. But it is convenient in part because it is crude: if we say the runtime of an algorithm is <span class="math inline">\(O(n^3)\)</span>, that means there is some <span class="math inline">\(N\)</span> and <span class="math inline">\(C\)</span> such that the runtime is bounded by <span class="math inline">\(Cn^3\)</span> for all <span class="math inline">\(n \geq N\)</span> – but that says nothing about the size of <span class="math inline">\(C\)</span> or <span class="math inline">\(N\)</span>. Indeed, we expect the tightest version of the constant <span class="math inline">\(C\)</span> to vary depending on details of the implementation and the system we use. As we will see in <a href="#sec-performance-model" class="quarto-xref"><span>Section 3.4</span></a>, we usually will want a scaling analysis as the <em>start</em> of understanding performance, but it is not the conclusion.</p>
</section>
<section id="sec-performance-arch" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-performance-arch"><span class="header-section-number">3.3</span> Architecture basics</h2>
<p>In introductory programming classes, students typically form a mental model of how an idealized computer works. There is a memory, organized into a linear address space. A program is stored in memory, and consists of machine language instructions for things like register read/write, logic, and arithmetic. The computer reads these instructions from memory and executes them in program order. And, we think, all operations take about the same amount of time.</p>
<p>This mental model is incomplete in many respects, though that does not keep it from being useful. The reader who has had an undergraduate computer architecture course (e.g.&nbsp;from <span class="citation" data-cites="patterson-hennesey-6e">Hennessey and Patterson (<a href="../references.html#ref-patterson-hennesey-6e" role="doc-biblioref">2017</a>)</span>) will already appreciate the impact on performance of the memory hierarchy and instruction-level parallelism on performance. For the reader who has not had such a course (or has not reviewed the material recently), a brief synopsis may be useful.</p>
<section id="memory-matters" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="memory-matters"><span class="header-section-number">3.3.1</span> Memory matters</h3>
<p>In a crude accounting of the work done by numerical codes, we often count the number of floating point operations (flops) required. On modern machines, though, the cost of a floating point operation pales in comparison to the cost of an access to main memory. On one core of the laptop on which this text is being written<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, more than 2500 64-bit floating point operations can (in theory) be performed in the latency period for a read request to main memory; and the subsequent bandwidth is such that we could execute more than three floating point operations per floating point number read from main memory. Though the details of these numbers vary from machine to machine, the core message remains the same: if we are not careful, the dominant cost of many numerical computations is not arithmetic, but data transfers.</p>
<section id="locality" class="level4" data-number="3.3.1.1">
<h4 data-number="3.3.1.1" class="anchored" data-anchor-id="locality"><span class="header-section-number">3.3.1.1</span> Locality</h4>
<p>What saves us from being forever limited by main memory is a <em>memory hierarchy</em> with small memories with fast access (caches) absorbing some of the traffic that would otherwise go to larger memories with slower access farther away. This hierarchy is engineered to reduce main memory traffic for programs that exhibit two sorts of <em>locality</em>:</p>
<ul>
<li><p><em>Spatial locality</em> is the tendency to consecutively access memory locations that are close together in address space.</p></li>
<li><p><em>Temporal locality</em> is the tendency to frequently re-use a (small) “working set” of data in a particular part of a computation.</p></li>
</ul>
<p>Some programs are born with data that fits in cache, some achieve good cache utilization through “natural” locality, and some have locality thrust upon them. In many cases, performance tuning of numerical codes falls into the last category: a naively written code will not have good locality and will tend to be limited by memory, but we can impvove matters by rearranging the computations for a more cache-friendly access pattern. But to get the best use from caches, we need a few more details.</p>
</section>
<section id="hits-misses-and-operational-intensity" class="level4" data-number="3.3.1.2">
<h4 data-number="3.3.1.2" class="anchored" data-anchor-id="hits-misses-and-operational-intensity"><span class="header-section-number">3.3.1.2</span> Hits, misses, and operational intensity</h4>
<p>Processors may have a <em>register file</em> containing data that can be operated on immediately, two or three levels of <em>cache</em> (with L1 the fastest and smallest, then L2, and then L3), and a main memory. When a program requests data from a particular address, the processor first consults with the lowest level of cache. If the data is in the cache (a <em>cache hit</em>), it can be returned otherwise. In the event of a <em>cache miss</em>, the processor consults with higher levels of cache to try to find the data, eventually going to main memory if necessary.</p>
<p>To exploit <em>spatial locality</em>, caches are organized into <em>lines</em> of several bytes; when data is read into cache, we fetch it a cache line at a time, possibly saving ourselves some subsequent cache misses. To exploit <em>temporal locality</em>, the processor tries to keep a line in cache until the space is needed for something else. An <em>eviction policy</em> determines when a cache line will be replaced by other data. The eviction policy depends in part on the <em>associativity</em> of the cache. In a <em>direct-mapped</em> cache, each address can only go in one cache location, usually determined by the low-order bits of the storage address. In an <span class="math inline">\(n\)</span>-way <em>set associative</em> cache, each address can go into one of <span class="math inline">\(n\)</span> possible cache locations; in case a line must be evicted, the processor will choose something like the least recently used of the set (an LRU policy). Higher levels of associativity are more expensive in terms of hardware, and so are usually associated with the lowest levels of cache.</p>
<p>Naturally, we would like to minimize the number of cache misses. To design code with few misses, it is useful to think of three distinct types of misses<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>:</p>
<ul>
<li><em>Compulsory</em>: when the data is loaded into cache for the first time.</li>
<li><em>Capacity</em>: when the working set is too big and the cache was filled with other things since it was last used.</li>
<li><em>Conflict</em>: when there is insufficient associativity for the access pattern.</li>
</ul>
<p>For codes with low <em>operational intensity</em> (or <em>arithmetic intensity</em>), compulsory misses are the limiting factor in how well we can use the cache. The operational intensity is defined to be the ratio of operations to memory reads. For example, consider a dot product of two vectors (assumed not to be resident in cache): if each vector is length <span class="math inline">\(n\)</span>, we have <span class="math inline">\(2n\)</span> floating point operations (adds and multiplies) on <span class="math inline">\(2n\)</span> floating point numbers for an operational intensity of one flop per float. We can, at best, hope that we are accessing the numbers in sequential order so that we only have a cache miss every few numbers (depending on the number of floating point numbers that fit into a cache line). Such a routine is inherently <em>memory-bound</em>, i.e.&nbsp;it is limited not by the time to do arithmetic but by the time to retrieve data.</p>
<p>In contrast to dot products, square matrix-matrix products involve <span class="math inline">\(2n^3\)</span> floating point operations, but only involve <span class="math inline">\(2n^2\)</span> input numbers — an operational intensity of <span class="math inline">\(n\)</span> operations per float. In this case, we are not so limited by compulsory misses. However, unless the matrices are small enough to fit entirely into cache, a naively-written code may still suffer <em>capacity</em> misses. To get around this, we might use <em>blocking</em> or <em>tiling</em>, reorganizing the matrix-matrix product in terms of products of smaller submatrices. Even with such a reorganization, we might need to be careful about <em>conflict</em> misses, particularly when <span class="math inline">\(n\)</span> is a multiple of a large power of 2 (the worst case scenario for set-associative caches).</p>
</section>
</section>
<section id="instruction-level-parallelism" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="instruction-level-parallelism"><span class="header-section-number">3.3.2</span> Instruction-level parallelism</h3>
<p>If we do not use caches well, our speed will be limited by memory traffic. But once we have reduced memory traffic enough, the rate at which we can execute operations becomes the limiting factor; that is, we are <em>compute bound</em> rather than <em>memory bound</em>. To improve the performance of compute-bound codes, we need to understand a little more about a little about <em>instruction-level parallelism</em>.</p>
<p>A typical processor consists of several <em>cores</em>. Codes that are not explicitly parallel run on just one core at a time. Each core presents a serial programming <em>interface</em>: the core acts like it executes machine instructions in program order. This interface is fine for reasoning about correctness of programs. But the interface hides a much more sophisticated implementation.</p>
<p>In introductory computer architecture classes, we usually start by discussing a <em>five stage pipeline</em>, where for each instruction we</p>
<ul>
<li><em>Fetch</em> the instruction from memory</li>
<li><em>Decode</em> the instruction</li>
<li><em>Execute</em> the instruction</li>
<li>Perform any <em>memory</em> operations</li>
<li><em>Write back</em> results to the register file</li>
</ul>
<p>In any given cycle, each pipeline stage can be occupied by a different instruction; while we are writing back the first instruction in a sequence, we might be performing a memory operation associated with the second instruction, doing some arithmetic for the third instruction, and decoding and fetching the fourth and fifth instruction. Hence, though each instruction typically has a <em>latency</em> of five cycles to execute (modulo waiting for memory), the processor in principle can execute instructions with a <em>bandwidth</em> of one instruction per cycle. We do not always achieve this peak number of instructions per cycle, though. For example, if the second instruction depends on the result of the first instruction, then we cannot execute the second instruction until the first instruction has written its results back to the register file. This results in a “bubble” in the pipeline where some of the stages are idle, and reduces the rate at which the processor can execute instructions.</p>
<p>The picture in most machines now is more complicated than a five stage pipeline. Modern processors are often <em>wide</em>: the front-end pipeline can fetch and decode several operations in a single cycle. The fetch-and-decode itself can be rather complex, with one machine language instruction turned into several “micro-ops” internally. Once an instruction is decoded, it is kept in a re-order buffer until the processor is ready to dispatch it to a <em>functional unit</em> like a floating point unit or memory unit. These functional units themselves have internal pipelines, and so can process several instructions concurrently. As the functional units complete their work, the results are written back in an order consistent with the program order.</p>
<p>A wide, pipelined, out-of-order processor can have many instructions in flight at the same time. The extent to which we can keep the processor fully occupied depends on two factors:</p>
<ul>
<li><p><em>Limited dependencies</em>: As with the five-stage pipeline, dependencies between instructions can limit the amount of instruction-level parallelism. These dependencies can take the form of data dependencies (one operation takes as input the result of a previous operation) or control dependencies (we cannot complete instructions after a branch until the branch condition has been computed). The out-of-order buffer and branch prediction techniques can help mitigate the impact of dependencies, but we still expect code with simple control structures and lots of independent operations to run faster than code with complicated control and lots of tight data dependencies.</p></li>
<li><p><em>Instruction diversity</em>: A mix of different types of instructions can keep the different functional units occupied concurrently.</p></li>
</ul>
<p>In addition to the <em>implicit</em> instruction-level parallelism we have just described, many modern processors provide <em>explicit</em> instruction-level parallelism in the form of <em>vector</em> or <em>SIMD</em> (single-instruction multiple-data) instructions that simultaneously compute the same operation (e.g.&nbsp;addition or multiplication) on short vectors of inputs.</p>
<p>Given code with simple structure and enough evident independent operations, compilers can do a good job at producing code that uses vector instructions, just as they do a good job at reordering instructions for the processor.</p>
</section>
</section>
<section id="sec-performance-model" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-performance-model"><span class="header-section-number">3.4</span> Performance modeling</h2>
<p>Runtime can depend on many parameters: the problem size and structure, the algorithms used in the computation, details of the implementation, the number and type of processors used, etc. Performance models predict runtime (and perhaps the use of other resources) as a function of these parameters. There are several reasons to want such a model:</p>
<ul>
<li><p>To decide whether a method is worth the effort of implementing or tuning.</p></li>
<li><p>To decide what algorithm to use (and what parameter settings) for the best runtime on a particular problem and system.</p></li>
<li><p>To decide whether a particular subcomputation is likely to be a bottleneck in a larger computation.</p></li>
<li><p>To determine whether the runtime of a computation is “reasonable” or if there is room for easy improvement in the implementation.</p></li>
</ul>
<p>Scaling analysis (<a href="#sec-performance-scaling" class="quarto-xref"><span>Section 3.2</span></a>) does not usually yield a good performance model on its own. Even if we estimate of the number of floating point operations in a given computation, we know from <a href="#sec-performance-arch" class="quarto-xref"><span>Section 3.3</span></a> that there may be several orders of magnitude depending in runtime depending how memory is used and the amount of instruction-level parallelism. At the same time, a <em>somewhat</em> inaccurate model is often fine to guide engineering decisions. Because models reflect our understanding, they will almost always be incomplete; indeed, the most useful models are <em>necessarily</em> incomplete, since otherwise they are too cumbersome to reason about!</p>
<p>Experiments reflect what really happens, and are a critical counterpoint to models. The division between performance models and experiments is not sharp. In the extreme case, we can use machine learning and other empirical function fitting methods can be used to estimate runtimes from experimental measurements under weak assumptions. But when strongly empirical models have many parameters, a lot of data is needed to fit them well; otherwise, the models may be <em>overfit</em>, and do a poor job of predicting performance except away from the training data. Gathering a lot of data may be appropriate for cases where the model is used as the basis for <em>auto-tuning</em> a commonly-used kernel for a particular machine architecture, for example. But performance experiments often aren’t cheap – or at least they aren’t cheap in the regime where people most care about performance – and so a simple, theory-grounded model is often preferable. There’s an art to balancing what should be modeled and what should be treated semi-empirically, in performance analysis as in the rest of science and engineering.</p>
<section id="applications-benchmarks-and-kernels" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="applications-benchmarks-and-kernels"><span class="header-section-number">3.4.1</span> Applications, benchmarks, and kernels</h3>
<p>The performance of application codes is usually what we really care about. But application performance is generally complicated. The main computation may involve alternating phases, each complex in its own right, in addition to time to load data, initialize any data structures, and post-process results. Because there are so many moving parts, its also hard to use measurements of the end-to-end performance of a given code on a given machine to infer anything about the speed expected of other codes. Sometimes it’s hard even to tell how the same code will run on a different machine!</p>
<p><em>Benchmark codes</em> serve to provide a uniform way to compare the performance of different machines on “characteristic” workloads. Usually benchmark codes are simplified versions of real applications (or of the computationally expensive parts of real applications); examples include the <a href="https://www.nas.nasa.gov/publications/npb.html">NAS parallel benchmarks</a>, the <a href="http://www.graph500.org/">Graph 500</a> benchmarks, and (on a different tack) Sandia’s <a href="https://mantevo.org/">Mantevo</a> package of mini-applications.</p>
<p><em>Kernels</em> are frequently-used subroutines such as matrix multiply, FFT, breadth-first search, etc. Because they are building blocks for so many higher-level codes, we care about kernel performance a lot; and a kernel typically involves a (relatively) simple computation. A common first project in parallel computing classes is to time and tune a matrix multiplication kernel.</p>
<p>Parallel <em>patterns</em> (or <a href="www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf">“dwarfs”</a>) are abstract types of computation (like dense linear algebra or graph analysis) that are higher level than kernels and more abstract than benchmarks. Unlike a kernel or a benchmark, a pattern is too abstract to benchmark. On the other hand, benchmarks can elucidate the performance issues that occur on a given machine with a particular type of computation.</p>
</section>
<section id="model-composition" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="model-composition"><span class="header-section-number">3.4.2</span> Model composition</h3>
<p>Counting floating-point operations is generally a bad way to estimate performance, because the “cost per flop” is so poorly defined. But the performance of larger building blocks may be much more stable, so that call counts (as opposed to operation counts) are a reasonable basis for performance modeling. For example, we frequently describe the performance of iterative solvers for sparse systems of linear equations in terms of the number of matrix-vector products; and for a particular matrix size, the time for a matrix-vector product will usually be about constant<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. If all other operations in the solver are cheap compared to the cost of the matrix-vector product, just measuring the number of products and separately building a model for the time <span class="math inline">\(t_{\mathrm{matvec}}\)</span> for one product (or measuring it) may be adequate for predicting performance.</p>
<p>If we decide that a particular iterative solver should be faster, we might try to make matrix-vector products run faster, perhaps by rearranging the data structure that represents the matrix. This might be accomplished with some setup cost <span class="math inline">\(t_{\mathrm{setup}}\)</span> to analyze the input matrix and rearrange the data. If the rearranging the multiplication speeds it up by a factor of <span class="math inline">\(S'\)</span>, then the cost of the original code vs the initial code is <span class="math display">\[\begin{aligned}
t_{\mathrm{original}} &amp;= n t_{\mathrm{matvec}} \\
t_{\mathrm{rearranged}} &amp;= t_{\mathrm{setup}} + n t_{\mathrm{matvec}} / S'
\end{aligned}\]</span> The rearranged computation will be faster if <span class="math display">\[
  t_{\mathrm{setup}} &lt; t_{\mathrm{matvec}} n (S'-1)/S'.
\]</span> Even a fairly large setup cost may be worthwhile if it is <em>amortized</em> over enough iterations. Even if rearranging the data structure does not make sense for a single linear solve, it may make sense to rearrange if we are solving many similar systems (in which case we might be able to pay the setup cost just once rather than for every solve).</p>
<p>Alternately, we could try to find an alternative method that requires fewer matrix-vector products, though this might involve more expensive iterations. This can be a net win even if each iteration is “less efficient” in terms of the rate of floating point operations. It is worth remembering that the key measure is not the arithmetic rate, but the time to completion (or time to adequate accuracy). In each case, we can reason about the performance with the same general strategy of reasoning about the performance of smaller building blocks.</p>
</section>
<section id="modeling-with-kernels" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="modeling-with-kernels"><span class="header-section-number">3.4.3</span> Modeling with kernels</h3>
<p>When we study dense matrix computations <span class="quarto-unresolved-ref">?sec-nla-ch</span>, we will write many of our algorithms in “block” fashion, so that almost all the work is done in level 3 BLAS operations. Because these have high operational intensity, it is possible (though difficult) to write these operations to run at pretty close to the peak arithmetic rate, at least for a single core. Fortunately, these operations are so common that we can usually rely on someone else to have done the hard work of writing a fast implementation in this setting. Hence, in dense matrix computations we often write that something requires a certain number of floating point operations that are “mostly level 3 BLAS,” and assume that the reader understands that for this problem, the number of floating point operations times the peak flop rate (or some adjusted version thereof) is a reasonable estimate for the runtime.</p>
<p>More generally, for codes that rely on common well-tuned kernel operations like the level 3 BLAS or tuned FFTs, we can sometimes get away with assuming that the kernel runs at an appropriate “speed of light” for the hardware.</p>
</section>
<section id="the-roofline-model" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="the-roofline-model"><span class="header-section-number">3.4.4</span> The Roofline model</h3>
<p>If we are going to assume kernels that run at an appropriate “speed of light” for hardware, we need to understand what that speed is. The <em>Roofline model</em> is a simple model of peak performance for different computational kernels (<span class="citation" data-cites="williams-waterman-patterson-2009">Williams, Waterman, and Patterson (<a href="../references.html#ref-williams-waterman-patterson-2009" role="doc-biblioref">2009</a>)</span>). The model consists of a log-log plot of the operational intensity <span class="math inline">\(I\)</span> (in floating point operations / byte) vs the operation rate (floating point operations per second). The actual operation rate always sits under a “roofline,” the minimum of the limit imposed by the peak memory bandwidth <span class="math inline">\(\beta\)</span> (operation rate is less than <span class="math inline">\(\beta \times
I\)</span>, a diagonal line on the plot) and the peak operational rate attainable by the hardware (a horizontal line on the plot). More elaborate versions of the roofline plot can include additional performance ceilings (e.g.&nbsp;the ceiling without the use of vector instructions, or without the use of multiple cores) and bandwidth ceilings (e.g.&nbsp;associated with multiple levels of cache).</p>
</section>
<section id="amdahls-law" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="amdahls-law"><span class="header-section-number">3.4.5</span> Amdahl’s law</h3>
<p>We measure improvements to performance of a code on a fixed problem size by the <em>speedup</em>: <span class="math display">\[
  S = \frac{T_{\mathrm{baseline}}}{T_{\mathrm{improved}}}.
\]</span> Suppose we have a code involving two types of work: some fraction <span class="math inline">\(\alpha\)</span> is work that we do not know how to speed up, and the remaining fraction <span class="math inline">\((1-\alpha)\)</span> we think we can improve. Let <span class="math inline">\(S'\)</span> be the speedup for the part that we know how to improve; then <span class="math display">\[
\begin{aligned}
  S &amp;=
  \frac{T_{\mathrm{baseline}}}
       {\alpha T_{\mathrm{baseline}} +
       (1-\alpha) T_{\mathrm{baseline}}/S'} \\
  &amp;=   \frac{1}{\alpha + (1-\alpha)/S'} \\
  &amp;=   \frac{S'}{1 + \alpha (S'-1)}
\end{aligned}
\]</span> No matter how big the speedup <span class="math inline">\(S'\)</span> for the part we know how to improve, the overall speedup is bounded by <span class="math inline">\(1/\alpha\)</span>. This observation is known as <em>Amdahl’s law</em>.</p>
<p>Amdahl’s law is best known in the context of parallel computing, where we are interested in speedup for a fixed problem size as a function of the number of processors <span class="math inline">\(p\)</span>: <span class="math display">\[
  S(p) = \frac{T_{\mathrm{serial}}}{T_{\mathrm{parallel}}(p)}.
\]</span> Studying the speedup <span class="math inline">\(S(p)\)</span> in this setting (or the parallel efficiency <span class="math inline">\(S(p)/p\)</span>) is known as a <em>strong scaling</em> study. In this setting, <span class="math inline">\(\alpha\)</span> is the fraction of serial work. If the rest of the computation can be perfectly sped up (i.e.&nbsp;<span class="math inline">\(S' = p\)</span>), then <span class="math display">\[
  S(p) = \frac{p}{1+\alpha (p-1)} \leq \frac{1}{\alpha}.
\]</span> In practice, this is usually a generous estimate: some overheads usually grow with the number of processors, so that past a certain number of processors the speedup often doesn’t just level off, but actually <em>decreases</em>.</p>
</section>
<section id="gustafsons-law" class="level3" data-number="3.4.6">
<h3 data-number="3.4.6" class="anchored" data-anchor-id="gustafsons-law"><span class="header-section-number">3.4.6</span> Gustafson’s law</h3>
<p>Amdahl’s law is an appropriate modeling tool when we are trying to improve the runtime to solve problems of a fixed size, whether by parallel computing or by tuning code. Sometimes, though, we want to improve runtime because we want to solve bigger problems! This leads to a different scaling relationship.</p>
<p>In <em>weak scaling</em> studies in parallel computing, we usually consider the <em>scaled speedup</em> <span class="math display">\[
S(p) = \frac{T_{\mathrm{serial}}(n(p))}
            {T_{\mathrm{parallel}}(n(p),p)}
\]</span> where <span class="math inline">\(n(p)\)</span> is a family of problem sizes chosen so that the work per processor remains constant. For weak scaling studies, the analog of Amdahl’s law is <em>Gustafson’s law</em>; if <span class="math inline">\(a\)</span> is the amount of serial work and <span class="math inline">\(b\)</span> is the parallelizable work, then <span class="math display">\[
S(p) \leq \frac{a + bP}{a + b} = p-\alpha(p-1)
\]</span> where <span class="math inline">\(\alpha = a/(a+b)\)</span> is the fraction of serial work.</p>
</section>
</section>
<section id="sec-performance-principles" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-performance-principles"><span class="header-section-number">3.5</span> Performance principles</h2>
<blockquote class="blockquote">
<p>There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the seed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We <em>should</em> forget about small efficiencies, say 97% of the time: premature optimization is the root of all evil.</p>
<p>Yet, we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only <em>after</em> that code has been identified. It is often a mistake to make a priori judgements about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail. …</p>
<p>– Donald Knuth (from <span class="citation" data-cites="knuth-1974">Knuth (<a href="../references.html#ref-knuth-1974" role="doc-biblioref">1974</a>)</span>)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
</blockquote>
<p>We want our codes to run fast, but not at great cost in terms of maintainability or development time. To do this, it is useful to keep some principles in mind.</p>
<section id="think-before-you-write" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="think-before-you-write"><span class="header-section-number">3.5.1</span> Think before you write</h3>
<p>Back-of-the-envelope performance models are often<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> enough to give us a sense of the “big computations,” parts of a code will be critical to performance. We might also have a sense in advance of what the natural algorithmic variants are for this code are. If we think a particular routine might be a performance bottleneck that we will want to play with, it is probably worth thinking about how to code that routine so it is easy to experiment with it (or replace it).</p>
<p>When thinking about performance, it is worthwhile remembering to explicitly account for I/O. Memory accesses may move at a snail’s pace compared to arithmetic; but I/O is positively glacial compared to either.</p>
<p>Before writing any code, it is also useful to</p>
<ul>
<li><p>Think through the implications of algorithms and data structures. Sometimes it is possible to just make a code do much less work without too much efort by using the right standard tools.</p></li>
<li><p>Think about data layouts, if only to make sure that the code does not become too wedded to a specific data layout in memory.</p></li>
<li><p>Decide if an approximation is good enough.</p></li>
</ul>
</section>
<section id="time-before-you-tune" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="time-before-you-tune"><span class="header-section-number">3.5.2</span> Time before you tune</h3>
<p>When codes are slow, it is often because a large amount of time is spent in a few key <em>bottlenecks</em>. In this case, our goal is to find and fix those bottleneck computations<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<p>For removing bottlenecks – or even for deciding that a code needs to be tuned in the first place – we need to pay attention to some practical points about the design of timing experiments:</p>
<ul>
<li><p>For codes that show any data-dependent performance, it is important to profile on something realistic, as the time breakdown will depend on the use case.</p></li>
<li><p>We also need to be aware that wall-clock time is not the only type of time we can measure – for example, many systems have some facility to monitor “CPU time,” in which only the time that a task is using the processor is counted. Wall-clock time is typically what we care about.</p></li>
<li><p>The system wall-clock time resolution is often much coarser than the CPU cycle time. Consequently, we need to make sure that enough work is done in a timing experiment or we might not get good data. A typical approach to this is to put code to be timed into a loop.</p></li>
<li><p>The same routine run repeatedly may run faster after a first iteration that warms up the cache.</p></li>
<li><p>There will probably be other processes running on the system, and cross-interference with other tasks can affect timing.</p></li>
</ul>
<p><em>Profiling</em> involves running a code and measuring how much time (and resources) are used in different parts of the code. One can profile with different levels of detail. The simplest case often involves manually instrumenting a code with timers. There are also tools that <em>automatically instrument</em> either the source code or binary objects to record timing information. <em>Sampling profilers</em> work differently; they use system facilities to interrupt the program execution periodically and measure where the code is. It is also possible to use <em>hardware counters</em> to estimate the number of performance-relevant events (such as cache misses or flops) that have occurred in a given period of time. We will discuss these tools in more detail as we get into the class (and we’ll use some of them on our codes).</p>
<p>As with everything else, there are tradeoffs in running a profiler: methods that provide fine-grained information can produce a <em>lot</em> of data, enough that storing and processing profiling data can itself be a challenge. There is also an issue that very fine-grained measurement can interfere with the software being measured. It is often helpful to start with a relatively crude, lightweight profiling technology in order to first find what’s interesting, then focus on the interesting bits for more detailed experimentation.</p>
</section>
<section id="shoulders-of-giants" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="shoulders-of-giants"><span class="header-section-number">3.5.3</span> Shoulders of giants</h3>
<p>A good computational kernel is a general building block with a simple interface that does a fair amount of work. We want kernels to be general in order to amortize the work of tuning. We also ideally like kernels with high operational intensity, though not all kernels will have this property.</p>
<p>We have already discussed the BLAS as an example of kernel design, and noted that the high arithmetic intensity of level 3 BLAS (e.g.&nbsp;matrix-matrix multiplication) makes it a particularly useful building block for high-performance code. Other common kernel operations include</p>
<ul>
<li>Applying a sparse matrix to a vector (or powers of a sparse matrix)</li>
<li>Computing a discrete Fourier transform</li>
<li>Sorting a list</li>
</ul>
<p>Code written using common kernel interfaces is <em>transportable</em>: the implementation will differ from plarform to platform depending on architectural details, but the common interface means that code that uses the different kernel implementations will run the same way. It is critical to get properly tuned implementations of these kernels — for example, linear algebra codes run with the reference BLAS library invariably have disappointing performance.</p>
<p>There are some cases when we want to be careful with general-purpose kernels. For example, for some types of structured matrices, the general matrix-matrix multiply routines from the BLAS may have higher operational complexity than a specialized implementation. Whether “higher operational complexity” means “more run time” depends on the size of the problem and the arithmetic rate that a specialized implementation attains — it is sometimes worthwhile to write the more specialized code, but not always! As another example, in some situations (like finite element codes or computer graphics), we may want to do many operations with small matrices (e.g.&nbsp;3-by-3 or 4-by-4), and the BLAS calls may not be as efficient for such small <span class="math inline">\(n\)</span>.</p>
</section>
<section id="help-tools-help-you" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="help-tools-help-you"><span class="header-section-number">3.5.4</span> Help tools help you</h3>
<p>Compilers, including the Julia compiler, are often quite effective at local optimizations, particularly those restricted to a “basic block” (straight-line code with no conditionals or loops). Loop optimizations are somewhat harder, and global optimizations that cross function boundaries are much harder. In practice, this means that very local optimizations are usually only effective when the programmer has information that the compiler might not have, but humans can help the compiler much more with figuring out global optimizations.</p>
<p>To take some concrete examples: compilers are better than humans at register allocation and instruction scheduling, and usually at branch joins and jump elimination. Indeed, at this level it is hard for humans to even attempt to interfere with what the compiler is doing! For operations like constant folding and propagation or the elimination of common subexpressions, the compiler might do a good job <em>if</em> it can figure out that there are no side effects (like I/O or changing the contents of an array) – but the compiler might not easily be able to establish that there are no side effects without some help<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Compilers are often good at simple loop transformations involving loop invariant code motion, unrolling, and vectorization. But compilers are also usually conservative about using algebraic identities to transform code<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, for example.</p>
<p>Apart from using tools for writing our code in performance-friendly Julia (as discussed in <a href="#sec-performance-julia" class="quarto-xref"><span>Section 3.6</span></a>), we can help the compiler in two ways. First, we recognize that the compiler mostly looks at a little bit of the code at a time, and does not know the higher-level semantics of that code. Complex algebraic transformations are usually up to the programmer. Second, the compiler is really good at dealing with simple code without too many dependencies. In particular, we want to avoid very complex loops or conditional statements, as well as tricky use of functional programming constructs in performance-sensitive inner loops.</p>
</section>
<section id="tune-data-structures" class="level3" data-number="3.5.5">
<h3 data-number="3.5.5" class="anchored" data-anchor-id="tune-data-structures"><span class="header-section-number">3.5.5</span> Tune data structures</h3>
<p>As we have seen, memory access and communication patterns are critical to performance. The system is designed to make it fast to process small amounts of data that fit in cache, ideally by going through it in memory order (a “unit-stride” access pattern). But this is not the most natural access pattern for all the codes we might want to write! Consequently, tuning often involves looking not at <em>code</em> but at the <em>data</em> that the code manipulate: rearranging arrays for unit stride, simplifying structures with many levels of indirection,using single precision for high-volume floating point data, etc. With a proper interface abstraction, the fastest way to high performance often involves replacing a low-performance data structure with an equivalent high-performance structure.</p>
<p>While we are mainly concerned with accessing data structures (reads and writes), access is not the only cost. Allocation and deallocation also cost something, as does garbage collection. It is easy to end up paying hidden allocation costs (often followed by hidden copying costs). Fortunately, Julia provides us with diagnostics to identify memory allocations; and with some care, we can write code to pre-allocate key data structures.</p>
<p>Matrices are a key data structure in many numerical codes. In Julia, as in MATLAB and Fortran, matrices are stored in a column-major format, with each column layed out consecutively in memory. Consequently, we usually prefer to process data column-by-column (rather than row-by-row). For example, the code</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>n</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>m</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        y[i] <span class="op">+=</span> A[i,j]<span class="op">*</span>x[j]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>will run significantly faster than</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>m</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>n</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        y[i] <span class="op">+=</span> A[i,j]<span class="op">*</span>x[j]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As another example, consider the layout of many instances of a particular structure type. We could organize this as an array of structures, which is good for locality of access to the data for one item; or we could organize a structure of arrays, which may be more friendly to vectorization. Particularly for read-only access patterns, we might also consider a <em>copy optimization</em> where we keep around both data structures<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, and use whichever data structure gives us the best performance in context.</p>
</section>
</section>
<section id="sec-performance-julia" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="sec-performance-julia"><span class="header-section-number">3.6</span> Performance in Julia</h2>
<p>So far, we have focused on general performance issues that are mostly relevant across languages. However, there are some things that are more specific to Julia.</p>
<section id="measurement-tools" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="measurement-tools"><span class="header-section-number">3.6.1</span> Measurement tools</h3>
<p>We repeat the advice of <a href="#sec-performance-principles" class="quarto-xref"><span>Section 3.5</span></a>: you should measure your code before tuning it! Fortunately, Julia provides several macros that measure runtime and memory allocation:</p>
<ul>
<li><code>@time</code> prints out runtime and allocation information</li>
<li><code>@timev</code> prints out a more verbose version of the timing and allocation information</li>
<li><code>@elapse</code> prints just the runtime</li>
<li><code>@timed</code> returns a structure that includes run time, memory allocation, and information about garbage collection</li>
<li><code>@elapsed</code> returns the elapsed time</li>
<li><code>@allocated</code> returns the total number of bytes allocated by the expression</li>
<li><code>@allocations</code> returns the numbre of allocations in the expression</li>
</ul>
<p>For more elaborate timing with tabular outputs, we can use the <a href="https://github.com/KristofferC/TimerOutputs.jl"><code>TimerOutputs.jl</code></a> package. The <code>@time</code> macros and even the more elaborate <code>TimerOutputs.jl</code> package time a single run of a function. The <a href="https://github.com/JuliaCI/BenchmarkTools.jl"><code>BenchmarkTools.jl</code></a> package runs a code multiple times in order to get more accurate timing information.</p>
<p>The Julia <a href="https://docs.julialang.org/en/v1/manual/profile/"><code>Profile</code></a> package implements a statistical profiler. Statistics are gathered by running an expression with the <code>@profile</code> macro. There are several different visualization interfaces that can be used to view the profiling results, including a particularly nice visualizer built into VS Code.</p>
</section>
<section id="avoiding-boxing" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="avoiding-boxing"><span class="header-section-number">3.6.2</span> Avoiding boxing</h3>
<p>In dynamically-typed languages, the types of values are often only known at runtime. Hence, the system keeps “boxes” that contain both type information and the value. The coding for this pair varies from system to system<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>, but the practice of boxing in general can cause problems with performance. The run-time system has to switch between different operations depending on the type, which introduces branches in the code for many operations and makes it effectively impossible for the compiler to do code transformations like vectorization. Boxed values also generally take more storage than unboxed values would.</p>
<p>Though Julia is a dynamically-typed language, it can often produce low-level code that avoids boxing. To do this, Julia uses a just-in-time (JIT) compiler that instantiates specialized versions of methods depending on the concrete type signature of the inputs<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. The just-in-time compilation process is necessary in part because it would be ridiculously expensive to produce specialized implementations of generic methods for every possible compatible type; however, most functions are only invoked with a small number of type signatures in any given program, so most conceivable implementations never need to be generated in practice. For a <em>type stable</em> method, the system can determine the concrete type of the return value from the concrete input type signature. Ideally, we would like type stability not just for the return value, but for other intermediate values and variables as well. If the JIT compiler can reliably determine the concrete types of quantities, it can work with that data directly without boxes.</p>
<p>The <a href="https://docs.julialang.org/en/v1/stdlib/InteractiveUtils/#InteractiveUtils.@code_warntype"><code>@code_warntype</code></a> macro in Julia does type inference for a particular method call and colors in red any expressions that represent a performance hazard because the type system can only determine an abstract type. In situations where it is convenient to allow abstract types on input but performance still matters, the <a href="https://docs.julialang.org/en/v1/manual/performance-tips/">Julia performance tips</a> recommends putting the inner part of the function into its own separate kernel function that is type stable (the “function barrier technique”).</p>
<p>Several other performance issues in Julia are associated with the potential need for boxing when a concrete type cannot be inferred. For example, the Julia manual recommends avoiding untyped globals, because there is possibility that they might be assigned to different types, so they might be boxed; this also poses is a hazard to the type stability of any method that uses them. We should also avoid containers of abstract types, since the items in the container then have to be boxed; and, for the same reason, we should be careful with structures with abstract types for fields. Finally, while Julia’s functional programming features are convenient, when creating a closure it is easy to run into unanticipated boxing of captured variables from the surrounding environment.</p>
</section>
<section id="temporary-issues" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="temporary-issues"><span class="header-section-number">3.6.3</span> Temporary issues</h3>
<p>Vector operations in Julia (addition and scalar multiplication) produce temporaries. For example, if <code>x</code> and <code>y</code> are vector variables, then the function</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">f1</span>(x,y) <span class="op">=</span> x<span class="op">.^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span>x<span class="op">.*</span>y <span class="op">+</span> y<span class="op">.^</span><span class="fl">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>will generate temporaries for <code>x.^2</code>, <code>2x</code>, <code>2x.*y</code>, and <code>y.^2</code> (as well as storage for the final result). If we use dotted operations throughout, then Julia fuses the operations and does not produce temporaries:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">f2</span>(x,y) <span class="op">=</span> x<span class="op">.^</span><span class="fl">2</span> <span class="op">.+</span> <span class="fl">2.0</span> <span class="op">.*</span> x <span class="op">.*</span> y <span class="op">.+</span> y<span class="op">.^</span><span class="fl">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>An equivalent, but less verbose, option is to use the <code>@.</code> macro to make a fused broadcast version of the whole expression:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">f3</span>(x,y) <span class="op">=</span> @. x<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span>x<span class="op">*</span>y <span class="op">+</span> y<span class="op">^</span><span class="fl">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We could also use a broadcast call to a helper function, which again will avoid allocating temporaries:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">f4scalar</span>(x, y) <span class="op">=</span> x<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span>x<span class="op">*</span>y <span class="op">+</span> y<span class="op">^</span><span class="fl">2</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">f4</span>(x, y) <span class="op">=</span> <span class="fu">f4scalar</span>.(x, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>On the machine on which this is being written, the function <code>f1</code> takes about five times the memory and six times the run time of any of the other versions for input vectors <code>x</code> and <code>y</code> of length a million.</p>
<p>If we pre-allocate storage for the result, we do not need <em>any</em> allocations:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">f5!</span>(result, x, y)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    @. result <span class="op">=</span> x<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span>x<span class="op">*</span>y <span class="op">+</span> y<span class="op">^</span><span class="fl">2</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>When <code>x</code> and <code>y</code> are long vectors, there is negligible runtime performance difference between writing to a pre-allocated vector and producing a newly-allocated output vector. If the vectors are short, though, we may start to notice the cost of allocating (and garbage collecting) these temporaries, particularly if the code has a large working set and more vectors leads to more cache pressure. On the other hand, if the input vectors are short <em>and</em> their sizes are known at compile time, we may also want to consider declaring them to be static arrays (using the <a href="https://github.com/JuliaArrays/StaticArrays.jl"><code>StaticArrays.jl</code> package</a>).</p>
<p>Another place where Julia allocates new storage is in slicing operations used to specify a subarray. This happens even if the “slice” is the whole array. For example, if <code>x</code> is a vector in Julia, writing</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>assigns the name <code>z</code> to the same vector object, while writing</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> x[<span class="op">:</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>creates a new copy of <code>x</code>. If we want to refer to the entries of a subarray without creating a copy, we can create a “view” object (a <code>SubArray</code>) by either calling the <code>view</code> function or using the <code>@views</code> macro:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> x[<span class="fl">1</span><span class="op">:</span><span class="fl">10</span>]        <span class="co"># Copy of the start of x</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> <span class="fu">view</span>(x, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>)  <span class="co"># View of the start of x</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="pp">@views</span> z2 <span class="op">=</span> x[<span class="fl">1</span><span class="op">:</span><span class="fl">10</span>] <span class="co"># View of the start of x</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>z1[<span class="fl">1</span>] <span class="op">=</span> <span class="fl">100</span>         <span class="co"># Does not alter x, only z1</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>z2[<span class="fl">1</span>] <span class="op">=</span> <span class="fl">100</span>         <span class="co"># Now x[1] == 100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In general, many of the linear algebra functions in Julia provide a mutating version that writes results into user-provided storage. This includes factorization routines (e.g.&nbsp;<code>cholesky!</code>), solves (with <code>ldiv!</code>), and matrix-vector or matrix-matrix products (with <code>mul!</code>). But using these mutating operations is more error-prone, and we do not recommend it until a timing experiment or a performance model suggests that allocating new storage might cause a problem.</p>
</section>
<section id="performance-annotations" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="performance-annotations"><span class="header-section-number">3.6.4</span> Performance annotations</h3>
<p>Julia provides several macros to tell the compiler to try certain types of transformations. For example,</p>
<ul>
<li><code>@inbounds</code>: tells the compiler that there is no need to check that array accesses are in bounds</li>
<li><code>@fastmath</code>: tells the compiler that it is OK to apply certain floating point transformations that are not equivalent to the original code (e.g.&nbsp;by allowing the compiler to pretend floating point addition and multiplication are associative)</li>
<li><code>@simd</code>: tells the compiler to try vectorizing the code through SIMD instructions. <code>@simd ivdep</code> promises that loop iterations are independent and references are free from aliasing.</li>
</ul>
<p>There are a number of packages that provide additional transformations (e.g.&nbsp;<code>@turbo</code> from <a href="https://github.com/JuliaSIMD/LoopVectorization.jl"><code>LoopVectorization.jl</code></a>.</p>
<p>We do not recommend using performance annotation macros until and unless a timing experiment or performance model suggests that it addresses a bottleneck. These macros effectively prompting the compiler to do certain transformations it might not otherwise do. If the compiler can deduce that these transformations are equivalent to the original code and will result in a performance improvement, the macros will often be unnecessary. If the compiler cannot deduce that these transformations are equivalent to the original code, but you as the human author think that they are obviously equivalent, you may want to double-check your assumptions of what is “obvious” before proceeding.</p>
</section>
</section>
<section id="sec-performance-misconceptions" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="sec-performance-misconceptions"><span class="header-section-number">3.7</span> Misconceptions and deceptions</h2>
<blockquote class="blockquote">
<p>It ain’t ignorance causes so much trouble; it’s folks knowing so much that ain’t so.<br>
– <a href="http://www.famous-quotes.com/author.php?page=3&amp;total=81&amp;aid=733">Josh Billings</a></p>
</blockquote>
<p>One of the common findings in pedagogy research is that an important part of learning an area is overcoming common <em>misconceptions</em> about the area; see, e.g. <span class="citation" data-cites="leonard-kalinowski-andrews-2014">Leonard, Kalinowski, and Andrews (<a href="../references.html#ref-leonard-kalinowski-andrews-2014" role="doc-biblioref">2014</a>)</span>, <span class="citation" data-cites="sadler-et-al-2013">Sadler et al. (<a href="../references.html#ref-sadler-et-al-2013" role="doc-biblioref">2013</a>)</span>, <span class="citation" data-cites="muller-2008">Muller (<a href="../references.html#ref-muller-2008" role="doc-biblioref">2008</a>)</span>. And there are certainly some common misconceptions about high performance computing! Some misconceptions are exacerbated by bad reporting, which leads to deceptions and delusions about performance.</p>
<section id="incorrect-mental-models" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="incorrect-mental-models"><span class="header-section-number">3.7.1</span> Incorrect mental models</h3>
<section id="algorithm-implementation" class="level4" data-number="3.7.1.1">
<h4 data-number="3.7.1.1" class="anchored" data-anchor-id="algorithm-implementation"><span class="header-section-number">3.7.1.1</span> Algorithm = implementation</h4>
<p>We can never time algorithms. We only time implementations, and implementations vary in their performance. In some cases, implementations may vary by orders of magnitude in their performance!</p>
</section>
<section id="asymptotic-cost-is-always-what-matters" class="level4" data-number="3.7.1.2">
<h4 data-number="3.7.1.2" class="anchored" data-anchor-id="asymptotic-cost-is-always-what-matters"><span class="header-section-number">3.7.1.2</span> Asymptotic cost is always what matters</h4>
<p>We can’t time algorithms, but we can reason about their asymptotic complexity. When it comes to scaling for large <span class="math inline">\(n\)</span>, the asymptotic complexity can matter a lot. But comparing the asymptotic complexity of two algorithms for modest <span class="math inline">\(n\)</span> often doesn’t make sense! QuickSort may not always be the fastest algorithm for sorting a list with ten elements…</p>
</section>
<section id="simple-serial-execution" class="level4" data-number="3.7.1.3">
<h4 data-number="3.7.1.3" class="anchored" data-anchor-id="simple-serial-execution"><span class="header-section-number">3.7.1.3</span> Simple serial execution</h4>
<p>Hardware designers go to great length to present us with the <em>interface</em> that modern processor cores execute a instructions sequentially. But this <em>interface</em> is not the actual <em>implementation</em>. Behind the scenes, a simple stream of x86 instructions may be chopped up into micro-instructions, scheduled onto different functional units acting in parallel, and executed out of order. The <em>effective behavior</em> is supposed to be consistent with sequential execution – at least, that’s what happens on one core – but that illusion of sequential execution does not extend to performance.</p>
</section>
<section id="flops-are-all-that-count" class="level4" data-number="3.7.1.4">
<h4 data-number="3.7.1.4" class="anchored" data-anchor-id="flops-are-all-that-count"><span class="header-section-number">3.7.1.4</span> Flops are all that count</h4>
<p>Data transfers from memory to the processor are often more expensive than the computations that are run on that data.</p>
</section>
<section id="flop-rates-are-what-matter" class="level4" data-number="3.7.1.5">
<h4 data-number="3.7.1.5" class="anchored" data-anchor-id="flop-rates-are-what-matter"><span class="header-section-number">3.7.1.5</span> Flop rates are what matter</h4>
<p>What matters is time to solution. Often, the algorithms that get the best flop rates are not the most asymptotically efficient methods; as a consequence, a code that uses the hardware less efficiently (in terms of flop rate) may still give the quickest time to solution.</p>
</section>
<section id="all-speedup-is-linear" class="level4" data-number="3.7.1.6">
<h4 data-number="3.7.1.6" class="anchored" data-anchor-id="all-speedup-is-linear"><span class="header-section-number">3.7.1.6</span> All speedup is linear</h4>
<p>See the comments above about Amdahl’s law and Gustafson’s law. We rarely achieve linear speedup outside the world of embarrassingly parallel applications.</p>
</section>
<section id="all-applications-are-equivalent" class="level4" data-number="3.7.1.7">
<h4 data-number="3.7.1.7" class="anchored" data-anchor-id="all-applications-are-equivalent"><span class="header-section-number">3.7.1.7</span> All applications are equivalent</h4>
<p>Performance depends on the nature of the computation, the nature of the implementation, and the nature of the hardware. Extrapolating performance from one computational style, implementation, or hardware platform to another is something that must be done very carefully.</p>
</section>
</section>
<section id="deceptions-and-self-deceptions" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="deceptions-and-self-deceptions"><span class="header-section-number">3.7.2</span> Deceptions and self-deceptions</h3>
<p>The article “Twelve Ways to Fool the Masses When Giving Performance Results on Parallel Computers” (<span class="citation" data-cites="bailey-1991">Bailey (<a href="../references.html#ref-bailey-1991" role="doc-biblioref">1991</a>)</span>) is a classic in performance analysis. It’s still worth reading (as are various follow-up pieces – see the Further Reading section), and highlights issues that we still see now. To summarize slightly, here’s my version of the list of common performance deceptions:</p>
<section id="unfair-comparisons-and-strawmen" class="level4" data-number="3.7.2.1">
<h4 data-number="3.7.2.1" class="anchored" data-anchor-id="unfair-comparisons-and-strawmen"><span class="header-section-number">3.7.2.1</span> Unfair comparisons and strawmen</h4>
<p>A common sin in scaling studies is to compare the performance of a parallel code on <span class="math inline">\(p\)</span> processors against the performance of the <em>same code</em> with <span class="math inline">\(p = 1\)</span>. This ignores the fact that the parallel code may have irrelevant overheads, or (worse) that there may be a better organization for a single processor. Consequently, the speedups no longer reflect the reasonable expectation of the reader that this is the type of performance improvement they might see when going to a good parallel implementation from a <em>good</em> serial implementation. Of course, it’s also possible to see great speedups by comparing a bad serial implementation to a correspondingly bad <em>parallel</em> implementation: a lot of unnecessary work can hide overheads.</p>
<p>A similar issue arises when computing with accelerators. Enthusiasts of GPU-accelerated codes often claim order of magnitude (or greater) performance improvements over using a CPU alone. Often, this comes from explicitly tuning the GPU code and not the CPU code (see, e.g., <span class="citation" data-cites="vuduc-et-al-2010">Vuduc et al. (<a href="../references.html#ref-vuduc-et-al-2010" role="doc-biblioref">2010</a>)</span>).</p>
</section>
<section id="using-the-wrong-measures" class="level4" data-number="3.7.2.2">
<h4 data-number="3.7.2.2" class="anchored" data-anchor-id="using-the-wrong-measures"><span class="header-section-number">3.7.2.2</span> Using the wrong measures</h4>
<p>If what you care about is time to solution, you might not really care so much about watts per GFlop (though you certainly do if you’re responsible for supplying power for an HPC installation). More subtlely, you don’t necessarily care about scaled speedup if the natural problem size is fixed (e.g.&nbsp;in some graph processing applications).</p>
</section>
<section id="deceptive-plotting" class="level4" data-number="3.7.2.3">
<h4 data-number="3.7.2.3" class="anchored" data-anchor-id="deceptive-plotting"><span class="header-section-number">3.7.2.3</span> Deceptive plotting</h4>
<p>There are so many ways this can happen:</p>
<ul>
<li><p>Use of a log scale when one ought to have a linear scale, and vice-versa;</p></li>
<li><p>Choosing an inappropriately small range to exaggerate performance differences between near-equivalent options;</p></li>
<li><p>Not marking data points clearly, so that there is no visual difference between data falling on a straight line because it closely follows a trend and data falling on a straight line because there are two points.</p></li>
<li><p>Hiding poor scalability by plotting absolute time vs numbers of processors so that nobody can easily see that the time for 100 processors (a small bar relative to the single-processor time) is equivalent to the time for 200 processors.</p></li>
<li><p>And more!</p></li>
</ul>
<p>Plots allow readers to absorb trends very quickly, but it also makes it easy to give wrong impressions.</p>
</section>
<section id="too-much-faith-in-models" class="level4" data-number="3.7.2.4">
<h4 data-number="3.7.2.4" class="anchored" data-anchor-id="too-much-faith-in-models"><span class="header-section-number">3.7.2.4</span> Too much faith in models</h4>
<p>Any model has limits of validity, and extrapolating outside those limits leads to nonsense. Treat with due skepticism claims that – according to some model – a code will run an order of magnitude faster in an environment where it has not yet been run.</p>
</section>
<section id="undisclosed-tweaks" class="level4" data-number="3.7.2.5">
<h4 data-number="3.7.2.5" class="anchored" data-anchor-id="undisclosed-tweaks"><span class="header-section-number">3.7.2.5</span> Undisclosed tweaks</h4>
<p>There are many ways to improve performance. Sometimes, better hardware does it; sometimes, better tuned code; sometimes, algorithmic improvements. Claiming that a jump in performance comes from a new algorithm without acknowledging differences in the level of tuning effort, or acknowledging non-algorithmic changes (e.g.&nbsp;moving from double precision to single precision) is deceptive, but sadly common. Hiding tweaks in the fine print in the hopes that the reader is skimming doesn’t make this any less deceptive!</p>
</section>
</section>
<section id="rules-for-presenting-performance-results" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="rules-for-presenting-performance-results"><span class="header-section-number">3.7.3</span> Rules for presenting performance results</h3>
<p>In the introduction to <span class="citation" data-cites="bailey-2010">Bailey, Lucas, and Williams (<a href="../references.html#ref-bailey-2010" role="doc-biblioref">2010</a>)</span>, David Bailey suggests nine guidelines for presenting performance results without misleading the reader. Paraphrasing only slightly, these are:</p>
<ol type="1">
<li><p>Follow rules on benchmarks</p></li>
<li><p>Only present actual performance, not extrapolations</p></li>
<li><p>Compare based on comparable levels of tuning</p></li>
<li><p>Compare wall clock times (not flop rates)</p></li>
<li><p>Compute performance rates from consistent operation counts based on the best serial codes.</p></li>
<li><p>Speedup should compare to best serial version. Scaled speedup plots should be clearly labeled and explained.</p></li>
<li><p>Fully disclose information affecting performance: 32/64 bit, use of assembly, timing of a subsystem rather than the full system, etc.</p></li>
<li><p>Don’t deceive skimmers. Take care not to make graphics, figures, and abstracts misleading, even in isolation.</p></li>
<li><p>Report enough information to allow others to reproduce the results. If possible, this should include</p>
<ul>
<li>The hardware, software and system environment</li>
<li>The language, algorithms, data types, and coding techniques used</li>
<li>The nature and extent of tuning</li>
<li>The basis for timings, flop counts, and speedup computations</li>
</ul></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bailey-1991" class="csl-entry" role="listitem">
Bailey, David H. 1991. <span>“Twelve Ways to Fool the Masses When Giving Performance Results on Parallel Comptuers.”</span> <em>Supercomputing Review</em>, August. <a href="https://www.davidhbailey.com/dhbpapers/twelve-ways.pdf">https://www.davidhbailey.com/dhbpapers/twelve-ways.pdf</a>.
</div>
<div id="ref-bailey-2010" class="csl-entry" role="listitem">
Bailey, David H., Robert F. Lucas, and Samuel Williams, eds. 2010. <em>Performance Tuning of Scientific Applications</em>. CRC Press. <a href="https://doi.org/10.1201/b10509">https://doi.org/10.1201/b10509</a>.
</div>
<div id="ref-patterson-hennesey-6e" class="csl-entry" role="listitem">
Hennessey, John L., and David A. Patterson. 2017. <em>Computer Architecture: A Quantitative Approach</em>. Sixth. Elsevier.
</div>
<div id="ref-knuth-1974" class="csl-entry" role="listitem">
Knuth, Donald E. 1974. <span>“Structured Programming with <span class="nocase">go to</span> Statements.”</span> <em>Computing Surveys</em> 6 (4).
</div>
<div id="ref-leonard-kalinowski-andrews-2014" class="csl-entry" role="listitem">
Leonard, Mary J., Steven T. Kalinowski, and Tessa C. Andrews. 2014. <span>“Misconceptions Yesterday, Today, and Tomorrow.”</span> <em>CBE – Life Sciences Education</em> 13 (2). <a href="https://doi.org/10.1187/cbe.13-12-0244">https://doi.org/10.1187/cbe.13-12-0244</a>.
</div>
<div id="ref-muller-2008" class="csl-entry" role="listitem">
Muller, Derek A. 2008. <span>“Designing Effective Multimedia for Physics Education.”</span> PhD thesis, University of Sydney.
</div>
<div id="ref-sadler-et-al-2013" class="csl-entry" role="listitem">
Sadler, Philip M., Gerhard Sonnert, Harold P. Coyle, Nancy Cook-Smith, and Jaimie L. Miller. 2013. <span>“The Influence of Teachers’ Knowledge on Student Learning in Middle School Physical Science Classrooms.”</span> <em>American Educational Research Journal</em> 50 (5). <a href="https://doi.org/10.3102/0002831213477680">https://doi.org/10.3102/0002831213477680</a>.
</div>
<div id="ref-vuduc-et-al-2010" class="csl-entry" role="listitem">
Vuduc, Richard, Aparna Chandramowlishwaran, Jee Choi, Kenneth Czechowski, Marat Guney, Logan Moon, and Aashay Shringarpure. 2010. <span>“On the Limits of <span>GPU</span> Acceleration.”</span> In <em>HotPar’10: Proceedings of the 2nd USENIX Conference on Hot Topics in Parallelism</em>. Berkeley, CA. <a href="https://doi.org/10.5555/1863086.1863099">https://doi.org/10.5555/1863086.1863099</a>.
</div>
<div id="ref-williams-waterman-patterson-2009" class="csl-entry" role="listitem">
Williams, Samuel, Andrew Waterman, and David Patterson. 2009. <span>“Roofline: An Insightful Visual Performance Model for Multicore Architectures.”</span> <em>Communications of the ACM</em> 52 (4). <a href="https://doi.org/10.1145/1498765.1498785">https://doi.org/10.1145/1498765.1498785</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen’s algorithm</a> for matrix-matrix multiplication has better asymptotic complexity than the standard algorithm, at <span class="math inline">\(O(n^{\log_2 7})\)</span> running time. Despite the better asymptotic complexity, Strassen’s algorithm is at best an improvement for rather large matrices, and is not used in most BLAS libraries.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>These numbers are for a Firestorm core (performance core) on an Apple M1 Pro, taken from a <a href="https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/2">review on Anandtech</a>. These cores have four floating point pipelines, each of which can execute operations on 128-bit vectors (two double-precision floating point numbers). At 3.2 GHz, this corresponds to a processor bandwidth of 25.6 floating point operations (adds or multiplies) per nanosecond. We estimate the latency to main memory at about 100 ns, so about 2560 floating point operations in the time to get the first byte of a memory transfer from memory. Memory bandwidth to one core seems to be about 60 GB/s, or about 7.5 double-precision floating point numbers per nanosecond.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In shared-memory parallelism, we are also concerned about <em>coherence</em> misses when multiple processors write to locations in memory that are close to each other, and thus the local cache lines must be invalidated to ensure that all processors are viewing memory in the same ways. This is important, but beyond the scope of the current treatment.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For small problems, the cost of the first matrix-vector product in a series of such computations may be slower than subsequent products. This is because the first product might suffer compulsory cache misses, but afterward the cache is “warmed.”<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The quote about “premature optimization” in <span class="citation" data-cites="knuth-1974">Knuth (<a href="../references.html#ref-knuth-1974" role="doc-biblioref">1974</a>)</span> is probably the best known quote from this paper, but the rest of the paper is worth reading as well.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This assumes that our back-of-the-envelope performance models are at least somewhat correct.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Some wag once called this process “deslugging.”<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>A function that always runs the same way with no side effects is called a <em>pure</em> function. Though Julia has some support for functional programming, unlike some languages, it does not assume functions are pure by default. There is a <code>@pure</code> macro in Julia for declaring functions are pure, but it is easily abused (enough so that the Julia developers have declared that it should only be used in the <code>Base</code> packages).<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>We <em>want</em> compilers to be conservative about applying some types of algebraic transformations to floating point code, for reasons we will discuss in <a href="../01-Fund1d/02-Arithmetic.html" class="quarto-xref"><span>Chapter 9</span></a>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Keeping multiple data structures representing the same data is sometimes good for performance, but violates the “don’t repeat yourself” advice from <a href="01-Julia.html" class="quarto-xref"><span>Chapter 2</span></a>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>A common representation of boxed values is via a tagged union: a structure with an tag (usually stored as an int) and then a union whose bits can be interpreted as an integer, floating point number, pointer to a more complex object, etc. Some language implementations use more elaborate compact representations to pack both data and type information into a 64-bit word (e.g.&nbsp;by using NaN encodings in floating point to signal non-floating-point types, or using the low-order bits of aligned pointers as a type tag).<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>The JIT compiler operates on functions. The system does not compile script-style code, or code entered at the REPL that is not inside a function. If you want performance, put it in a function!<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../00-Background/01-Julia.html" class="pagination-link" aria-label="Julia programming">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Julia programming</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../00-Background/03-LA.html" class="pagination-link" aria-label="Linear Algebra">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>