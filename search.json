[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Methods for Data Science",
    "section": "",
    "text": "Preface\nThis is an incomplete draft of a text in progress on Numerical Methods for Data Science that I am working on during my sabbatical in Spring 2024. It is being written using Quarto as a typesetting system and Julia as the programming language for most of the computational examples. This project began its life as a text to accompany Cornell CS 6241: Numerical Methods for Data Science, a graduate course at Cornell designed for a target audience of beginning graduate students with a firm foundation in linear algebra, probability and statistics, and multivariable calculus, along with some background in numerical analysis. The focus is on numerical methods, with an eye to how thoughtful design of numerical methods can help us solve problems of data science.\nOver several iterations of the course, I have seen students from a variety of backgrounds. They often have some grounding in computational statistics, machine learning, or data analysis in other disciplines, which is helpful. But the majority have not had even a semester introductory survey in numerical analysis, let alone a deeper dive. It also became clear that the list of topics was overly ambitious for a one-semester course, even for students with a strong numerical analysis background.\nHence, the new goal of this project is a textbook with three parts. The first piece is an expanded version of a “background notes” handout that I have developed over several years of teaching. This material might be covered in previous courses in computing and mathematics, but many students can use a refresher of some of the details. The second part is what I think of as “Numerical Methods applied to Data Science.” This corresponds roughly to methods covered in an undergraduate survey course covering standard topics (close to what I usually cover in Cornell CS 4220, though with some changes), with an attempt to provide examples and problems that are identifiably connected to data science. The third part is “Numerical Methods for Data Science,” and covers more advanced topics, and is appropriate for a graduate course. Both the second and the third parts are a little longer than what I can comfortably cover in a single semester, allowing the reader (or the instructor) some flexibility to pick and choose topics.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Overview and philosophy\nThe title of this course is Numerical Methods for Data Science. What does that mean? Before we dive into the course technical material, let’s put things into context. I will not attempt to completely define either “numerical methods” or “data science,” but will at least give some thoughts on each.\nNumerical methods are algorithms that solve problems of continuous mathematics: finding solutions to systems of linear or nonlinear equations, minimizing or maximizing functions, computing approximations to functions, simulating how systems of differential equations evolve in time, and so forth. Numerical methods are used everywhere, and many mathematicians and scientists focus on designing these methods, analyzing their properties, adapting them to work well for specific types of problems, and implementing them to run fast on modern computers. Scientific computing, also called Computational Science and Engineering (CSE), is about applying numerical methods — as well as the algorithms and approaches of discrete mathematics — to solve “real world” problems from some application field. Though different researchers in scientific computing focus on different aspects, they share the interplay between the domain expertise and modeling, mathematical analysis, and efficient computation.\nI have read many descriptions of data science, and have not been satisfied by any of them. The fashion now is to call oneself a data scientist and (if in a university) perhaps to start a master’s program to train students to call themselves data scientists. There are books and web sites and conferences devoted to data science; SIAM even has a journal on the Mathematics of Data Science. But what is data science, really? Statisticians may claim that data science is a modern rebranding of statistics. Computer scientists may reply that it is all about machine learning1 and scalable algorithms for large data sets. Experts from various scientific fields might claim the name of data science for work that combines statistics, novel algorithms, and new sources of large scale data like modern telescopes or DNA sequencers. And from my biased perspective, data science sounds a lot like scientific computing!\nThough I am uncertain how data science should be defined, I am certain that a foundation of numerical methods should be involved. Moreover, I am certain that advances in data science, broadly construed, will drive research in numerical method design in new and interesting directions. In this course, we will explore some of the fundamental numerical methods for optimization, numerical linear algebra, and function approximation, and see the role they play in different styles of data analysis problems that are currently in fashion. In particular, we will spend roughly two weeks each talking about\nYou will not strictly need to have a prior numerical analysis course for this course, though it will help (the same is true of prior ML coursework). But you should have a good grounding in calculus, linear algebra, and probability, as well as some “mathematical maturity.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#overview-and-philosophy",
    "href": "intro.html#overview-and-philosophy",
    "title": "1  Introduction",
    "section": "",
    "text": "Linear algebra and optimization concepts for ML.\nLatent factor models, factorizations, and analysis of matrix data.\nLow-dimensional structure in function approximation.\nFunction approximation and kernel methods.\nNumerical methods for graph data analysis.\nMethods for learning models of dynamics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#readings",
    "href": "intro.html#readings",
    "title": "1  Introduction",
    "section": "1.2 Readings",
    "text": "1.2 Readings\nIn the next chapter, we give a lightning review of some background material, largely to remind you of things you have forgotten, but also perhaps to fill in some things you may not have seen. Nonetheless, I have never believed it is possible to have too many books, and there are many references that you might find helpful along the way. All the texts mentioned here are either openly available or can be accessed as electronic resources via many university libraries. I note abbreviations for the books where there are actually assigned readings.\n\n1.2.1 General Numerics\nIf you want to refresh your general numerical analysis chops and have fun doing it, I recommend the Afternotes books by Pete Stewart. If you would like a more standard text that covers most of the background relevant to this class, you may like Heath’s book (expanded for the “SIAM Classics” edition). I was involved in a book on many of the same topics, together with Jonathan Goodman at NYU. O’Leary’s book on Scientific Computing with Case Studies is probably the closest of the lot to the topics of this course, with particularly relevant case studies. And Higham’s Accuracy and Stability of Numerical Methods is a magesterial treatment of all manner of error analysis (highly recommended, but perhaps not as a starting point).\n\nAfternotes on Numerical Analysis and Afternotes Goes to Graduate School, Stewart\nScientific Computing: An Introductory Survey, Heath\nPrinciples of Scientific Computing, Bindel and Goodman\nScientific Computing with Case Studies, O’Leary\nAccuracy and Stability of Numerical Algorithms, Higham\n\n\n\n1.2.2 Numerical Linear Algebra\nI learned numerical linear algebra from Demmel’s book, and still tend to go to it as a reference when I think about how to teach. Trefethen and Bau is another popular take, created from when Trefethen taught at Cornell CS. Golub and Van Loan’s book on Matrix Computations ought to be on your shelf if you decide to do this stuff professionally, but I also like the depth of coverage in Stewart’s Matrix Algorithms (in two volumes). And Elden’s Matrix Methods in Data Mining and Pattern Recognition is one of the closest books I’ve found to the spirit of this course (or at least part of it).\n\nALA: Applied Numerical Linear Algebra, Demmel\nNumerical Linear Algebra, Trefethen and Bau\nMatrix Algorithms, Vol 1 and Matrix Algorithms, Vol 2, Stewart\nMatrix Methods in Data Mining and Pattern Recognition, Elden\n\n\n\n1.2.3 Numerical Optimization\nMy go-to book on numerical optimization is Nocedal and Wright, with the book by Gill, Murray, and Wright as a close second (the two Wrights are unrelated). For the particular case of convex optimization, the standard reference is Boyd and Vandeberghe. And given how much of data fitting revolves around linear and nonlinear least squares problems, we also mention an old favorite by Bjorck.\n\nNO: Numerical Optimization, Nocedal and Wright\nPractical Optimization, Gill, Murray, and Wright\nConvex Optimization, Boyd and Vandenberghe\nNumerical Methods for Least Squares Problems, Bjorck\n\n\n\n1.2.4 Machine Learning and Statistics\nThis class is primarily about numerical methods, but the application (to tasks in statistics, data science, and machine learning) is important to the shape of the methods. My favorite book for background in this direction is Hastie, Tribshirani, and Friedman, but the first book I picked up (and one I still think is good) was Bishop. And while you may decide not to read the entirety of Wasserman’s book, I highly recommend at least reading the preface, and specifically the “statistics/data mining dictionary”.\n\nESL: Elements of Statistical Learning, Hastie, Tribshirani, and Friedman\nPattern Recognition and Machine Learning, Bishop\nAll of Statistics, Wasserman\n\n\n\n1.2.5 Math Background\nIf you want a quick refresher of “math I thought you knew” and prefer something beyond my own notes, Garrett Thomas’s notes on “Mathematics for Machine Learning” are a good start. If you want much, much more math for ML (and CS beyond ML), the book(?) by Gallier and Quaintance will keep you busy for some time.\n\nMathematics for Machine Learning, Thomas\nMuch more math for CS and ML, Gallier and Quaintance",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "The statisticians could retort that machine learning is itself a modern rebranding of statistics, with some justification.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "00-Background/00-Intro.html",
    "href": "00-Background/00-Intro.html",
    "title": "Background Plus a Bit",
    "section": "",
    "text": "Throughout the book, we assume the fundamentals of linear algebra, multivariable calculus, and probability. We also assume some facility with programming and either a knowledge of the basics of Julia1 or a willingness to pick it up. From this perspective, the chapters in this section of the book should be treated as review, and an impatient reader might reasonably decide to skip over them. But we counsel most readers to be patient enough to at least skim these chapters first, for two reasons:\nOf course, even for those readers who have seen all the contents of these chapters before, we hope there will be some value in seeing the material again from a different prspective.\nWe begin the section with two chapters on computing. In 2  Julia programming, we discuss the Julia programming language, focusing on the mechanics of the language along with some tips on Julia style. Those readers who are familiar with Julia (or choose to program in some other language) may still appreciate 3  Performance Basics, where we give some basics of performance analysis of computer codes. Much of this chapter is independent of the Julia programming language, though we do also give some pointers on writing fast Julia code.\nAfter discussing computing, we turn to mathematics. In 4  Linear Algebra, we review some standard topics in linear algebra, as well as a few topics in multilinear algebra. Unlike our discussion later in the book, where we will often focus on specific bases and concrete spaces, we try to keep an eye on the “basis free” properties of linear spaces and maps upon them. We also try to avoid being overly abstract by regularly tying concepts back to spaces of polynomials and providing code that implements those concepts. Similarly, regular examples in code will feature in our chapters on calculus and analysis (5  Calculus and analysis) and probability theory (7  Probability).",
    "crumbs": [
      "Background Plus a Bit"
    ]
  },
  {
    "objectID": "00-Background/00-Intro.html#footnotes",
    "href": "00-Background/00-Intro.html#footnotes",
    "title": "Background Plus a Bit",
    "section": "",
    "text": "The examples in this book will be in Julia. If you are unfamiliar with Julia but familiar with MATLAB or Octave, you should be able to read most of the code. The syntax may be slightly more mysterious if you primarily program in some other language, but I will generally assume that you have the computational maturity to figure things out.↩︎",
    "crumbs": [
      "Background Plus a Bit"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html",
    "href": "00-Background/01-Julia.html",
    "title": "2  Julia programming",
    "section": "",
    "text": "2.1 Interacting with Julia\nEntering this chapter, we assume you have written code in some other languages, but not programmed much (or at all) in Julia, the language that we will use for the code in this book. Julia is a relatively young language initially released in 2012. In comparison the first releases of MATLAB and Python were 1984 and 1991, respectively. Despite its relative youth, Julia has become popular in scientific computing for its speed, simple MATLAB-like matrix syntax, and support for a variety of programming paradigms. Among other reasons, we will use Julia for our codes because\nThe rest of this chapter is intended to get you started with Julia. It is not comprehensive, and we recommend that the curious reader explore further with the excellent Julia documentation. But we also includes some treatment of more advanced corners of the language.\nWe can use Julia non-interactively: typing julia filename.jl at the command line will execute any Julia code in the text file filename.jl. But both for developing new code and for learning the language, it is very useful to work with Julia interactively. The two most common approaches are",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#interacting-with-julia",
    "href": "00-Background/01-Julia.html#interacting-with-julia",
    "title": "2  Julia programming",
    "section": "",
    "text": "Typing julia at the command line will start an interactive Julia session, sometimes known as a read-evaluate-print loop (REPL). One can also start an interactive Julia session from within in some editors, such as Visual Studio.\nOne can interact with Julia using a notebook interface, such as Jupyter (likely the most popular) or Pluto. We view notebooks using a web browser.\n\n\n2.1.1 The Julia prompt\nRunning julia with no arguments takes us to the Julia prompt. From here, we can type and run general Julia code, though for anything too long we would usually write the code in a file. But even typing simple commands can have some surprises!\nThe Julia prompt supports many of the amentities of modern command prompts, including:\n\nHistory: One can scroll through past commands using the up and down arrows.\nEmacs-style editing: Code typed into the command prompt can be edited using navigation key bindings similar to the Emacs editor. For example, Ctrl-A and Ctrl-E jump to the beginning and end of the current line, Ctrl-K “kills” text (putting it into a buffer called the “kill ring”), and Ctrl-Y retrieves the last line from the kill ring.\nTab completion: Typing a partial command into Julia and then pressing the tab key will lead to Julia either finishing the command (if it is unambiguous). If the completion is ambiguous, pressing tab a second time will produce a list of suggested possible completions (if the completion is ambiguous).\n\nJulia allows us to use Unicode characters in code, including code written at the command line. The simplest way to enter these characters is to type a LaTeX special character command and then tab; for example \\pi followed by tab produces the character \\(\\pi\\) (and in Julia, as in mathematics, this represents \\(3.14159...\\) unless superceded by another local definition). While it is possible to use Julia without taking advantage of Unicode, in many cases it is the most concise way of writing expressions. For example, writing isapprox(x, y) or x ≈ y (rendering the approximately equal symbol by tab-completion on \\approx are equivalent in functionality. But the latter is certainly shorter and arguably more idiomatic. Several popular editors also have Julia modes that support Unicode via tab-completion of LaTeX commands, including Visual Studio, Emacs, and Vim.\nFrom the Julia prompt, one can also access\n\nHelp: We access the help system by typing the ? character and then the name of a function (or module, global variable, etc). Tab completion works in the help system as well, both for completing command names and for entering Unicode. For example, typing ?\\pi followed by a tab will get system help on the constant \\(\\pi\\).\nShell: If we would like to list directories or quickly run other programs, we can access the shell by typing the ; character. To exit shell mode, we type the backspace character at an otherwise-empty shell prompt.\nPackages: To enter the package manager, we type the ] character; when done, we can exit the package manager by typing a backspace, as with shell mode. We will have more to say about the package manager later in this chapter.\n\nPressing Ctrl-C lets us break out of a running Julia command. We can exit from Julia by typing exit() at the prompt or pressing Ctrl-D.\n\n\n2.1.2 Jupyter notebooks\nThe Jupyter notebook system was first released in 2015 as a multi-language extension to the earlier IPython notebooks. The Jupyter system supports “computational notebooks” built from cells containing text documentation written in Markdown; code written in Julia or another supported language; and code outputs, which may include both text and graphics. Early versions of Jupyter supported Julia, Python, and R (hence the name),though support for many other languages has been added since. In 2018, a more full-featured environment called JupyterLab was released, and today lives alongside the “classic” Jupyter notebook interface. Both classic Jupyter and JupyterLab use the same file formats .ipynb files), which may contain all cell types, including computed outputs.\nUsers typically interact with Jupyter through a web browser interface, though there is also support for working with Jupyter notebooks in editors like Visual Studio Code and Emacs. An example of a running notebook is shown in ?fig-tbd. The actual computations for a Jupyter noteook run in a separate “kernel” that communicates with the web browser via a network protocol. The kernel and the brower interface are separate; they may run on the same computer, but are not required to. The kernel responds as though individual cells were typed into the command prompt in the order that the user chooses to execute them, with output then redirected to be shown in the web browser. Because the order of cell execution is chosen by the user, it is not always possible to tell how a notebook arrived at some result just by looking at the output: the user might have executed cells in some unintuitive order, or perhaps a cell was run and then deleted. For this reason, when preparing to save a notebook (whether to share with collaborators or because it is time to quit for the day), we recommend restarting the kernel and re-running all cells from the start so that “what you see is what you get.”\nThe Julia kernel for Julia is provided in the IJulia package. This can be installed from the Julia command prompt by entering the package mode (by typing ] at the prompt) and typing\nadd IJulia\nOnce the IJulia package is installed and we have exited back to the normal Julia prompt, we can run Jupyter with a Julia kernel from the Julia prompt by typing\nusing IJulia\nIJulia.notebook()\nRunning Jupyter in this way results in a new installation of the Jupyter system, independent of any installations of Jupyter that may already exist on your computer. If you already have Jupyter installed (e.g. as part of a Python installation), you can tell it about the IJulia kernel by making sure the JUPYTER environment variable points to the desired jupyter executable (e.g. by assigning the path to ENV[\"JUPYTER\"] from the Julia prompt), and either running build IJulia at the package prompt, or at the Julia prompt running\nusing IJulia\ninstallkernel()\nOnce the kernel is installed, one can start Jupyter as normal (e.g. by typing jupyter lab at the shell), and then select the Julia kernel when starting a new notebook or loading an existing one.\n\n\n2.1.3 Pluto notebooks\nThe Pluto notebook is a Julia-specific take on the computational notebook concept. As with Jupyter, the user interacts with Julia computations through a web interface, with computations executing in a separate Julia process. However, there are several key differences between the two systems:\n\nPluto does not distinguish between “code cells” and “documentation cells.” The Pluto equivalent of a documentation cell is a code cell that renders Markdown text, which is then displayed as the cell output.\nA single Pluto cell can only contain one statement, though this includes things like function definitions or begin/end blocks.\nPluto maintains a dependency graph between cells that is used to determine execution order. If cell A defines a symbol that is used within cell B, then executing any update to cell A will also trigger an update of cell B, along with any other cells that depend on cell A, whether directly or indirectly. This mostly addresses the issue of uncertain execution order that sometimes causes headaches for Jupyter users. However, the dependency graph construction is not perfect, and can sometimes be confused by too-clever macros. When this happens, or when there is a change in the packages used by a Pluto notebook, it is usually a good idea to recompute the whole notebook from scratch.\nPluto notebooks are saved as regular Julia files, with special comments to delimit the cells and indicate the order in which they are displayed. Within the file, the cells are ordered according to the depencency graph, so that a Pluto notebook can also be run as an ordinary Julia program.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#simple-expressions",
    "href": "00-Background/01-Julia.html#simple-expressions",
    "title": "2  Julia programming",
    "section": "2.2 Simple expressions",
    "text": "2.2 Simple expressions\nWe start our deeper dive into Julia with a treatment of simple expressions, the very simplest of which are literals (Table 2.1).\n\n\n\nTable 2.1: Examples of literals in Julia\n\n\n\n\n\nType\nExamples\n\n\n\n\nInt\n123, 1_234\n\n\nUInt\n0x7B, 0o173 0b1111011\n\n\nFloat64\n10.1, 10.1e0, 1.01e1\n\n\nFloat32\n10.1f0, 1.01f1\n\n\nLogical\ntrue, false\n\n\nChar\n'c'\n\n\nString\n\"string\"\n\n\nSymbol\n:symbol\n\n\n\n\n\n\nOrdinary decimal integer expressions (e.g. 123) are interpreted as type Int, the system signed integer type. In principle, this will be 32-bit or 64-bit depending on the preferred default integer size for the system. In practice, most current systems are 64-bit. Unsigned integer literals can be written in hexadecimal, octal, or binary; these listerals default to the smallest type that will contain them. Floating point literals default to 64-bit, unless an exponent is included. For both integer and floating point types, underscores can be included on input as spacers; that is, 1_234 and 1234 are interpreted the same way.\nIn addition to standard numerical literals, Julia provides a few constants, including \\(\\pi\\) and \\(e\\); we type Unicode symbols for these with tab completion on \\pi and \\euler. There are also special constants for exceptional floating point values. For example, Inf or Inf64 denote double-precision floating point infinity and NaN or NaN64 denote double-precision not-a-number encodings; the single-precision (32-bit) analogues are Inf32 and NaN32.\nJulia supports string literals surrounded by either single quotes or triple quotes. Triple-quoted strings can include regular quote characters and newlines with no special characters, e.g.\nbox_quote = \"\"\"\n\"All models are wrong, but some models are useful.\"\n    -- George Box\"\"\"\nJulia characters correspond to Unicode codepoints, and strings likewise use Unicode encodings. In particular, Julia string literals are represented in program code and in memory using UTF-8, which defines a variable-length encoding for characters. This leads to some complexities in Julia string processing, which we address in Section 2.5.3.\nA symbol in Julia is represented in memory as a special type of string that is entered in a global dictionary for fast comparisons. Symbols can be used like strings that admit particularly fast comparison, but they are more important for the role that they play in metaprogramming, where evaluating a symbol corresponds to evaluating a variable named in the program.\n\n2.2.1 Assignment, scopes, and variable types\nLike many other languages, the Julia assignment operation = represents binding a name to a value. For example,\nx = 10\nassigns a name x to the value 10. Julia also allows destructuring assignments in which an ordered collection (a tuple or vector) is unpacked into multiple outputs, e.g.\nL, U, p = lu(A)  # Assign multiple outputs\nThere is also a form of destructuring assignment for when the right hand side has named quantities, e.g.\np = (x = 1.0, y = 2.0, z = 3.0)\n(; y, z) = p  # Sets y = p.y, z = p.z\nThis format works for named tuples and for structures.\nWhat is more complicated is the scope, i.e. the part of the code in which this assignment holds.\n\n2.2.1.1 Scope\nGlobal variables in Julia belong to a “top-level” namespace called the global scope. Technically, there is a separate global scope for each module, as we will describe in more detail in ?sec-tbd. But when we start a Julia session, we are working with a new (anonymous) module with its own global scope, and when we refer to “the” global scope, this is the namespace that we mean.\nCertain types of code blocks create new local scopes, or variable namespaces with bindings that are only meaningful inside the code block. Julia is lexically scoped, so the visibility of variable bindings depends on the code structure, not the execution path through the code. Local scopes can (and often are) nested. The let statement in Julia serves the sole purpose of creating a new scope with local bindings; for example:\n\nlet\n    p = 0\n    let p = 1      # New version of p\n        println(p) # Prints 1 (p from inner scope)\n    end\n    println(p)   # Prints 0 (p from outer scope)\nend\n\n1\n0\n\n\nThe let statement actually has two parts:\n\nA series of assignment statements on the same line as the let, which result in bindings of variable names that are purely local to the let;\nA code block that by default makes use of the bindings in the assignment block and any bindings in the outer scope.\n\nThe difference can be somewhat subtle. For example, a single newline can change the semantics of the previous example by moving the assignment p=0 from the assignment block of the let (where it creates a new version of p) to the code block of the let (where it re-binds the symbol p from the enclosing scope):\n\nlet\n    p = 0\n    let\n        p = 1      # Reassign outer version of p\n        println(p) # Prints 1\n    end\n    println(p)   # Prints 1 again\nend\n\n1\n1\n\n\nAt the same time, we can use the local keyword to make a version of the variable that is explicitly local to the let code block:\n\nlet\n    p = 0\n    let\n        local p = 1  # Create local version of p\n        println(p)   # Prints 1\n    end\n    println(p)   # Prints 0\nend\n\n1\n0\n\n\nJulia distinguishes between constructs that create “hard” local scopes (such as function and macro definitions or the assignment part of let blocks) and those that create “soft” local scopes (such as loops, conditionals, the code part of let statements, and try blocks), with the two differing in their treatment of assignments to existing names. For example, a for statement establishes a soft local scope, in which assignments to existing variables in enclosing scopes do not create a shadow variable:\nfunction do_more_stuff()\n    x = 0\n    for k = 1:5\n        x = k    # x is the same variable as before\n    end\n    println(x)   # Prints 5\nend\nHowever, if x were not declared before the for loop, the version inside the loop body would be purely local to the loop:\nfunction do_more_stuff()\n    for k = 1:5\n        x = k    # x is local to this iteration\n    end\n    println(x)   # Error -- x doesn't exist here!\nend\nIn any local scope, we can explicitly declare that we want to use a global variable. For example:\n\nfunction modify_global_xyz()\n    xyz = 0\n    let\n        global xyz\n        xyz = 1\n    end\nend\n\nmodify_global_xyz()\nprintln(xyz) # Prints 1\n\n1\n\n\nHowever, if we try to assign to a global variable name in a local scope without using the global keyword, different things will happen depending not only on whether the local scope is “hard” or “soft,” but also on whether we are in an interactive session or running code non-interactively:\n\nHard local scope: Always create a shadow of the global variable.\nSoft local scope: If interactive, assign to the global variable. Otherwise, create a shadow variable and issue a warning to the user.\n\n\n\n2.2.1.2 Variable types\nAll values in Julia have concrete types. When a variable in Julia is given a type, it restricts the types of values that can be assigned to the variable to only those that are compatible. For example, if we would like x to represent a floating point number, we could write:\nx :: Float64 = 0  # Implicitly converts the integer 0 to the floating point 0.0\nOnce the type of x has been specified, we cannot assign an incompatible value to x:\nx :: Float64 = 0  # OK so far, x is now floating point 0.0\nx = \"Oops\"        # Error!\nThough values must have concrete types, we can give variables abstract types. For example, Julia has many numeric types, all of which are subtypes of type Number. We can assign any numeric value to a variable declared to have type Number, but non-numeric values are not allowed:\nx :: Number = 0   # OK, x is now Int(0), and Int is a subtype of Number\nx = 0.0           # Still OK, Float64 is a subtype of Number\nx = \"Oops\"        # Error!\nWe will have much more to say about types in ?sec-tbd.\n\n\n\n2.2.2 Arithmetic operations\n\n\n\nTable 2.2: Standard arithmetic operations\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n+x\nUnary plus (identity)\n\n\n-x\nUnary minus\n\n\nx + y\nAddition\n\n\nx - y\nSubtraction\n\n\nx * y\nMultiplication\n\n\nx / y\nDivision\n\n\nx ÷ y\nInteger division\n\n\nx \\ y\nInverse divide (\\(\\equiv\\) y/x)\n\n\np // q\nRational construction\n\n\nx ^ y\nPower\n\n\nx % y\nRemainder\n\n\n\n\n\n\nWe show some of the standard Julia arithmetic operations in Table 2.2. These mostly behave similarly to other languages, but with some subtleties around division. In particular:\n\nThe ordinary division operator produces a floating point result even when the inputs are integers and the result is exactly representable as an integer. For example, 6/3 yields 2.0.\nInteger division always rounds toward zero; for example, 14 ÷ 5 yields 2, while -14 ÷ 5 yields -2. The remainder operation is consistent with integer division.\nInteger division and remainder can also be used with non-integer types; for example, 14.1 ÷ 5 yields 2.0, and 14.1 % 5 yields 4.1.\nThe backslash (inverse divide) operation can be used with any numeric types, but is most frequently used in the context of linear algebra. When A and b are a matrix and a vector, we use A\\b to compute \\(A^{-1} b\\) without computing an explicit inverse.\nThe construct p // q is used to construct a rational number. The numerator and denominator must be integers.\n\nJulia also allows for implicit multiplication when a literal number (integer or floating point) precedes a symbol with no space. For example, 2.0im produces a complex floating point value equal to \\(2i\\).\nThere are updating operations of all the arithmetic operations (except for rational construction), formed by combining the operation name with an equal sign. These are essentially equivalent to a binary operation followed by an assignment; for example,\nx += 1  # Equivalent to x = x + 1\nx *= 2  # Equivalent to x = x * 2\n\n\n2.2.3 Logical operations\nJulia supports the usual logical and p && q, or p || q, and !p constructions. The arguments to these operations must be logical; unlike some languages, we cannot interpret integers or lists as logical values, for example. The and and or operators are short-circuited, meaning that the second argument is only executed if the first argument is insufficient to determine the truth value. For example, consider the code fragment\n\nlet\n    x = true || begin println(\"Hello 1\"); true end\n    y = false || begin println(\"Hello 2\"); true end\n    x, y\nend\n\nHello 2\n\n\n(true, true)\n\n\nBoth x and y are assigned to be true, but only the string \"Hello 2\" is printed.\nJulia also supports a ternary conditional operator: p ? x : y evaluates p and returns either x or y depending whether p is true or false, respectively. Like the logical and and or, the ternary conditional operator is short-circuiting; only one of the x and y clauses will be evaluated. The conditional operator is equivalent to a Julia if statment; that is, these two statements do the same thing:\nz1 = p ? x : y\nz2 = if p then x else y end\n\n\n2.2.4 Comparisons\nJulia provides several different comparison operations. Most types support tests for structural equality (==) or inequality (!= or ≠). This is a forgiving sort of test, allowing for things like promotion of the arguments to a common type. For example, all the following expressions evaluate to true:\n\n1 == 1   &&\n1 != 2   &&\n2 == 2.0 &&\n[1; 2] == [1.0; 2.0]\n\ntrue\n\n\nThe substitutional equality operation (===) and inequality operation (!==) test whether two entities are indistinguishable. This is usually a more restrictive notion of equality. For example, all the following expressions evaluate to true:\n\n1 === 1  &&\n1 !== 2  &&\n2 !== 2.0\n\ntrue\n\n\nValues that correspond to mutable objects never satisfy ===, unless they are exactly the same object. For example, Julia allows us to change the contents of an array (it is mutable), but not the contents of a string (it is immutable). Therefore, both the following expressions are true:\n\n\"12\" === \"12\"  &&\n[1; 2] !== [1; 2]\n\ntrue\n\n\nFor the most part, substitutional equality implies structural equality, but not vice-versa. We say “for the most part” because of the example of the default not-a-number (NaN). According to the standard floating point semantics (Chapter 9), not-a-number is not equal to anything in the floating point system, including itself. But the NaN symbol does satisfy substitutional equality. Therefore, both the following are true:\n\nNaN != NaN  &&\nNaN === NaN\n\ntrue\n\n\nWe will have more to say about the vagaries of floating point equality when we get to Chapter 9. For now we simply mention that Julia also supports an approximate equality operation that can be used for comparing whether floating point quantities are within some tolerance of each other. For example, the following statements are both true:\n\n1.0 ≈ 1.00000001 &&\n1.0 ≠ 1.00000001\n\ntrue\n\n\nIn addition to testing for equality or inequality, we can test ordered types with &lt;, &gt;, &lt;= (or ≤), and &gt;= (or ≥). Ordered types include integers, floating point numbers, strings, symbols, and logicals. For strings and symbols, we use lexicographic ordering (aka alphabetic ordering); for logicals, we have false &lt; true.\n\n:a &lt; :b   &&\n\"a\" &lt; \"b\" &&\nfalse &lt; true\n\ntrue\n\n\n\n\n2.2.5 Simple calls\nSimple calls in Julia have the form function_name(args). The arguments are comma-separated, starting with any positional arguments followed by any keyword arguments. Julia uses the types of the provided positional arguments to distinguish between different methods with the same name and number of arguments. Most operators in Julia are just a thin layer of “syntactic sugar” that hide ordinary method calls. For example, writing 1+2 and +(1,2) are equivalent; but 1+2 and 1.0+2.0 invoke different methods, since the types are different (both Int for the former expression, both Float64 in the latter).\nA call may provide only the leading positional arguments if the remaining arguments are assigned default values. Keyword arguments always have a default value. For example, the linear algebra routine cholesky takes positional arguments: a matrix A and a flag variable that indicates whether the computation should use pivoting or not (defaulting to NoPivot()). In addition, Cholesky takes a logical keyword argument check that indicates whether the routine should produce an exception on failure (check == true, the default) or should produce no exception (check == false). Therefore, all the following are valid calls to cholesky\nC = cholesky(A)  # without pivoting, check result\nC = cholesky(A, check=false)  # without pivot, no check\nC = cholesky(A, RowMaximum()) # pivoted, check result\nC = cholesky(A, RowMaximum(), check=false), # pivot, no check\nJulia passes arguments using a “call-by-sharing” convention, in which names within the function are bound to the values passed in. This means that if a mutable value (e.g. an array) is passed as an argument to a function, the function is allowed to modify that argument. By convention, functions that modify their arguments end with an exclamation mark. For example, calling cholesky(A) function produces a new output matrix and leaves A alone, while cholesky!(A) modifies the matrix A.\nSometimes, it is convenient to package the positional arguments to a function into a tuple. A tuple can be unpacked into an argument list by following it with ..., sometimes called the splat operation. This is sometimes useful in function calls; for example\na = (1.0, 1.00000001)\nisapprox(a...)      # Same as isapprox(1.0, 1.00000001)\nWhile it is mostly used for calling, Julia does also allow this sort of unpacking in other contexts than function calls; for example\nb = (a, 2)          # Yields ((1.0, 1.00000001), 2)\nbunpack = (a..., 2) # Yields (1.0, 1.00000001, 2)\n\n\n2.2.6 Broadcasting operations\nFunction calls and operations can be applied elementwise to arrays using broadcasting syntax. This involves putting a dot before the operator name or after the function name. For example:\nθs = [0; π/4; π/2; 3π/4; π] cos.(θs) # [1; sqrt(2)/2; 0; -sqrt(2)/2; -1]\nθs .+ 1                              # [1; π/4+1; π/2+1; 3π/4+1; π+1]\nThe @. macro can be used to “dot-ify” everything in an expression, e.g.\nxs =range(0, 1, length=100) y = @. xs^2 + 2xs + 1.0\nevaluates the map \\(x \\mapsto x^2 + 2x + 1\\) to every element of the vector xs.\nVectorized operations are critical to performance in some languages. In Julia, they are not so performance critical. However, they are convenient for concise code.\n\n\n2.2.7 Indexing and slicing\nSquare brackets are used to access elements of an array or similar object,or to refer to sub-arrays of an array (slices). Indices are one-based; and in the context of indexing, the symbol end refers to the final index. For example,\nlst = [5; 2; 2; 3]\nlst[2]        # Evaluates to 2\nlst[end]      # Evaluates to 3\nlst[1:2]      # Evaluates to [5; 2]\nlst[[2; 1]]   # Evaluates to [2; 5]\nlst[2:end]    # Evaluates to [2; 2; 3]\nlst[2:3] .= 4 # Now lst is [5; 4; 4; 3]\nWe can index into an array to either get elements or subarrays or to assign to them. When getting elements or subarrays, the default behavior is to get a copy of the extracted data; so, for example\nlst = [5; 2; 2; 3]\nsublst = lst[1:2]    # New storage for sublst\nsublist[:] = [3; 4]  # Changes contents of sublst, not lst\nIf we want to modify a subarray in the main array from which it was extracted, we can use a view. A view has its own indexing, but uses the storage of another object. For example:\nlst = [5; 2; 2; 3]\nsublst = view(lst, 1:2)  # Provides a view into lst\nsublist[:] = [3; 4]      # lst is now [3; 4; 2; 3]\nWe frequently provide indices that are integers (to get individual elements) or arrays of integers (whether ranges like 1:2 or concrete index lists like [1 4 2]). But we can also use logical indexing, in which an iterable boolean list tells us whether the corresponding entry should be kept in the result or excluded. For example:\nlst = [5; 2; 2; 3]\nidx_big = (lst .&gt; 2)  # [true, false, false, true]\nlst[idx_big]          # Results in [5; 3]\nIn some circumstances, we also have index spaces that are fundamentally non-integer (or can be). An example is with named tuples, which can be accessed either by indexing positionally or by name. For example, consider the following tuple:\nxyz = (x=4, y=5, z=6)  # Create a named tuple\nxyz[1]                 # Returns 4 (the value of x)\nxyz[:y]                # Returns 5 (the value of y)\nOur examples so far have been for single-index arrays (vectors), but everything generalizes natural to multi-dimensional arrays.\n\n\n2.2.8 Structure access\nSeveral compound data types contain named fields, including record types (defined with the struct keyword) and named tuples (mentioned in the previous section). These fields can be accessed with the syntax val.field. For example, consider again the xyz tuple from the previous example:\nxyz = (x=4, y=5, z=6)  # Create a named tuple\nxyz.z                  # Returns 6 (structure access syntax)\nMany compound data types are immutable, so that fields cannot be assigned. For those that are mutable, though, we can also use val.field on the left hand side of an assignment statement.\n\n\n2.2.9 Strings\nOne particularly convenient feature of Julia is string interpolation. For example, consider the following Julia code:\n\nlet\n  sqrt2approx = 99.0/70.0\n  \"The error in sqrt(2)-$sqrt2approx is $(sqrt(2)-sqrt2approx)\"\nend\n\n\"The error in sqrt(2)-1.4142857142857144 is -7.215191261922271e-5\"\n\n\nIn evaluating the string, Julia replaces $sqrt2approx with the value of the variable sqrt2approx, and it replaces $(sqrt(2)-sqrt2approx) with the string representation of the number sqrt(2)-sqrt2approx. For more controlled formatting (e.g. printing only a few digits of the error), the standard recommendation is to use the @sprintf macro from the Printf package.\nIn addition to building strings by interpolation, Julia lets us build strings by concatentation. The operator * concatenates two strings; for example \"Hello \" * \"world\" produces \"Hello world\". The ^ operation on strings can also be used for repetition; for example \"Hello \"^2 produces \"Hello Hello \".",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#control-flow",
    "href": "00-Background/01-Julia.html#control-flow",
    "title": "2  Julia programming",
    "section": "2.3 Control flow",
    "text": "2.3 Control flow\nWe have already discussed one form of control flow, with short-circuit evaluation of logical and conditional operators. Beyond this, we have conditionals and loops. We treat exception handling in Section 2.6.\n\n2.3.1 Conditional statements\nJulia has an if/elseif/else/end statement structure. The statement is actually an expression, which evaluates to whatever branch is taking. For example\n\nis_leap_year(year) =\n    if year % 400 == 0\n        true # Multiples of 400 years are leap years\n    elseif year % 100 == 0\n        false # Multiples of 100 years otherwise are not\n    elseif year % 4 == 0\n        true  # Which is an exception to \"every fourth year\"\n    else\n        false # Otherwise, not a leap year\n    end\n\nIf none of the conditions are satisfied and there is no else clause, the result evaluates to nothing.\nUnlike loops, if statements do not introduce a local scope.\n\n\n2.3.2 Loops\nA while loop executes while the loop condition is true\n\nfunction gcd(a,b)\n    while b != 0\n        a, b = b, a%b\n    end\n    a\nend\n\nA for loop iterates over a collection. Perhaps most frequently, this is an integer range:\n\nfunction mydot(x, y)\n    @assert length(x) == length(y)\n    result = 0.0\n    for i = 1:length(x)\n        result += x[i]*conj(y[i])\n    end\n    result\nend\n\nHowever, we can also iterate over other collections, whether they are concrete or abstract. For example, the zip function creates an abstract collection of tuples constructed from elements of parallel lists:\n\nfunction mydot2(x, y)\n    @assert length(x) == length(y)\n    result = 0.0\n    for (xi, yi) in zip(x, y)\n        result += xi*conj(yi)\n    end\n    result\nend\n\nIn the for loop syntax, we can use =, in, or ∈ between the loop variable name and the collection to be iterated over. We can define new things that a for loop can iterate on by overloading the Base.iterate function.\nThe break statement jumps out of a loop in progress.\nBoth while loops and for loops produce “soft” local scope: references to already-used names from surrounding scopes will not be shadowed, but any new names will be bound in the local scope.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#functions-and-methods",
    "href": "00-Background/01-Julia.html#functions-and-methods",
    "title": "2  Julia programming",
    "section": "2.4 Functions and methods",
    "text": "2.4 Functions and methods\n\n2.4.1 Defining functions\nJulia provides three primary syntactic variants for defining functions:\n\nUsing the function keyword\nUsing an equational definition\nUsing the -&gt; operator\n\nThe function keyword is the standard choice for defining complicated functions that might involve several statements. We have seen a few examples already, but for now let’s consider again the Euclidean algorithm for the greatest common denominator, seen in Section 2.3.2:\nfunction gcd(a, b)\n    while b != 0\n        a, b = b, a%b\n    end\n    a\nend\nThe defined function gcd takes two positional arguments (a and b). On a function call, the concrete arguments to the call are bound to the names a and b in a local scope, and then the body of the function is then evaluated to obtain a return value. We note that as with other compound statements in Julia (e.g. begin/end blocks and let/end blocks), the value of the function body is the last expression in the body – in this case, the final value of a. We could also write return a, but this is not strictly necessary in this case. In general, return statements are only really needed when we want to exit before reaching the end of a function body.\nFor short expressions, it is more concise to define functions using Julia’s equational syntax:\nN0(x) = (x-1.0)*(x-2.0)*(x-3.0)/6.0\nThe function body on the right-hand side of the function definition can be any valid Julia expression, including a compound statement inside a begin/end constrution. Conventionally, though, we use simple expressions for the function bodies when using this syntax.\nOur examples thus far have all involved named functions, but Julia also supports anonymous functions (sometimes called lambdas). We can either write anonymous functions with the function keyword, or using the -&gt; syntax:\nf1 = function(x) x^2 + 2x + 1 end\nf2 = x -&gt; x^2 + 2x + 1\nThe body in the function version is a compound expression ending with end. The body in the -&gt; version is a single expression, similar to what we saw with the equational syntax.\n\n\n2.4.2 Defaults and keywords\nMost Julia functions take arguments. We can define arguments later in the list to have default values; for example:\nincrement(x, amount=1) = x + amount\nDefault argument values may depend on earlier arguments.\nJulia functions can take keyword arguments as well as positional arguments. Keyword arguments must have a default value. In the function definition, a semicolon separates the positional argument list from the keyword argument list. For example, consider the following routine to print a list in sorted order:\nfunction print_sorted(l; by=identity)\n    l = sort(l, by=by)\n    for v in l\n        println(v)\n    end\nend\nThe keyword argument by indicates a function that returns the sort key, and is passed through to the call to the sort function in the first line. We need the semicolon separator in the function definition to distinguish keyword arguments from positional arguments with defaults. However, there is no such requirement when calling the function, so the call to sort only uses commas to separate arguments.\n\n\n2.4.3 Closures\nWhen a Julia function is created inside an enclosing local scope, it may refer to variables that were created inside that scope. In this case, Julia creates a closure of the function definition together with variable bindings from the surrounding environment.\nFor example, suppose we have a naive Newton iteration for solving a 1D equation \\(f(x) = 0\\):\n\n\"\"\"    naive_newton(f, df, x; maxiter=10, ftol=1e-8)\n\nRun Newton iteration to find a zero of `f`\n(with derivative `df`) from the starting\npoint `x`.  Returns when the magnitude of `f`\nis less than `ftol`, otherwise errors out\nafter running for `maxiter` steps.\n\"\"\"\nfunction naive_newton(f, df, x; maxiter=10, ftol=1e-8)\n    fx = f(x)\n    for _ = 1:maxiter\n        if abs(fx) &lt; ftol\n            return x\n        end\n        x -= fx/df(x)\n        fx = f(x)\n    end\n    if abs(fx) &lt; ftol\n        return x\n    end\n    error(\"Did not converge in $maxiter steps\")\nend\n\nWe would like to evaluate the list of function values in order to plot the convergence. We could do this by modifying the naive_newton routine, or we could record each call to the function.\n\nfunction plot_naive_newton(f, df, x; maxiter=10, ftol=1e-8)\n\n    # Create a list of values and a closure that\n    # records function evaluations to the list\n    fs = []\n    function frecord(x)\n        fx = f(x)\n        push!(fs, fx)\n        fx\n    end\n\n    x = naive_newton(frecord, df, x,\n                     maxiter=maxiter, ftol=ftol)\n    plot(filter(fx-&gt;fx &gt; 0, abs.(fs)),\n         xlabel=\"k\",\n         yscale=:log10, label=\"\\$f(x_k)\\$\")\nend\n\nNow a call to plot_naive_newton(sin, cos, 0.5) (for example) will show a plot of the function value magnitudes at each step of the iteration.\n\nplot_naive_newton(sin, cos, 0.5)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Methods\nJulia allows a single function name to provide a common interface to different implementations (methods) based on the number and type of the arguments. The process of looking up the appropriate method from the invocation can take into account the types of more than one of the arguments (multiple dispatch). For example, consider the definitions\n\nmyrotate(z :: Complex, θ) = exp(1im*θ)*z\nmyrotate(xy :: Vector, θ) =\n    let c = cos(θ), s = sin(θ), x = xy[1], y = xy[2]\n        [c*x-s*y; s*x+c*y]\n    end\n\nBoth versions of myrotate correspond to a rotation of a plane, but we have different implementations depending on whether the first argument is a complex number or a vector.\n\nprintln(myrotate(1.0 + 0.0im, π/4))\nprintln(myrotate([1.0; 0.0], π/4))\n\n0.7071067811865476 + 0.7071067811865475im\n[0.7071067811865476, 0.7071067811865475]\n\n\nMethods are also very useful for defining structurally recursive functions to process data structures. For example, consider “flattening” a structure of nested tuples into a vector:\n\nfunction flatten_tuple(t)\n    result = []\n    process(x :: Tuple) = process.(x)\n    process(x) = push!(result, x)\n    process(t)\n    result\nend\n\nlet\n    t = ((1,2), (3,(4,5)), 6)\n    flatten_tuple(t)\nend\n\n6-element Vector{Any}:\n 1\n 2\n 3\n 4\n 5\n 6\n\n\nWe will have more to say about Julia’s type system in Section 2.5.\n\n\n2.4.5 Operator overloading\nAs mentioned previously, operators in Julia are just a special type of function. We can add methods to these functions in order to implement operators acting on new types. For example, consider a new type Polynomial representing polynomials expressed in the monomial basis (we will return to this in Section 2.5):\n\nstruct Polynomial\n    coeffs :: Vector{Float64}\nend\n\nWe would like to be able to add, subtract, and multiply polynomials. For example, to add implementation of the + and * operators from Base that works with a Polynomial, we would write\n\nfunction Base.:+(p :: Polynomial, q :: Polynomial)\n      n = max(length(p.coeffs), length(q.coeffs))\n      c = zeros(n)\n      c[1:length(p.coeffs)] = p.coeffs\n      c[1:length(q.coeffs)] += q.coeffs\n      Polynomial(c)\nend\n\nfunction Base.:*(p :: Polynomial, q :: Polynomial)\n      n = length(p.coeffs) + length(q.coeffs) - 1\n      c = zeros(n)\n      for (i, pi) in enumerate(p.coeffs)\n            c[i:length(q.coeffs)+i-1] += pi*q.coeffs\n      end\n      Polynomial(c)\nend\n\nIf we would like to treat constants as a type of polynomial, we need to add a conversion routine and some additional methods for the + and * functions.\n\nBase.convert(::Type{Polynomial}, c :: Number) = Polynomial([c])\nBase.:+(p :: Polynomial, q) = p + convert(Polynomial, q)\nBase.:+(p, q :: Polynomial) = convert(Polynomial, p) + q\nBase.:*(p :: Polynomial, q) = p * convert(Polynomial, q)\nBase.:*(p, q :: Polynomial) = convert(Polynomial, p) * q\n\nWe might also want to be able to evaluate a polynomial, since it represents a function:\n\nfunction (p :: Polynomial)(x)\n    px = 0*x\n    for c in reverse(p.coeffs)\n        px = px*x+c\n    end\n    px\nend\n\nWith these definitions in place, we can write code like\n\nlet\n    x = Polynomial([0; 1])  # x\n    p = x*(2*x+1)\n    p(2)\nend\n\n10.0\n\n\nThe ambitious reader might also try writing subtraction, polynomial division and remainders.\n\n\n2.4.6 Higher-order functions\nA higher-order function is a function that takes another function as an argument. Higher-order functions like map, filter, and reduce are the bread-and-butter of functional programming languages, and Julia supports these as well:\nmap(x-&gt;x+1, [1; 2; 3])           # Yields [2; 3; 4]\nfilter(x-&gt;x &gt; 0, [1; -1; 2; 3])  # Yields [1; 2; 3]\nreduce(+, [1; 2; 3])             # Yields 6\nHigher-order functions are quite common in numerical methods, as many of the operations that we want to perform are naturally expressed in terms of operations on functions (computing integrals and derivatives, optimizing, finding zeros). Indeed, we already saw a simple example of a higher-order function with the naive_newton example in Section 2.4.3.\n\n\n2.4.7 do syntax\nA common pattern in higher-order functions is to have a function that takes one function argument and potentially a few other operations. When the argument function is small, it is convenient to use the -&gt; operation to define a short anonymous function, as we did in Section 2.4.6. But sometimes when the argument function is not small, it gets clunky to write it in place as an anonymous function. For example, suppose we wanted a verbose version of the filter example above, one that prints out a description of what is going on at each step. Using the notation we have written so far, we could write something like\n\nfilter(function(x)\n           ispositive = x &gt; 0\n           action = ispositive ? \"Keep it\" : \"Toss it\"\n           println(\"Testing x = $x: $action\")\n           ispositive\n        end, [1; -1; 2; 3])\n\nTesting x = 1: Keep it\nTesting x = -1: Toss it\nTesting x = 2: Keep it\nTesting x = 3: Keep it\n\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nBecause this pattern occurs so often in Julia, however, there is a special syntax that makes it easier to write:\n\nfilter([1; -1; 2; 3]) do x\n    ispositive = x &gt; 0\n    action = ispositive ? \"Keep it\" : \"Toss it\"\n    println(\"Testing x = $x: $action\")\n    ispositive\nend\n\nTesting x = 1: Keep it\nTesting x = -1: Toss it\nTesting x = 2: Keep it\nTesting x = 3: Keep it\n\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nBehind the scenes, Julia rewrites constructions of the form\nf(fargs) do args\n    body\nend\nto\nf(function(args) body end, fargs)\nBeyond being used with functions like map and reduce, the do syntax is often used in Julia for tasks like I/O that involve a required setup and teardown phase. For example, writing to a file in Julia is often done using syntax like the following:\nopen(\"foo.txt\", \"w\") do f\n    println(f, \"Hello, world\")\nend\nThe version of the open method that takes a function argument in the terse syntax will call the function with the file handle, then close the file afterward.\n\n\n2.4.8 Composition and pipes\nIn addition to nested calls (e.g. f(g(x))), Julia supports two special syntax constructs for function composition:\n\nThe composition operator (∘) acts like composition in mathematics.\nThe pipeline operator (|&gt;) chains together inputs and outputs like the pipe symbol in a Unix shell command or pipes in R.\n\nFor example:\nsqr(x) = x.^2\nnorm2a(v) = sqrt(sum(sqr(v)))\nnorm2b = sqrt ∘ sum ∘ sqr\nnorm2c(v) = v |&gt; sqr |&gt; sum |&gt; sqrt\n\n\n2.4.9 Generators and comprehensions\nGenerators and comprehensions are a final type of specialized use of anonymous functions. A generator expression has the form expr for var in collection, with the option of additional comma-separated var in collection expressions. The expression evaluates to an iterable collection that is lazily evaluated. For example, consider the generator expression\ng = ((i,j) for i=1:2, j=1:2)\nWe can loop over the generator the same as we would over any other collection; with each iteration, the expression (i,j) is evaluated with a different binding of i and j from the iteration space, visited in column-major order (early indices change fastest). For example, consider the output of the following code fragment:\n\nlet\n    g = ((i,j) for i=1:2, j=1:2)\n    for v in g\n        println(v)\n    end\nend\n\n(1, 1)\n(2, 1)\n(1, 2)\n(2, 2)\n\n\nIterable expressions can be used in many places where Julia would accept a list or vector. For example:\n\nsum(i^2 for i = 1:10)\n\n385\n\n\nAn array comprehension is similar to a generator, but with surrounding square brackets. Rather than producing a lazily-evaluated collection, it produces a concrete array of results. For example, if \\(k(x,y)\\) is a bivariate function of interest, we can use a generator to concisely produce a matrix of function samples over a regular grid:\nxgrid = range(0.0, 1.0, length=20)\nkxx = [k(x,y) for x in xgrid, y in xgrid]",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#sec-julia-types",
    "href": "00-Background/01-Julia.html#sec-julia-types",
    "title": "2  Julia programming",
    "section": "2.5 Types",
    "text": "2.5 Types\nAt a certain level, computers only manipulate sequences of bits. But programmers usually work at a more abstract level, where those bit sequences are interpreted as different types of values: these 64 bits represent an integer, those 64 bits represent a floating point number, that other set of 64 bits represents the string \"program\" (with a null terminator). One of the classic distinctions drawn among programming languages is how types are treated. In statically-typed languages like C++ and Fortran, every variable and expression is associated with a particular type that can be determined at compile time1. In dynamically-typed languages like MATLAB, Python, and R, values have types but variables are not restricted in the types of values to which they may refer. But, of course, things are not so simple — C++ is mostly statically typed, but run-time polymorphism allows some elements of dynamic typing; while modern Python is mostly dynamically typed, but allows optional type annotations.\nJulia is a dynamically-typed language, in which variables are just names that can be bound to values of different types. At the same time, Julia has a rich type system with optional type declarations using the :: syntax as described in Section 2.2.1.2. These type declarations serve three purposes:\n\nCorrectness: Sometimes an operation only makes sense with certain types of inputs, and declaring that explicitly makes it possible for the system to provide an early error rather than an error at runtime.\nExpressivity: Declaring the types of arguments to functions allows the same function name to dispatch to multiple methods, as discussed in Section 2.4.4.\nPerformance: When the Julia system can infer types in a function call, the just-in-time compiler can create faster specialized versions of the code, much like what we would find in C or Fortran.\n\nWhile type declarations is often helpful, Julia also benefits from not requiring them. As an example, consider the gcd function from Section 2.3.2. We might argue that it is appropriate to restrict the arguments to be Integer types, i.e.\nfunction gcd(a :: Integer, b :: Integer)\n    while b != 0\n        a, b = b, a%b\n    end\n    a\nend\nHowever, the Euclidean algorithm also makes sense in more general Euclidean domains, such as polynomials. So we might extend the operations on the Polynomial type from Section 2.4.5 so that comparisons with zero and the remainder operator are both allowed. With this extension, the original gcd function (without the :: Integer annotations) would produce the correct result.\n\n2.5.1 Type hierarchy\nJulia types form a tree-shaped hierarchy, where each abstract type may have multiple subtypes. For example, Integer is a subtype of Number, and concrete integer types like Int64 are subtypes of Integer. The syntax &lt;: is used to denote a subtype relation; for example, we would write Integer &lt;: Number to test that Integer is indeed a subtype of Number. At the root of the type hierarchy is the type Any.\nOnly abstract types may serve as supertypes in Julia. Concrete types, whether primitive types (like Logical, UInt16, or Float64), or various compound types, are always leaves of the hierarchy.\nIt is possible (and sometimes useful) to declare new abstract types, e.g.\nabstract type MyAwesomeAbstraction end\n\n\n2.5.2 Numeric types\nThe primitive concrete numeric types in Julia are\n\nFloating point numbers Float16, Float32, and Float64. These are all subtypes of AbstractFloat. The 32-bit and 64-bit formats are supported in hardware, but operations with the 16-bit format are software only. We will have much more to say about floating point in Chapter 9.\nSigned integer types Int8, Int16, Int32, Int64, and Int128. These are all subtypes of Signed, which is a subtype of Integer. The type Int is an alias for Int64 or Int32, depending on whether Julia is being run on a 64-bit or a 32-bit system.2\nUnsigned integer types UInt8, UInt16, UInt32, UInt64, and UInt128. These are all subtypes of Unsigned, which is a subtype of Integer.\n\nTechnically, type Logical is also a subtype of type Integer, and it can be used in arithmetic expressions like an integer – for example, true + true evaluates to 2, while 1.0 * false evaluates to 0.0. However, we do not recommend using logical variables in this way.\nIn addition to the primitive types, Julia supports parameterized types for rational and complex numbers. For example, 3 // 4 produces a representation of \\(3/4\\) of type Rational{Int64}, while 1.0 + 1.0im produces a representation of \\(1+1i\\) of type Complex{Float64}.\nJulia also supports arbitrary-precision BigInt and BigFloat types, though we will rarely make use of these.\n\n\n2.5.3 Strings and symbols\nStrings in Julia inherit from the AbstractString type, and are logically sequences of Unicode characters3. Unicode characters (or “code points”) in turn inherit from the AbstractChar type, and are associated with 32-bit unsigned integers. The character A, for example, corresponds to code point U+0041; this is character 65 in decimal, but Unicode codepoints are typically written in hexadecimal. More generally, code points U+0000 through U+007F correspond to the classic ASCII character set, which includes Latin characters and punctuation used in standard English.\nThe default character type Char in Julia takes 32 bits in memory. The default String type is not an array of 32-bit Char entities; instead, Julia uses UTF-8 strings, which use a variable-length encoding. Unicode codepoint numbers corresponding to ASCII are encoded with one byte per character, and other characters may take more than one byte to represent. Hence, the length of a string (in characters) and the sizeof a string (in bytes) mean different things in Julia. These are also different complexity: computing the length (in codeunits) in a UTF-8 string requires that we scan the string from start to end. To avoid the time cost of a scan, indexing into a standard string is done by bytes. For example, in the string s = \"sπot\", we can safely refer to s[1] (s), s[2] (π), s[4] (o), and s[5] (t); but accessing s[3] will cause a runtime exception.\nSymbols inherit from type Symbol. They are encoded as “interned” UTF-8 strings, which means that the string is stored in a hash table, and the data passed around the Julia environment is the location in that hash table.\n\n\n2.5.4 Tuples\nThe type of a tuple is a Cartesian product of the tuple elements. In Julia, this is represented as Tuple, a parameterized type whose parameters are the component types. For example, the type of (1.0, 2) is Tuple{Float64,Int64}. Named tuples are similar; the type of (x=1.0, y=2) is @NamedTuple{x :: Float64, y :: Int64}.\n\n\n2.5.5 Structs and parameterization\nWe declare a structure type (sometimes called a record type) with the struct keyword; for example, we might define a type for rational numbers as\n# Version 0\nstruct MyRational &lt;: Number\n    p\n    q\nend\nThen MyRational(1,2) will give us an instance of MyRational with p set to 1 and q set to 2. However, this definition also allows us to legitimately write MyRational(\"bad\", \"idea\"), and we probably do not want to allow a ratio of two strings. A second attempt at a definition might be\n# Version 1\nstruct MyRational &lt;: Number\n    p :: Integer\n    q :: Integer\nend\nWith this definition, we are restricted to integer values for p and q. However, we may still be unhappy with this if we read about Julia performance, and see the advice to make fields be concrete types to produce efficient code (Integer is an abstract type). This brings us to our third attempt, which involves making MyRational a parametric type:\nstruct MyRational{T&lt;:Integer} &lt;: Number\n    p :: T\n    q :: T\nend\nWriting {T&lt;:Integer} after the structure name says that we want to have a type parameter T which is a subtype of Integer. With this definition, the constructor MyRational(1,2) will give us an instance of type MyRational{Int64} (assuming a 64-bit machine).\nWe may decide that we would like to make p and q have reduced form, and disallow a zero denominator. To do this, we would create a new inner constructor function:\nstruct MyRational{T&lt;:Integer} &lt;: Number\n    p :: T\n    q :: T\n    MyRational(p :: Integer, q :: Integer) =\n        if q == zero(q)\n            error(\"Divide by zero\")\n        else\n            p, q = promote(p, q)\n            r = gcd(p, q)\n            new{typeof(p)}(p÷r, q÷r)\n        end\nend\nThere are several things to unpack here:\n\nThe MyRational takes two abstract Integers. These could be different types of integers! In order to gracefully deal with this, we promote the arguments to a common type with the promote command (see Section 2.5.12)\nInside the MyRational constructor, we cannot just call another constructor called MyRational. Instead, we call new to create the object. This is only used inside of inner constructors.\nThe new call needs a type parameter, which we get from the type of p.\n\nIn addition to inner constructors, we can also define an outer constructor, so named because the definition is outside the struct statement. For example, we might want to define a constructor that takes just one argument (with the second being an implicit 1):\nMyRational(p :: Integer) = MyRational(p, one(p))\nAs described earlier, we might also decide that we wanted to overload the operators. Note that this does not require any special magic with the type parameters to MyRational:\nBase.:+(a :: MyRational, b :: MyRational) =\n    MyRational(a.p*b.q + a.q*b.p, a.q*b.q)\nBase.:-(a :: MyRational, b :: MyRational) =\n    MyRational(a.p*b.q - a.q*b.p, a.q*b.q)\nBase.:*(a :: MyRational, b :: MyRational) =\n    MyRational(a.p*b.p, a.q*b.q)\nBase.:/(a :: MyRational, b :: MyRational) =\n    MyRational(a.p*b.q, a.q*b.p)\nWhat if we want to get the type associated with p and q? We can, of course, use typeof(p) to do this, but we can also write a method to extract the type. Unlike the overloaded operators, this method does explicitly refer to the type parameter, and so we need some additional syntax:\ninttype(:: MyRational{T}) where {T} = T\nUnpacking this construction, we have\n\nAn anonymous argument of type MyRational{T}. We do not need to give the argument a name, because we do not care about the value of the variable; we only care about its type.\nThe where {T} clause to specify that this method definition is parameterized by the type T.\n\n\n\n2.5.6 Structs and inheritance\nInheritance from abstract types can be a used to provide shared functionality in two ways:\n\nWe can keep some information in abstract type parameters.\nWe can write generic methods for an abstract type that use specific implementations for concrete types (Julia’s version of “duck typing” from languages like Python).\n\nAs a somewhat silly example, let’s consider an abstraction around a list of points in \\(d\\)-dimensional space, where each point is considered to be a length-\\(d\\) tuple.\n\nabstract type Points{d,T} end\ndim(p :: Points{d,T}) where {d,T} = d\nBase.eltype(p :: Points{d,T}) where {d,T} = T\n\nThe dimension d and the element type T are type parameters for the abstract type, and nothing special needs to be done for types inheriting from Points{d,T} to use the dim or eltype functions.\nThe abstraction of “list of points” can be implemented with multiple data structures and associated versions of the Base functions length and getindex. For example, we could use parallel arrays for the coordinates:\n\nstruct ParallelPoints2d{T} &lt;: Points{2,T}\n    x :: Vector{T}\n    y :: Vector{T}\nend\nBase.length(ps :: ParallelPoints2d) = length(ps.x)\nBase.getindex(ps :: ParallelPoints2d, i) = (ps.x[i], ps.y[i])\n\nOr we could keep the points in a matrix, where each column represents a different point:\n\nstruct MatrixPoints{d,T} &lt;: Points{d,T}\n    xy :: Matrix{T}\nend\nBase.length(ps :: MatrixPoints) = size(ps.xy,2)\nBase.getindex(ps :: MatrixPoints, i) = Tuple(view(ps.xy,:,i))\n\nFor the case of MatrixPoints, it is redundant to explicitly specify the dimension of the space when we provide a matrix of points to initialize the object. Julia can infer from the type of the matrix what T should be; but to get d, it needs some help.\n\nMatrixPoints(xy :: AbstractMatrix) =\n    MatrixPoints{size(xy,1),eltype(xy)}(xy)\n\nJust using the length and index (and the dim function from above), we can write generic implementations of operations like converting a list of points to a matrix, whatever the underlying data structure.\n\nfunction to_matrix(ps :: Points)\n    result = zeros(eltype(ps), dim(ps), length(ps))\n    for j = 1:length(ps)\n        result[:,j] .= ps[j]\n    end\n    result\nend\n\nto_matrix(ParallelPoints2d([1, 2, 3], [4, 5, 6]))\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nOr we might like to be able to iterate over a list of points; for this, we again only need to know that length and getindex are implemented to be able to provide a generic iterator.\n\nBase.iterate(ps :: Points, i=1) =\n    i &gt; length(ps) ? nothing : (ps[i], i+1)\n\nThere are some times when we might want slightly more specialized behavior based on the type. For example, for output, we might want to list 1D points as just individual numbers rather than as tuples (i.e. output 1 rather than (1,)). To do this, we can write a generic version of the show routine (used for output), but also one that specializes to the one-dimensional case. Julia will choose the more specific version for a given type.\n\n# Generic version of show\nfunction Base.show(io :: IO, ps :: Points)\n    print(io, \"[ \")\n    for p in ps\n        print(io, p)\n        print(io, \" \")\n    end\n    print(io, \"]\")\nend\n\n# Version of show for 1D points\nfunction Base.show(io :: IO, ps :: Points{1,T}) where {T}\n    print(io, \"[ \")\n    for p in ps\n        print(io, p[1])\n        print(io, \" \")\n    end\n    print(io, \"]\")\nend\n\n# Illustrate printing in 1D or 2D with different structs\nlet\n    println(ParallelPoints2d([1, 2, 3], [4, 5, 6]))\n    println(MatrixPoints([1 2 3; 4 5 6]))\n    println(MatrixPoints([1 2 3 4 5 6]))\nend\n\n[ (1, 4) (2, 5) (3, 6) ]\n[ (1, 4) (2, 5) (3, 6) ]\n[ 1 2 3 4 5 6 ]\n\n\nBetween type parameters and dispatch to different methods for concrete types, the Julia language provides most of the amenities that would be implemented via inheritance in other languages. Perhaps the biggest exception to this is that Julia provides no direct support for identifying data fields that should be implemented for all structs inheriting from some abstract type – though one can write macros that provide such support.\n\n\n2.5.7 Mutable structs\nBy default, the entries of a struct are immutable. Using the example from the previous section, once we have created a MyRational with a given p and q, we cannot change those values. Note that if any field of a structure is a writeable container (like an array), we can write into that container; the container is immutable, but its contents are not. Still, sometimes we would genuinely like to be able to update a structure, and for this we have mutable structures, which are declared with the mutable keyword. For example, consider the following counter struct:\nmutable struct FooCounter\n    p :: Int\n    FooCounter() = new(0)\nend\nNow we can write an increment operator that updates this structure (using an exclamation mark in the name, as is the convention for functions that change their inputs):\nincrement!(f :: FooCounter) = (f.p += 1)\n\nfoo = FooCounter()        # foo.p initialized to 0\nprintln(increment!(foo))  # Prints 1\nprintln(increment!(foo))  # Prints 2\n\n\n2.5.8 Array types\nAn Array{T,d} is a \\(d\\)-index array of type T. The type aliases Vector{T} and Matrix{T} correspond to arrays with d set to 1 and 2, respectively. It is possible to make T an abstract type, but it is generally much more efficient to make T a concrete type.\nIn addition to arrays, there are several constructs that are subtypes of the AbstractArray interface. These includes:\n\nRange: An abstraction for a regularly-spaced range, which can be constructed by the colon operator (e.g. 1:3) or with the range command.\nAdjoint: (Conjugate) transpose of a matrix.\nBitArray: A compact storage format for an array of logical values.\nSparseMatrixCSC: A sparse matrix format.\nSubArray: A submatrix of an existing matrix, which can be constructied using the view macros.\nStaticArray: An fast type for small arrays whose dimensions are part of the type.\n\nThese constructs can all be indexed, sliced, iterated over, and otherwise treated much like ordinary arrays (though not all are writeable).\n\n\n2.5.9 Other collections\nIn addition to tuples and array-like collections, Julia supports dictionary and set types. A dictionary contains key-value pairs, written as key =&gt; value in Julia; the =&gt; operator constructs a Pair object. The structure supports addition and deletion of pairs, testing for keys, iteration, and lookup (either with get, which supports default values, or with dict[key], which does not). There are a few variants, the most frequently used of which is the ordinary dictionary (Dict).\nJulia also supports a Set type, which supports addition and removal, testing for inclusion or subset relations, and operations like union, intersection, and set differences.\n\n\n2.5.10 Unions, Nothing, and Missing\nJulia allows Union types whose values may be one of a selection of types. Two specific examples have to do with data that may be “null” or missing4. In Julia, the type Nothing (with instance nothing) is used the way that programmers use “null” in some other languages, while the type Missing (with instance missing) is used to indicate missing data. These types are most often used in the context of Union{T, Nothing} or Union{T, Missing} types. For example, values of type Union{T, Missing} can either be something of type T or the value missing.\n\n\n2.5.11 Types of types and value types\nTypes are themselves things that can be represented and manipulated in Julia. Each type has type DataType, which is a subtype of the parametric abstract type Type{T}. In general, a type T (and no other type) is an instance of Type{T}. This can be useful for certain special types of method dispatch situations. Similarly, value types (Val{T}) are parameterized types that take a value as a parameter, and can be used for specialized method dispatch or for helping the compiler with performant code.\n\n\n2.5.12 Type conversions and promotions\nThe convert function in Julia is used to convert values of one type to another type (when possible). The system calls convert automatically when assigning a value of one type to a variable or field of another type. The first argument is the target type, and the second is the value to be converted. Defining new methods for convert is a good use of the Type type; for example, we might define a conversion from an Integer to the MyRational type defined earlier as:\nBase.convert(::Type{MyRational}, p :: Integer) = MyRational(p)\nA more elaborate example might involve converting MyRational values with different integer parameter types:\nBase.convert(::Type{MyRational{T}}, r :: MyRational) where {T} =\n    MyRational(T(r.p), T(r.q))\nThe promote function in Julia takes a list of values and converts them both to a compatible greatest type. We typically would not write new methods for promote; instead, we write new methods for promote_rule. Again using MyRational as an example, we might have\nBase.promote_rule(::Type{MyRational{T}}, ::Type{S}) where {T&lt;:Integer,S&lt;:Integer} =\n    MyRational{promote_type(T,S)}",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#sec-julia-exceptions",
    "href": "00-Background/01-Julia.html#sec-julia-exceptions",
    "title": "2  Julia programming",
    "section": "2.6 Exception handling",
    "text": "2.6 Exception handling\nJulia has throw/catch system for handling exceptional conditions, similar to many other modern languages (Python, MATLAB, R, Java, C++). Information about exceptions is encoded in via a struct that is a subtype of Exception, which we throw when the error is detected:\n\"\"\"    imlog(x)\nCompute the imaginary part of the principal branch\nof the log of negative number x.\n\"\"\"\nimlog(x) =\n    (x &gt;= zero(x)) ?\n    throw(DomainError(\"Argument must be negative\")) :\n    log(-x)\nThe error function throws an instance of an ErrorException.\nExceptions that are thrown at some point in a call chain can be caught by callers using a try/catch statement.\n\"\"\"    generous_log(x :: Number)\nComputes the log of a number; if a domain error is\nthrown for the log of a negative real number, tries\nwith a complex version of the number.\n\"\"\"\ngenerous_log(x :: Number) =\n    try\n        log(x)\n    catch e\n        if e isa DomainError\n            log(Complex(x))\n        else\n            throw(e)\n        end\n    end\nThese statements may also have an optional else clause that is executed when the try statement succeeded without exceptions; and an optional finally clause that is executed when exiting the try/catch, independent of how the exit occurred.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#documentation",
    "href": "00-Background/01-Julia.html#documentation",
    "title": "2  Julia programming",
    "section": "2.7 Documentation",
    "text": "2.7 Documentation\nFunctions, types, and modules can all have documentation strings just before them, which are referenced by the Julia help system. Documentation strings are formatted using Markdown. A fenced code block beginning with jldoctest can be used to produce a usage example that is suitable for automatic testing.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#sec-julia-linalg",
    "href": "00-Background/01-Julia.html#sec-julia-linalg",
    "title": "2  Julia programming",
    "section": "2.8 Vectors, matrices, and linear algebra",
    "text": "2.8 Vectors, matrices, and linear algebra\nWe have already discussed some of Julia’s array support. However, one of the attractive features of Julia is the support not only for arrays, but for numerical linear algebra computations. Many of these facilities are in the LinearAlgebra package, which we will almost always use throughout this book.\n\n2.8.1 Building matrices and vectors\nBy default, we think of a one-dimensional array as a column vector, and a two-dimensional array as a matrix. We can do standard linear algebra operations like scaling (2*A), summing like types of objects (v1+v2), and matrix multiplication (A*v). The expression\nw = v'\nrepresents the adjoint of the vector v with respect to the standard inner product (i.e. the conjugate transpose). The tick operator also gives the (conjugate) transpose of a matrix. We note that the tick operator in Julia does not actually copy any storage; it just gives us a re-interpretation of the argument. This shows up, for example, if we write\n\nlet\n    v = [1, 2]  # v is a 2-element Vector{Int64} containing [1, 2]\n    w = v'      # w is a 1-2 adjoint(::Vector{Int64}) with eltype Int64\n    v[2] = 3    # Now v contains [1, 3] and w is the adjoint [1, 3]'\nend\n\n3\n\n\nJulia gives us several standard matrix and vector construction functions.\nZ = zeros(n)   # Length n vector of zeros\nZ = zeros(n,n) # n-by-n matrix of zeros\nb = rand(n)    # Length n random vector of U[0,1] entries (from Random)\ne = ones(n)    # Length n vector of ones\nD = diagm(e)   # Construct a diagonal matrix\ne2 = diag(D)   # Extract a matrix diagonal\nThe identity matrix in Julia is simply I. This is an abstract matrix with a size that can usually be inferred from context. In the rare cases when you need a concrete instantiation of an identity matrix, you can use Matrix(I, n, n).\n\n\n2.8.2 Concatenating matrices and vectors\nIn addition to functions for constructing specific types of matrices and vectors, Julia lets us put together matrices and vectors by horizontal and vertical concatenation. This works with matrices just as well as with vectors! Spaces are used for horizontal concatenation and semicolons for vertical concatenation.\ny = [1; 2]        # Length-2 vector\ny = [1 2]         # 1-by-2 matrix\nM = [1 2; 3 4]    # 2-by-2 matrix\nM = [I A]         # Horizontal matrix concatenation\nM = [I; A]        # Vertical matrix concatenation\nJulia uses commas to separate elements of a list-like data type or an array. So [1, 2] and [1; 2] give us the same thing (a length 2 vector), but [I, A] gives us a list consisting of a uniform scaling object and a matrix — not quite the same as horizontal matrix concatenation.\n\n\n2.8.3 Transpose and rearrangemenent\nJulia lets us rearrange the data inside a matrix or vector in a variety of ways. In addition to the usual transposition operation, we can also do “reshape” operations that let us interpret the same data layout in computer memory in different ways.\n# Reshape A to a vector, then back to a matrix\n# Note: Julia is column-major\navec = reshape(A, prod(size(A)));\nA = reshape(avec, n, n)\n\nidx = randperm(n)   # Random permutation of indices (need to use Random)\nAc = A[:,idx]       # Permute columns of A\nAr = A[idx,:]       # Permute rows of A\nAp = A[idx,idx]     # Permute rows and columns\n\n\n2.8.4 Submatrices, diagonals, and triangles\nJulia lets us extract specific parts of a matrix, like the diagonal entries or the upper or lower triangle. Some operations make separate copies of the data referenced:\nA = randn(6,6)    # 6-by-6 random matrix\nA[1:3,1:3]        # Leading 3-by-3 submatrix\nA[1:2:end,:]      # Rows 1, 3, 5\nA[:,3:end]        # Columns 3-6\n\nAd = diag(A)      # Diagonal of A (as vector)\nA1 = diag(A,1)    # First superdiagonal\nAu = triu(A)      # Upper triangle\nAl = tril(A)      # Lower triangle\nOther operations give a view of the matrix without making a copy of the contents, which can be much faster:\nA = randn(6,6)          # 6-by-6 random matrix\nview(A,1:3,1:3)         # View of leading 3-by-3 submatrix\nview(A,:,3:end)         # View of columns 3-6\nAu = UpperTriangular(A) # View of upper triangle\nAl = LowerTriangular(A) # View of lower triangle\n\n\n2.8.5 Matrix and vector operations\nJulia provides a variety of elementwise operations as well as linear algebraic operations. To distinguish elementwise multiplication or division from matrix multiplication and linear solves or least squares, we put a dot in front of the elementwise operations.\ny = d.*x   # Elementwise multiplication of vectors/matrices\ny = x./d   # Elementwise division\nz = x + y  # Add vectors/matrices\nz = x .+ 1 # Add scalar to every element of a vector/matrix\n\ny = A*x    # Matrix times vector\ny = x'*A   # Vector times matrix\nC = A*B    # Matrix times matrix\n\n# Don't use inv!\nx = A\\b    # Solve Ax = b *or* least squares\ny = b/A    # Solve yA = b or least squares\n\n\n2.8.6 Things best avoided\nThere are few good reasons to compute explicit matrix inverses or determinants in numerical computations. Julia does provide these operations. But if you find yourself typing inv or det in Julia, think long and hard. Is there an alternate formulation? Could you use the forward slash or backslash operations for solving a linear system?",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#useful-packages",
    "href": "00-Background/01-Julia.html#useful-packages",
    "title": "2  Julia programming",
    "section": "2.9 Useful packages",
    "text": "2.9 Useful packages\nBeyond the LinearAlgebra package mentioned in Section 2.8, there are several other packages that we will use or recommend in this book:\n\n2.9.1 Statistics\nThe Statistics package provides a variety of standard statistics computations (mean, median, etc). There is also a StatsBase package that provides some additional statistics. There has been discussion of refactoring these.\nThe StatsFuns package provides a variety of special functions that occur in statistical computations (e.g. the log gamma function or various common pdf, cdf, and inverse cdf functions). The Distributions package is recommended as providing a more convenient interface, but sometimes we will want the low-level interface.\n\n\n2.9.2 Sparse matrices\nThe SparseArrays package is generally useful for setting up sparse matrix and vectors. In addition, Julia provides packages for a variety of sparse direct and iterative solvers, including the SuiteSparse package (direct solvers) and the IterativeSolvers package (iterative solvers).\n\n\n2.9.3 DataFrames\nThe DataFrames package provides functionality for manipulating tabular data similar to the Python Pandas package.\n\n\n2.9.4 Automatic differentiation\nThough we will only make limited use of these within the book, there are several Julia packages that support automatic differentiation, including ForwardDiff, ReverseDiff, Zygote, and Enzyme.\n\n\n2.9.5 Plots and tables\nThe Plots package provides a standard interface for producing plots with a variety of plotting backends. There are other plotting packages in Julia (the second-most popular of which is probably Makie), but the Plots package makes a good starting point.\nFor text output of tabular data, we recommend the PrettyTable package. This can also write to a variety of output formats, including HTML, Markdown, and LaTeX.\n\n\n2.9.6 I/O\nThe FileIO package provides a common interface for reading (and writing) a wide variety of data file types, including text, tabular, image, sound, and video formats.\n\n\n2.9.7 Development tools\nThe Test package in Julia provides a variety of useful tools for writing unit tests. When tests fail and it is necessary to step through code, the Debugger package is useful. For performance evaluation, we recommend the BenchmarkTools and Profile packages, as well as DispatchDoctor for checking type stability.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#macros-and-metaprogramming",
    "href": "00-Background/01-Julia.html#macros-and-metaprogramming",
    "title": "2  Julia programming",
    "section": "2.10 Macros and metaprogramming",
    "text": "2.10 Macros and metaprogramming\nJulia provides a LISP-like metaprogramming facility that allows us to manipulate Julia code as data. This is a very powerful tool, allowing us to (for example) write macros, i.e. code that generates new code on our behalf. Metaprogramming requires care to get right, and the usual advice is to only use it when other tools do not suffice. However, there are some times when it really is the right tool, and so we will touch on it here at least lightly.\n\n2.10.1 Code as data\nThe heart of metaprogramming is the understanding that code is a type of data, and can be manipulated as such. We distinguish code that we want to treat as data by quoting it, which involves bracketing the code by :( and ) or by quote and end. For example, the expression\n\nquoted_sum = :(1 + 1)\n\n:(1 + 1)\n\n\nassigns quoted_sum to the unevaluated code for 1 + 1. To evaluate the code, we can call the eval function\n\neval(quoted_sum)\n\n2\n\n\nQuoted expressions are printed by the Julia interpreter in natural Julia syntax. Internally, though, these expressions are not represented by strings, but by expression trees. We can see the structure of the expresion tree with the dump command, e.g.\n\ndump(quoted_sum)\n\nExpr\n  head: Symbol call\n  args: Array{Any}((3,))\n    1: Symbol +\n    2: Int64 1\n    3: Int64 1\n\n\nThe expression tree data structure involves internal nodes of type expr that have a head field indicating the type of operation represented, and an args array indicating the arguments of the operation. The leaves in the expression tree are literals, including symbols.\nJust as we can use string interpolation to build strings that include computations, we can also use interpolation with quoted code; for example\n\nlet\n    x = 1\n    :($x + 1)\nend\n\n:(1 + 1)\n\n\nWe can produce the same expression tree without quoting by explicitly using the constructor for Expr objects, e.g.\n\nlet\n    x = 1\n    Expr(:call, :+, x, 1)\nend\n\n:(1 + 1)\n\n\n\n\n2.10.2 Macros\nFor the reader who really wants to learn to write macros, we recommend the documentation together with the book On Lisp (Graham 1993). We will write small macros a few times in order to simplify our life; and in any case, we also use macros written by others often enough that we are obliged to say a few words from a user’s perspective.\nA macro maps a tuple of input expressions to an output expression, which is then compiled. The actual work of the macro code occurs at compile time; at run time, the system evaluates whatever output expression was generated by the macro. Macros are frequently used for various types of programming utilities. Debugging, timing, assertions, and formatted output in Julia are all conventionally done with macros.\nThe argument expressions can be parenthesized or not; for example, the following two lines are functionally identical invocations of the @assert macro:\n@assert(f(), \"Call to f failed\")\n@assert f() \"Call to f failed\"\n\n2.10.2.1 A toy definition\nAs a toy example, of a macro definition, consider a macro that prints the input expression\n\nmacro printarg(e)\n    s = \"$e = \"\n    quote\n        let v = $e\n            println($s, v)\n            v\n        end\n    end\nend\n\n@printarg (macro with 1 method)\n\n\nWhen calling a macro, we preface the name with an @ sign; for example\n\n@printarg 1+1\n\n1 + 1 = 2\n\n\n2\n\n\nThis is essentially the functionality of the built-in @show macro, except that our printarg macro fails when the argument involves an expression with a variable. For example, @show works fine in this context, while @printarg would fail:\n\nlet\n    x = 1\n    @show x + x\nend\n\nx + x = 2\n\n\n2\n\n\nThe problem has to do with the fact that Julia macros and rename variables internally when there is a possibility of accidental conflict with names in an outer environment in a local scope. To deliberately refer to a variable in the outer environment, that variable must be escaped. We can see the difference by expanding both the @printarg and the @show macros on the same input using the @macroexpand macro:\n\n@macroexpand @printarg x + x\n\n\nquote\n    #= In[47]:4 =#\n    let var\"#153#v\" = Main.x + Main.x\n        #= In[47]:5 =#\n        Main.println(\"x + x = \", var\"#153#v\")\n        #= In[47]:6 =#\n        var\"#153#v\"\n    end\nend\n\n\n\nWe can see that all the variable names in this expression are explicitly scoped to the Main module. In contrast, for @show, we have\n\n@macroexpand @show x + x\n\n\nquote\n    Base.println(\"x + x = \", Base.repr(begin\n                #= show.jl:1232 =#\n                local var\"#155#value\" = x + x\n            end))\n    var\"#155#value\"\nend\n\n\n\nHere we see that a local variable (with a name that will not conflict with any other name in the system) is assigned to the expression x+x.\n\n\n2.10.2.2 A modest example\nAs an example of a more significant small utility macro, consider a closure defined inside of an outer function. Such closures sometimes suffer5 from a performance issue because the compiler analysis does not determine that variables from the outer context (captured variables) are “type stable” (see Chapter 3). Putting the function into a let block can help address the issue, e.g. replacing\nf = x -&gt; C*x\nwith\nf = let C=C x -&gt; C*x end\nThis does make a semantic change to the program; for example, the code\nC = 10\nf = x -&gt; C*X\nC = 20\nf(5)\nwill produce 100, while introducing an enclosing let block would produce 50. Nonetheless, in many cases captured variables remain unchanging for the useful lifetime of a closure, and this is a valid transformation.\nLet us assume we are interested in automatically producing a surrounding let block. In this case, we first need a list of variables referenced in an expression. We do this by recursing through the call tree and adding any symbols we encounter (except for function names) to a set. Note that we can make this code very concise by defining different methods associated with the type of the first argument (dispatching based on an Expr, a Symbol, or another type)\n\nsymbols!(e, s = Set{Symbol}()) = nothing\n\nsymbols!(e :: Symbol, s = Set{Symbol}()) = push!(s, e)\n\nfunction symbols!(e :: Expr, s = Set{Symbol}())\n    args = e.head == :call ? e.args[2:end] : e.args\n    for arg in args\n        symbols!(arg, s)\n    end\n    s\nend\n\nNow we are in a position to define the macro:\n\nmacro letclosure(e)\n\n    # Check that this is a function definition with -&gt;\n    @assert e.head == :(-&gt;)\n\n    # Get the symbols that are not in the argument list\n    s = Set{Symbol}()\n    sarg = Set{Symbol}()\n    symbols!(e.args[2], s)\n    symbols!(e.args[1], sarg)\n    setdiff!(s, sarg)\n\n    # Bind local variables to the outer version\n    bindings = [quote $v = $(esc(v)) end for v in s]\n\n    # Put the expression into the let block\n    quote\n        let\n            $(bindings...)\n            $e\n        end\n    end\nend\n\nThere are a few points to note about our definition:\n\nFor local variables, the macro system produces “local” versions of names that do not coincide with any other names in the module. When we want such a coincidence, we need to explicitly escape a symbol with the esc function.\nAs in other contexts, the “splatting” operator ($(bindings...)) expands the contents of a list in place.\n\nWhen we evaluate the macro, the code is transformed and the result is inserted into our program; for example:\n\nlet\n    C = 10\n    f = @letclosure x -&gt; C*x\n    C = 20\n    f(5)\nend\n\n50\n\n\nIf we want to see how a macro expands out to regular code, we can use the @macroexpand macro. Continuing with our example above, we have:\n\n@macroexpand @letclosure x -&gt; C*x\n\n\nquote\n    #= In[53]:18 =#\n    let\n        #= In[53]:19 =#\n        begin\n            #= In[53]:14 =#\n            var\"#159#C\" = C\n        end\n        #= In[53]:20 =#\n        (var\"#161#x\",)-&gt;begin\n                #= In[55]:1 =#\n                var\"#159#C\" * var\"#161#x\"\n            end\n    end\nend\n\n\n\nNote in particular that in the variable bindings, the regular C appears on the right hand side, but a local version of C is assigned to. All the other variables in the generated code likewise have purely local names that are guaranteed not to coincide with other names in the module.\n\n\n\n2.10.3 Macros for structs\nOne simple use for macros is to define “boilerplate” code, such as data fields that must be defined for every struct inheriting from a particular abstract data type. For example, we might assume that every instance of a User has a name and ID, but some types of users might have additional information.\n\nabstract type User end\n\nmacro make_user_type(T, fields)\n    esc(quote\n            struct $T &lt;: User\n                lastname  :: String\n                firstname :: String\n                id        :: String\n                $(fields.args...)\n            end\n        end)\nend\n\nIn this scenario, a LocalUser might requires only a name and ID (the first two fields for any User-derived type).\n\n@make_user_type LocalUser begin end\nLocalUser(\"Brown\", \"Bob\", \"bbrown\")\n\nLocalUser(\"Brown\", \"Bob\", \"bbrown\")\n\n\nFor an external user, though, we also want to keep track of an employer.\n\n@make_user_type ExternalUser begin\n    employer :: String\nend\nExternalUser(\"Smith\", \"Sam\", \"ssmith\", \"NSA\")\n\nExternalUser(\"Smith\", \"Sam\", \"ssmith\", \"NSA\")\n\n\nBecause there is a common set of fields, we can define utility functions that work on these fields for any class of user.\n\nfullname(u :: User) = \"$(u.firstname) $(u.lastname)\"\n\nfullname(LocalUser(\"Brown\", \"Bob\", \"bbrown\")),\nfullname(ExternalUser(\"Smith\", \"Sam\", \"ssmith\", \"NSA\"))\n\n(\"Bob Brown\", \"Sam Smith\")\n\n\nOf course, it is possible to implement this type of functionality without macros as well — it just might involve slightly more typing.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#a-matching-example",
    "href": "00-Background/01-Julia.html#a-matching-example",
    "title": "2  Julia programming",
    "section": "2.11 A matching example",
    "text": "2.11 A matching example\nBeyond writing language utilities, Julia macros are useful for writing embedded domain-specific languages (DSLs) for accomplishing particular tasks. In this setting, we are really writing a language interpreter embedded in Julia, using the Julia parse tree as an input and producing Julia code as output.\nAs an example of both Julia macro design and Julia programming more generally, we will design a small language extension for writing functions that transform code based on structural pattern matching; for example, a rule written as:\n0-x =&gt; x -&gt; -x\ncorresponds to a function something like\ne -&gt;\n    # Check that e is a binary minus and the first argument is zero.\n    if (e isa Expr && e.head == :call &&\n        e.args[1] == :- && length(e.args) == 3 &&\n        e.args[2] == 0)\n\n        # Bind the name \"x\" to the second argument\n        let x = e.args[3]\n            result = :(-$x)   # Create a \"-x\" expression\n            true, result      # Yes it matched, return result\n        end\n    else\n        false, nothing        # No, it didn't match\n    end\nThat is, we want a function that takes an input\n\nIs this an expression with a binary - operation at the top?\nIs the second term in the sum a zero?\nIf both are true, bind x to the second term in the sum, produce a new expression :(-$x), and assign to result. Return the pair (true, result).\nOtherwise, return (false, nothing).\n\nThe one-line description is already more concise for this simple example; if we wanted to match against a more complicated pattern, the Julia code corresponding to a rule in the syntax\npattern =&gt; (argument_symbols) -&gt; result_code\nbecomes that much more complex. Our key task is therefore take rules written with the first syntax and to convert them automatically to standard functions.\nThis is a more ambitious example, and can be safely skipped over. However, it is both a useful example of a variety of features in Julia and introduces concrete tools we will use in later chapters (e.g. when discussing automatic differentiation).\n\n2.11.1 Preprocessing\nThe parse tree for Julia code includes not only expressions, but also line numbers, e.g.\n\n:(x-&gt;x+1)\n\n\n:(x-&gt;begin\n          #= In[60]:1 =#\n          x + 1\n      end)\n\n\n\nIn the code shown, the comment line immediately after the begin statement corresponds to a LineNumberNode. Such LineNumberNode nodes are useful for debugging, but are a nuisance for our attempts at pattern matching. Therefore, before doing anything else, we will write a utility to get rid of such nodes. Because LineNumberNode is a distinct type, we can use Julia’s multiple dispatch as an easy way to accomplish this.\nfilter_line_numbers(e :: Expr) =\n    let args = filter(a -&gt; !(a isa LineNumberNode), e.args)\n        Expr(e.head, filter_line_numbers.(args)...)\n    end\nfilter_line_numbers(e) = e\n\n\n2.11.2 Matching\nAt the heart of our matching algorithm will be a function match_gen that generates code to check whether an expression matches a pattern. The inputs to the match_gen function are\n\nA dictionary of bindings, mapping symbols to values.\nA pattern\nA symbol naming the current expression being matched.\n\n\n2.11.2.1 Literals\nIn the simplest case, for non-symbol leaf nodes, we declare a match when the pattern agrees with the expression.\nmatch_gen!(bindings, e, pattern) = :($e == $pattern)\nFor example, the following code matches the expression named expr to the pattern 0:\n\nmatch_gen!(Dict(), :expr, 0)\n\n:(expr == 0)\n\n\n\n\n2.11.2.2 Symbols\nThings are more complicated when we match a symbol. If the symbol is not in the bindings dictionary, we just check that the expression equals the (quoted) symbol. Otherwise, there are two cases:\n\nFirst use: we create a new binding for it.\nRepeat use: Check if expr matches the previous binding.\n\nmatch_gen!(bindings, e, s :: Symbol) =\n    if !(s in keys(bindings))\n        qs = QuoteNode(s)\n        :($e == $qs)\n    elseif bindings[s] == nothing\n        binding = gensym()\n        bindings[s] = binding\n        quote $binding = $e; true end\n    else\n        binding = bindings[s]\n        :($e == $binding)\n    end\nWe illustrate all three cases below.\n\nlet\n    bindings = Dict{Symbol,Any}(:x =&gt; nothing)\n    println( match_gen!(bindings, :expr, :x) )\n    println( match_gen!(bindings, :expr, :x) )\n    println( match_gen!(bindings, :expr, :y) )\nend\n\n\nbegin\n\n    #= /Users/dbindel/work/class/nmds/nmds/src/matcher.jl:121 =#\n\n    var\"##343\" = expr\n\n    #= /Users/dbindel/work/class/nmds/nmds/src/matcher.jl:121 =#\n\n    true\n\nend\n\nexpr == var\"##343\"\n\nexpr == :y\n\n\n\n\n\n\n2.11.2.3 Expressions\nThe most complex case is matching expressions. The basics are not complicated: an item e matches an Expr pattern if e is an expression with the same head as the pattern and if the arguments match.\nmatch_gen!(bindings, e, pattern :: Expr) =\n    let head = QuoteNode(pattern.head),\n        argmatch = match_gen_args!(bindings, e, pattern.args)\n        :( $e isa Expr && $e.head == $head && $argmatch )\n    end\nThe building block for checking the arguments will be to check that a list of expressions matches a list of patterns.\nmatch_gen_lists!(bindings, exprs, patterns) =\n    foldr((x,y) -&gt; :($x && $y),\n          [match_gen!(bindings, e, p)\n           for (e,p) in zip(exprs, patterns)])\nThe more complicated case is ensuring that the arguments match. This is in part because we want to accomodate the possibility that the last argument in the list is “splatted”; that is, a pattern like f(args...) should match f(1, 2, 3) with args bound to the tuple [1, 2, 3]. In order to do this, we would first like to make sure that we can sensibly identify a “splatted argument.”\nis_splat_arg(bindings, e) =\n    e isa Expr &&\n    e.head == :(...) &&\n    e.args[1] isa Symbol &&\n    e.args[1] in keys(bindings)\nNote that we only consider something a “splatted argument” if the argument to the splat operator is a symbol in the bindings table.\nTo implement the check, we create a let statement to bind a local name to each argument. If the last pattern is splatted, we make sure the last term in the tuple on the left-hand side of the statement is splatted (and then remove the pattern splat). Finally, we generate checks to make sure each of the locally-named arguments matches with the associated term in the pattern list.\nmatch_gen_args!(bindings, e, patterns) =\n    if isempty(patterns)\n        :(length($e.args) == 0)\n    else\n        nargs = length(patterns)\n        lencheck = :(length($e.args) == $nargs)\n        args = Vector{Any}([gensym() for j = 1:length(patterns)])\n        argstuple = Expr(:tuple, args...)\n\n        # De-splat pattern / splat arg assignment and\n        #   adjust the length check\n        if is_splat_arg(bindings, patterns[end])\n            patterns = copy(patterns)\n            patterns[end] = patterns[end].args[1]\n            argstuple.args[end] = Expr(:(...), argstuple.args[end])\n            lencheck = :(length($e.args) &gt;= $(nargs-1))\n        end\n\n        argchecks = match_gen_lists!(bindings, args, patterns)\n        :($lencheck && let $argstuple = $e.args; $argchecks end)\n    end\nWe strongly recommend the reader trace through this code for some examples until enlightenment strikes.\n\n\n2.11.2.4 Compiling the match\nGiven a list of symbols and a pattern in which they appear, we can produce code to generate a mapping from expressions to either (true,bindings) where bindings is a list of all the subexpressions bound to the name list; or (false, nothing) if there is no match.\nfunction compile_matcher(symbols, pattern)\n    bindings = Dict{Symbol,Any}(s =&gt; nothing for s in symbols)\n\n    # Input expression symbol and matching code\n    # (symbol bindings are indicated by bindings table)\n    expr = gensym()\n    test = match_gen!(bindings, expr, pattern)\n\n    # Get result vals (symbol/nothing) and associated variable names\n    result_vals = [bindings[s] for s in symbols]\n    declarations = filter(x -&gt; x != nothing, result_vals)\n\n    # Produce the matching code\n    results = Expr(:tuple, result_vals...)\n    :($expr -&gt;\n        let $(declarations...)\n            if $test\n                (true, $results)\n            else\n                (false, nothing)\n            end\n        end)\nend\nIt is convenient to compile this into a macro. The macro version also filters line numbers out of the input pattern and out of any exprssion we are trying to match.\nmacro match(symbols, pattern)\n    @assert(symbols.head == :tuple &&\n            all(isa.(symbols.args, Symbol)),\n            \"Invalid input symbol list\")\n\n    pattern = filter_line_numbers(pattern)\n    matcher = compile_matcher(symbols.args, pattern)\n    esc(:($matcher ∘ filter_line_numbers))\nend\n\n\n\n2.11.3 Rules\nA rule has the form\npattern =&gt; args -&gt; code\nwhere args is the list of names that are bound in the pattern, and these names can be used in the subsequent code. We want to allow the code to potentially use not only the matched subexpression, but also to refer to the symbol as a whole; we use the named argument feature in tuples for that. So, for example, in the rule\nx - y =&gt; (x,y;e) -&gt; process(e)\nwe mean to match any subtraction, bind the operands to x and y and the expression as a whole to e, and call process(e) to process the expression.\nNow that we have a macro for computing matches, we can use it to help with parsing the rule declarations into argument names, expression name (if present), the pattern, and the code to be called on a match.\nfunction parse_rule(rule)\n    match_rule = @match (pattern, args, code) pattern =&gt; args -&gt; code\n    isok, rule_parts = match_rule(rule)\n    if !isok\n        error(\"Syntax error in rule $rule\")\n    end\n    pattern, args, code = rule_parts\n\n    match_arg1 = @match (args, expr) (args..., ; expr)\n    match_arg2 = @match (args, expr) (args..., )\n    begin ismatch, bindings = match_arg1(args); ismatch end ||\n    begin ismatch, bindings = match_arg2(args); ismatch end ||\n    begin bindings = (Vector{Any}([args]), nothing) end\n    symbols, expr_name = bindings\n\n    if !all(isa.(symbols, Symbol))\n        error(\"Arguments should all be symbols in $symbols\")\n    end\n    if !(expr_name == nothing || expr_name isa Symbol)\n        error(\"Expression parameter should be a symbol\")\n    end\n\n    symbols, expr_name, pattern, code\nend\nMatching a pattern on an input produces a boolean variable (whether there was a match or not) and a table of bindings from names in the pattern to symbols generated during the match. In order to safely access the right-hand-side symbols that were generated during the match, we need to declare them (in a let statement). If there is a match, we bind them to the ordinary input names (things like :x) in a second let statement, then call the code in that second let statement and assign the result to the result symbol. The resulting code must be used in a context where the expr and result symbols are already set up.\nfunction compile_rule(rule, expr, result)\n\n    symbols, expr_name, pattern, code = parse_rule(rule)\n    bindings = Dict{Symbol,Any}(s =&gt; nothing for s in symbols)\n    test = match_gen!(bindings, expr, pattern)\n\n    # Get list of match symbols and associated declarations\n    result_vals = [bindings[s] for s in symbols]\n    declarations = filter(x -&gt; x != nothing, result_vals)\n\n    # Set up local bindings of argument names in code\n    binding_code = [:($s = $(r == nothing ? (:nothing) : r))\n                    for (s,r) in zip(symbols, result_vals)]\n    if expr_name != nothing\n        push!(binding_code, :($expr_name = $expr))\n    end\n\n    # Produce the rule\n    ismatch = gensym()\n    quote\n        let $(declarations...)\n            $ismatch = $test\n            if $ismatch\n                $result = let $(binding_code...); $code end\n            end\n            $ismatch\n        end\n    end\nend\nNow that we are able to compile a rule, we set up a macro for compiling a rule into a standalone function. The input expression symbol (expr) is the named argument to this standalone function, and at the end the function returns both the match condition (which is what the compiled code evaluates to) and the result (which the compiled code produces as a side effect).\nmacro rule(r)\n    expr, result = gensym(), gensym()\n    code = compile_rule(filter_line_numbers(r), expr, result)\n    esc(quote\n            $expr -&gt;\n                let $result = nothing\n                    $code, $result\n                end\n        end)\nend\nFinally, we often want to combine rules, executing the first one that matches a given input. The @rules macro takes a set of rules packaged into a block and tries them in sequence, returning the result of whichever rule was executed (or nothing if no rule was executed).\nmacro rules(rblock :: Expr)\n    rblock = filter_line_numbers(rblock)\n    if rblock.head != :block\n        error(\"Rules must be in a begin/end block\")\n    end\n\n    # Set up input name, output name, and rule list\n    expr, result = gensym(), gensym()\n    rules = rblock.args\n\n    # Or together all the tests\n    rule_calls =\n        foldr((x,y) -&gt; :($x || $y),\n              [compile_rule(r, expr, result) for r in rules])\n\n    # Call all the rules, return the computed result\n    esc(quote\n            $expr -&gt;\n                let $result = nothing\n                    $rule_calls\n                    $result\n                end\n        end)\nend\n\n\n2.11.4 Examples\nWe conclude our matching example by giving a few examples of the process in practice.\n\n2.11.4.1 Re-associating operations\nJulia parses chains of additions and multiplications into one long list. For example, 1+2+3 parses to (1+2)+3 rather than to +(+(1,2), 3). For some operations, it is simpler to only allow binary addition and multiplication nodes. The following function converts these nodes.\nfunction reassoc_addmul(e)\n\n    # Apply folding to an op(args...) node (op = +, *)\n    fold_args(args, op) =\n        foldl((x,y) -&gt; :($op($x, $y)), args)\n\n    # Rules for processing one expression node\n    r = @rules begin\n        +(args...) =&gt; args -&gt; fold_args(args, :+)\n        *(args...) =&gt; args -&gt; fold_args(args, :*)\n        e =&gt; e -&gt; e\n    end\n\n    # Recursively process the graph\n    process(e :: Expr) = r(Expr(e.head, process.(e.args)...))\n    process(e) = e\n\n    # Process the input expression\n    process(e)\nend\nFor example, the following expression has both long sums and long products that we re-associate into binary operations.\n\nlet\n    e = :(sin(1+2+3*4*f))\n    println(\"Original:    $e\")\n    println(\"Transformed: $(reassoc_addmul(e))\")\nend\n\nOriginal:    sin(1 + 2 + 3 * 4 * f)\nTransformed: sin((1 + 2) + (3 * 4) * f)\n\n\nIf we wanted to, we could also “de-associate” by collapsing sums and multiplications into single nodes.\nfunction deassociate(e)\n    r = @rules begin\n        x * (*(args...)) =&gt; (x, args) -&gt; Expr(:call, :*, x, args...)\n        (*(args...)) * x =&gt; (args, x) -&gt; Expr(:call, :*, args..., x)\n        x + (+(args...)) =&gt; (x, args) -&gt; Expr(:call, :+, x, args...)\n        (+(args...)) + x =&gt; (args, x) -&gt; Expr(:call, :+, args..., x)\n        e =&gt; e -&gt; e\n    end\n    process(e :: Expr) = r(Expr(e.head, process.(e.args)...))\n    process(e) = e\n    process(e)\nend\nWe take the output of our “reassociate” code as an example input.\n\ndeassociate(:(sin((1+2) + (3*4)*f)))\n\n:(sin(1 + 2 + 3 * 4 * f))\n\n\n\n\n2.11.4.2 Simplifying expressions\nAutomatically-generated code of the sort that comes out of naive automatic differentiation often involves operations that can be easily simplified by removing additions with zeros, multiplications with one, and so forth. For example, we write simplify_sum and simplify_mul routines to handle simplification of sums and products involving zeros and ones.\nsimplify_sum(args) =\n    let args = filter(x -&gt; x != 0, args)\n        if length(args) == 1\n            args[1]\n        else\n            Expr(:call, :+, args...)\n        end\n    end\n\nsimplify_mul(args) =\n    let args = filter(x -&gt; x != 1, args)\n        if any(args .== 0)\n            0\n        elseif length(args) == 1\n            args[1]\n        else\n            Expr(:call, :*, args...)\n        end\n    end\nBuilding on simplifying sums and products, we can recurse up and down an expression tree to apply these and other similar rules.\nfunction simplify(e)\n    r = @rules begin\n        +(args...) =&gt; args -&gt; simplify_sum(args)\n        *(args...) =&gt; args -&gt; simplify_mul(args)\n        x - 0 =&gt; x -&gt; x\n        0 - x =&gt; x -&gt; :(-$x)\n        -(0)  =&gt; (;x) -&gt; 0\n        x / 1 =&gt; x -&gt; x\n        0 / x =&gt; x -&gt; 0\n        x ^ 1 =&gt; x -&gt; x\n        x ^ 0 =&gt; x -&gt; 1\n        e     =&gt; e -&gt; e\n    end\n    process(e :: Expr) = r(Expr(e.head, process.(e.args)...))\n    process(e) = e\n    process(e)\nend\nWe illustrate our simplifier with some operations\n\nlet\n    compare(e) = println(\"$e simplifies to $(simplify(e))\")\n    compare(:(0*x + C*dx))\n    compare(:(2*x^1*dx))\n    compare(:(1*x^0*dx))\nend\n\n0 * x + C * dx simplifies to C * dx\n2 * x ^ 1 * dx simplifies to 2 * x * dx\n1 * x ^ 0 * dx simplifies to dx",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#elements-of-julia-style",
    "href": "00-Background/01-Julia.html#elements-of-julia-style",
    "title": "2  Julia programming",
    "section": "2.12 Elements of Julia style",
    "text": "2.12 Elements of Julia style\nWe write programs to be executed by a computer, an unforgivingly literal-minded audience. But the humans who read our code are a more challenging audience by far. We humans get bored. We get sidetracked. We misunderstand the simple things, and declare that we understand complicated things based on only-partly-correct models of how the world works. We read, forget, read again, and forget again. In the face of such an audience, what’s a programmer to do?\nWe seek to write code that accomplishes something worthwhile, and to write in a simple, direct style that can be easily understood by anyone. This is easier said than done. There are some classic books with practical guidance that transcends specific programming languages, and we highly recommend the books of Kernighan and Plauger (1978), Knuth (1984), Kernighan and Pike (1999), and Hunt and Thomas (1999). The style guide for the Julia programming language also provides useful guidance. But since we are here, we will provide our own two cents on what we think are some of the key issues to writing “good” Julia code.\n\n2.12.1 Formatting conventions\nJulia code tends to have some common formatting conventions that make it easier to read code written by varying authors:\n\nIndentation by four spaces\nFunctions that modify (mutate) an input end with an exclamation mark\nModule and type names use CamelCase\nFunction names are lowercase and omit separators when feasible (underscores can be used otherwise)\n\nWe also recommend limiting globally visible names (modules, structures, or functions) to Latin characters, or providing a Latin alternative when a larger set of Unicode characters is used (for example, pi as an alternative to π).\nThere are a number of style guides with more detailed opinions on the proper formatting of Julia code. If you want to adhere tightly to such a guide, we recommend a tool like JuliaFormatter.\n\n\n2.12.2 Correctness first\nCorrectness is a tricky concept in numerical computing. Approximation is at the heart of most numerical methods, whether for data science or otherwise. Hence, we rarely get to decide whether we have “the right answer”; instead, we have to reason about whether the result of a computation is accurate enough for a downstream task. To make matters even more complicated, we usually face tradeoffs between speed and accuracy, and so need to design simultaneously for “right enough” and “fast enough.”\nNeither correctness nor performance are easy to reason about. However, it is easier to tell when numerical code is slow than when it is wrong. Our mental models of why code is slow may often be wrong, but our observation that code is slow are not. Therefore, we usually want to start with simple code together with analysis and tests to ensure that it is accurate, and revise the code to tune performance later, informed by profiling (Chapter 3).\n\n\n2.12.3 Catch errors early\nThe earlier we catch an error, the easier it is to correct. We would rather:\n\nCatch conceptual errors before we start writing code.\nCatch implementation errors at compile time than at run time.\nAnalyze errors in an individual function than have to perform a root cause analysis.\n\nTests, type annotations, and careful assertions all help us with finding errors early.\n\n\n2.12.4 Put it in a (small) function\nFor a human audience, functions are a natural unit for code. A well-designed function gives name to a concept, and ideally the name, comments, and code are all in obvious agreement – it “does what it says on the cover.” Small functions are also relatively easy to test. And assigning appropriate type annotations to the arguments and return values of a function also helps us catch errors in how the function is used.\nApart from readibility, there are good performance reasons for putting code in functions. The Julia compiler works with functions. Code written at a command line or in a script style will not be processed in the same way, and will typically run more slowly. It is also easier to use Julia’s analysis tools to understand the performance implications of design choices made in short functions.\n\n\n2.12.5 Keep the scope small\nFunctions and control flow constructs in Julia create new scopes, so Julia variable names often have short lives. This is convenient for human readers, with our limited memories, particularly when we want to save typing and use short names. Variable names with limited scope (and limited opportunity for reassignment) are also easy for the compiler to analyze, and result in more efficient code.\n\n\n2.12.6 Respect interfaces\nUnlike some languages (e.g. Python, Java, and C++), Julia does not have a strong data hiding mechanism, though it is possible to code in a way that hides data structures (e.g. by tucking them inside functions). Nonetheless, if the option exists, it is usually preferable to use getter and setter functions rather than directly accessing the contents of a Julia structure. This makes it easier to monitor access to the data structures for debugging, and it makes it easier to change data structures as the code evolves.\n\n\n2.12.7 DRY (Don’t Repeat Yourself)\nA general design principle for any data system is that code and data should ideally have a single, concise, authoritative representation.\nCompared to some languages, Julia code is fairly concise, and does not require much boilerplate6. Several aspects of the language contribute to this conciseness:\n\nThe ability of the system to dispatch to different methods based on argument types simplifies the process of writing generic code.\nFunctions are first-class entities that can be passed around with context, so we can pass around specialized functions for a specific subtask rather than coding it everywhere. For example, we can pass logging routines into our functions as callbacks rather than adding logging infrastructure to all our routines.\nConversion methods and promotion rules limit the number of variants of methods that we might have to write for different combinations of argument types.\nWhen other methods fail, we can remove redundancy by writing macros to generate repetitive code for us.\n\n\n\n2.12.8 KISS and YAGNI\nPeer to DRY in our opinions on design style are two other acronyms:\n\nKISS: Keep It Simple, Stupid\nYAGNI: You Ain’t Gonna Need It\n\nThat is, we want to start with simple, working code that solves a known problem, and only get fancier if there is a measurable reason to do so. Simple code and simple data structures are easier for people to read, and preferable absent other real concerns. For example, any number of Julia constructs that we have discussed (promotion rules, macros, and expansion of tuples into parameter lists) are arguably harder to read than alternative constructions. It is nice to have these features in the language in situations to avoid writing redundant code or to address performance bottlenecks during profiling. But if we have not observed redundancy or performance issues in practice, it is usually best to write the simplest thing possible. In the event we decide that we do need a less simple code construct, whether to avoid repetition or to deal with a performance issue, we have the experience of writing the short and simple thing first. And, as with writing English prose, it is usually easier to revise what exists than to start from a blank page.\n\n\n\n\nGraham, Paul. 1993. On Lisp. Prentice Hall. https://paulgraham.com/onlisp.html.\n\n\nHunt, Andy, and David Thomas. 1999. The Pragmatic Programmer: From Journeyman to Master. Addison Wesley.\n\n\nKernighan, Brian W., and Rob Pike. 1999. The Practice of Programming. Addison Wesley.\n\n\nKernighan, Brian W., and P. J. Plauger. 1978. The Elements of Programming Style. Second. McGraw-Hill.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/01-Julia.html#footnotes",
    "href": "00-Background/01-Julia.html#footnotes",
    "title": "2  Julia programming",
    "section": "",
    "text": "In the beginning, the creators of the C language allowed the same piece of memory to be treated as belonging to different types. This has made a lot of people very angry and been widely regarded as a bad move.↩︎\nAt the time of this writing, the default Int type in Julia is almost always 64 bits. It is not so easy to find 32-bit systems any more.↩︎\nIn 2003, Joel Spolsky wrote a blog post on “The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)”, which includes the memorably line “if you are a programmer working in 2003 and you don’t know the basics of characters, character sets, encodings, and Unicode, and I catch you, I’m going to punish you by making you peel onions for 6 months in a submarine. I swear I will.” That was two decades ago at the time of this writing. Unicode as a concept and UTF-8 as an encoding are both ubiquitous. There is no excuse for not knowing a little about it.↩︎\nThe notion of “null” is surprisingly subtle. This can refer to something that is “deliberately left blank” (e.g. a default value for an optional argument), something that is “well-defined but unknown” (e.g. a record of someone’s age) or something that is inappropriate to the task at hand (e.g. spouses’ name when not married).↩︎\nAs the Julia documentation points out, improvements to the compiler may mean this will no longer be an issue soon. This does not change the value of this as an example.↩︎\nPer Wikipedia, the metal printing plates used for typesetting widely-distributed ads or syndicated columns were called “boilerplate” by analogy with the rolled steel plates used to make water biolers.↩︎",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Julia programming</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html",
    "href": "00-Background/02-Performance.html",
    "title": "3  Performance Basics",
    "section": "",
    "text": "3.1 Time to what?\nNumerical code should be right enough and fast enough. Correctness comes first, but we are also impatient, and want our codes to run fast. We do not recommend our readers should become reckless speed demons: improvements in running time should be weighed against the time required to implement and maintain the code, and for most of us it is unwise to get into the business of rewriting our codes every year to eke every jot of speed out of the newest processor. But certain details of how we implement our methods can make order-of-magnitude differences in how fast our codes run on most modern processors, and a little knowledge of those details (along with a few tools) can go a long way toward keeping us happy with both the performance and the tidiness of our codes.\nThe key metric in performance of numerical methods is usually the wall clock time to a solution. In addition to wall clock time, one might want to understand how much memory, disk space, network bandwidth, or power are used in a given computation. However, all these measures may depend on implementation details, on details of the problem being solved, and on the system used.\nWe would certainly like code that we have optimized to run fast on problems other than our test problems; and ideally, we would like our performance to be portable (or at least “transportable” with small changes) to new systems. To accomplish this, it is useful to have both performance models that we can use to generalize beyond a single instance and performance experiments to validate our models and to fit any free parameters. Sometimes we use proxy measures for wall clock time, such as the number of arithmetic operations in a code, with the suggestion that these proxies relate to wall clock time by a simple (usually linear) model. Such proxy measures should be treated with caution unless backed by data. We discuss this in Section 3.2, Section 3.4, and Section 3.7.\nThe notion of “time to solution” has some additional subtleties. Some algorithms run to completion, then return a result more-or-less instantaneously at the end. In many cases, though, a computation will produce intermediate results. In this case, there are usually two quantities of interest in characterizing the performance:\nFor problems involving information retrieval or communication, we usually refer to these as the latency and bandwidth. But the idea of an initial response time and subsequent throughput rate applies more broadly.\nWhen we think about concepts of latency and throughput, we are measuring performance not by a single number (time to completion), but by a curve of utility versus time. When we think about a fixed latency and throughput, we are implicitly defining a piecewise linear model of utility versus time: there’s an initial period of zero utility, followed by a period of linearly increasing utility with constant slope (until completion). The piecewise linear model is attractive in its simplicity, but more complex models are sometimes useful. For example, to understand the performance of an iterative solver, the right way to measure performance might be in terms of approximation error versus time.\nWe are also often interested in situations where we incrementally recompute something based on new data. In this case, we might have initial setup costs that need only be run once, and thereafter are not required again (or are only required periodically). In this case, there is a tradeoff: the setup costs may be expensive, but what we care about is the setup cost together with the incremental costs. In this case, we say the setup costs are amortized over the computation.\nWhile it is not our main topic in this chapter, it is also worthwhile to pay attention to resources that we care about that fall outside our computation. This might include things like input/output costs – reading data in and writing results out can be surprisingly expensive! It can also include things like the number of times that we require human attention, or the amount of experimental data required to adequately fit a model.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#time-to-what",
    "href": "00-Background/02-Performance.html#time-to-what",
    "title": "3  Performance Basics",
    "section": "",
    "text": "The time to first result (response time)\nRate at which results are subsequently produced (throughput)",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#sec-performance-scaling",
    "href": "00-Background/02-Performance.html#sec-performance-scaling",
    "title": "3  Performance Basics",
    "section": "3.2 Scaling analysis",
    "text": "3.2 Scaling analysis\nFor most problem classes, there is some natural measure of the size of the problem. This could be the number of data points we measure, the size of a linear system to be solved, etc. Scaling analysis for algorithms has to do with the way the cost changes as the size parameter \\(n\\) grows, along with other parameters such as the number of processors.\nWe usually write scaling analyis using order notation (aka “big O” notation), which refers to sets of functions with certain growth rates; we say \\(f(n)\\) is\n\n\\(O(g(n))\\) if \\(f\\) grows no faster than \\(g\\): there is an integer \\(N\\) and positive constant \\(C\\) such that for all \\(n \\geq N\\), \\(f(n) \\leq C\ng(n)\\).\n\\(o(g(n))\\) if \\(f\\) grows more slowly than \\(g\\): for any positive \\(C\\), there is an integer \\(N\\) such that for all \\(n &gt; N\\), \\(f(n) &lt; C g(n)\\).\n\\(\\Omega(g(n))\\) if \\(f\\) grows no more slowly than \\(g\\): there is an integer \\(N\\) and positive constant \\(C\\) such that for all \\(n \\geq N\\), \\(f(n) \\geq C g(n)\\).\n\\(\\omega(g(n))\\) if \\(f\\) grows strictly faster than \\(g\\): for any positive \\(C\\), there is an integer \\(N\\) such that for all \\(n &gt; N\\), \\(f(n) &lt; C g(n)\\).\n\\(\\Theta(g(n))\\) if \\(f\\) and \\(g\\) grow at the same rate: there is an integer \\(N\\) and positive constants \\(c\\) and \\(C\\) such that for all \\(n\n\\geq N\\), \\(c g(n) \\leq f(n) \\leq C g(n)\\).\n\nEven though order notation defines sets of functions, we usually abuse notation and write things like \\(f(n) = O(g(n))\\) rather than the more proper \\(f(n) \\in O(g(n))\\). We will see the same notation in a different context (\\(x\\) going to zero rather than \\(n\\) going to infinity) in Chapter 5.\nAs a concrete example, that will recur frequently, let us consider the complexity of the Basic Linear Algebra Subroutines (BLAS), a collection of standard linear algebra operations often used as building blocks for higher-level algorithms. Typical examples are dot products, matrix-vector products, and matrix-matrix products. For the case of square matrices, these cost \\(O(n^1)\\), \\(O(n^2)\\), and \\(O(n^3)\\) time, respectively. In the language of the BLAS standard, we call these level 1, level 2, and level 3 BLAS calls. However, while this captures the correct scaling, it is nowhere near the full story: a fast implementation of the standard matrix-matrix multiply1 can be orders of magnitude faster than the standard three nested loops.\nOrder notation is convenient, and we will use it often. But it is convenient in part because it is crude: if we say the runtime of an algorithm is \\(O(n^3)\\), that means there is some \\(N\\) and \\(C\\) such that the runtime is bounded by \\(Cn^3\\) for all \\(n \\geq N\\) – but that says nothing about the size of \\(C\\) or \\(N\\). Indeed, we expect the tightest version of the constant \\(C\\) to vary depending on details of the implementation and the system we use. As we will see in Section 3.4, we usually will want a scaling analysis as the start of understanding performance, but it is not the conclusion.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#sec-performance-arch",
    "href": "00-Background/02-Performance.html#sec-performance-arch",
    "title": "3  Performance Basics",
    "section": "3.3 Architecture basics",
    "text": "3.3 Architecture basics\nIn introductory programming classes, students typically form a mental model of how an idealized computer works. There is a memory, organized into a linear address space. A program is stored in memory, and consists of machine language instructions for things like register read/write, logic, and arithmetic. The computer reads these instructions from memory and executes them in program order. And, we think, all operations take about the same amount of time.\nThis mental model is incomplete in many respects, though that does not keep it from being useful. The reader who has had an undergraduate computer architecture course (e.g. from Hennessey and Patterson (2017)) will already appreciate the impact on performance of the memory hierarchy and instruction-level parallelism on performance. For the reader who has not had such a course (or has not reviewed the material recently), a brief synopsis may be useful.\n\n3.3.1 Memory matters\nIn a crude accounting of the work done by numerical codes, we often count the number of floating point operations (flops) required. On modern machines, though, the cost of a floating point operation pales in comparison to the cost of an access to main memory. On one core of the laptop on which this text is being written2, more than 2500 64-bit floating point operations can (in theory) be performed in the latency period for a read request to main memory; and the subsequent bandwidth is such that we could execute more than three floating point operations per floating point number read from main memory. Though the details of these numbers vary from machine to machine, the core message remains the same: if we are not careful, the dominant cost of many numerical computations is not arithmetic, but data transfers.\n\n3.3.1.1 Locality\nWhat saves us from being forever limited by main memory is a memory hierarchy with small memories with fast access (caches) absorbing some of the traffic that would otherwise go to larger memories with slower access farther away. This hierarchy is engineered to reduce main memory traffic for programs that exhibit two sorts of locality:\n\nSpatial locality is the tendency to consecutively access memory locations that are close together in address space.\nTemporal locality is the tendency to frequently re-use a (small) “working set” of data in a particular part of a computation.\n\nSome programs are born with data that fits in cache, some achieve good cache utilization through “natural” locality, and some have locality thrust upon them. In many cases, performance tuning of numerical codes falls into the last category: a naively written code will not have good locality and will tend to be limited by memory, but we can impvove matters by rearranging the computations for a more cache-friendly access pattern. But to get the best use from caches, we need a few more details.\n\n\n3.3.1.2 Hits, misses, and operational intensity\nProcessors may have a register file containing data that can be operated on immediately, two or three levels of cache (with L1 the fastest and smallest, then L2, and then L3), and a main memory. When a program requests data from a particular address, the processor first consults with the lowest level of cache. If the data is in the cache (a cache hit), it can be returned otherwise. In the event of a cache miss, the processor consults with higher levels of cache to try to find the data, eventually going to main memory if necessary.\nTo exploit spatial locality, caches are organized into lines of several bytes; when data is read into cache, we fetch it a cache line at a time, possibly saving ourselves some subsequent cache misses. To exploit temporal locality, the processor tries to keep a line in cache until the space is needed for something else. An eviction policy determines when a cache line will be replaced by other data. The eviction policy depends in part on the associativity of the cache. In a direct-mapped cache, each address can only go in one cache location, usually determined by the low-order bits of the storage address. In an \\(n\\)-way set associative cache, each address can go into one of \\(n\\) possible cache locations; in case a line must be evicted, the processor will choose something like the least recently used of the set (an LRU policy). Higher levels of associativity are more expensive in terms of hardware, and so are usually associated with the lowest levels of cache.\nNaturally, we would like to minimize the number of cache misses. To design code with few misses, it is useful to think of three distinct types of misses3:\n\nCompulsory: when the data is loaded into cache for the first time.\nCapacity: when the working set is too big and the cache was filled with other things since it was last used.\nConflict: when there is insufficient associativity for the access pattern.\n\nFor codes with low operational intensity (or arithmetic intensity), compulsory misses are the limiting factor in how well we can use the cache. The operational intensity is defined to be the ratio of operations to memory reads. For example, consider a dot product of two vectors (assumed not to be resident in cache): if each vector is length \\(n\\), we have \\(2n\\) floating point operations (adds and multiplies) on \\(2n\\) floating point numbers for an operational intensity of one flop per float. We can, at best, hope that we are accessing the numbers in sequential order so that we only have a cache miss every few numbers (depending on the number of floating point numbers that fit into a cache line). Such a routine is inherently memory-bound, i.e. it is limited not by the time to do arithmetic but by the time to retrieve data.\nIn contrast to dot products, square matrix-matrix products involve \\(2n^3\\) floating point operations, but only involve \\(2n^2\\) input numbers — an operational intensity of \\(n\\) operations per float. In this case, we are not so limited by compulsory misses. However, unless the matrices are small enough to fit entirely into cache, a naively-written code may still suffer capacity misses. To get around this, we might use blocking or tiling, reorganizing the matrix-matrix product in terms of products of smaller submatrices. Even with such a reorganization, we might need to be careful about conflict misses, particularly when \\(n\\) is a multiple of a large power of 2 (the worst case scenario for set-associative caches).\n\n\n\n3.3.2 Instruction-level parallelism\nIf we do not use caches well, our speed will be limited by memory traffic. But once we have reduced memory traffic enough, the rate at which we can execute operations becomes the limiting factor; that is, we are compute bound rather than memory bound. To improve the performance of compute-bound codes, we need to understand a little more about a little about instruction-level parallelism.\nA typical processor consists of several cores. Codes that are not explicitly parallel run on just one core at a time. Each core presents a serial programming interface: the core acts like it executes machine instructions in program order. This interface is fine for reasoning about correctness of programs. But the interface hides a much more sophisticated implementation.\nIn introductory computer architecture classes, we usually start by discussing a five stage pipeline, where for each instruction we\n\nFetch the instruction from memory\nDecode the instruction\nExecute the instruction\nPerform any memory operations\nWrite back results to the register file\n\nIn any given cycle, each pipeline stage can be occupied by a different instruction; while we are writing back the first instruction in a sequence, we might be performing a memory operation associated with the second instruction, doing some arithmetic for the third instruction, and decoding and fetching the fourth and fifth instruction. Hence, though each instruction typically has a latency of five cycles to execute (modulo waiting for memory), the processor in principle can execute instructions with a bandwidth of one instruction per cycle. We do not always achieve this peak number of instructions per cycle, though. For example, if the second instruction depends on the result of the first instruction, then we cannot execute the second instruction until the first instruction has written its results back to the register file. This results in a “bubble” in the pipeline where some of the stages are idle, and reduces the rate at which the processor can execute instructions.\nThe picture in most machines now is more complicated than a five stage pipeline. Modern processors are often wide: the front-end pipeline can fetch and decode several operations in a single cycle. The fetch-and-decode itself can be rather complex, with one machine language instruction turned into several “micro-ops” internally. Once an instruction is decoded, it is kept in a re-order buffer until the processor is ready to dispatch it to a functional unit like a floating point unit or memory unit. These functional units themselves have internal pipelines, and so can process several instructions concurrently. As the functional units complete their work, the results are written back in an order consistent with the program order.\nA wide, pipelined, out-of-order processor can have many instructions in flight at the same time. The extent to which we can keep the processor fully occupied depends on two factors:\n\nLimited dependencies: As with the five-stage pipeline, dependencies between instructions can limit the amount of instruction-level parallelism. These dependencies can take the form of data dependencies (one operation takes as input the result of a previous operation) or control dependencies (we cannot complete instructions after a branch until the branch condition has been computed). The out-of-order buffer and branch prediction techniques can help mitigate the impact of dependencies, but we still expect code with simple control structures and lots of independent operations to run faster than code with complicated control and lots of tight data dependencies.\nInstruction diversity: A mix of different types of instructions can keep the different functional units occupied concurrently.\n\nIn addition to the implicit instruction-level parallelism we have just described, many modern processors provide explicit instruction-level parallelism in the form of vector or SIMD (single-instruction multiple-data) instructions that simultaneously compute the same operation (e.g. addition or multiplication) on short vectors of inputs.\nGiven code with simple structure and enough evident independent operations, compilers can do a good job at producing code that uses vector instructions, just as they do a good job at reordering instructions for the processor.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#sec-performance-model",
    "href": "00-Background/02-Performance.html#sec-performance-model",
    "title": "3  Performance Basics",
    "section": "3.4 Performance modeling",
    "text": "3.4 Performance modeling\nRuntime can depend on many parameters: the problem size and structure, the algorithms used in the computation, details of the implementation, the number and type of processors used, etc. Performance models predict runtime (and perhaps the use of other resources) as a function of these parameters. There are several reasons to want such a model:\n\nTo decide whether a method is worth the effort of implementing or tuning.\nTo decide what algorithm to use (and what parameter settings) for the best runtime on a particular problem and system.\nTo decide whether a particular subcomputation is likely to be a bottleneck in a larger computation.\nTo determine whether the runtime of a computation is “reasonable” or if there is room for easy improvement in the implementation.\n\nScaling analysis (Section 3.2) does not usually yield a good performance model on its own. Even if we estimate of the number of floating point operations in a given computation, we know from Section 3.3 that there may be several orders of magnitude depending in runtime depending how memory is used and the amount of instruction-level parallelism. At the same time, a somewhat inaccurate model is often fine to guide engineering decisions. Because models reflect our understanding, they will almost always be incomplete; indeed, the most useful models are necessarily incomplete, since otherwise they are too cumbersome to reason about!\nExperiments reflect what really happens, and are a critical counterpoint to models. The division between performance models and experiments is not sharp. In the extreme case, we can use machine learning and other empirical function fitting methods can be used to estimate runtimes from experimental measurements under weak assumptions. But when strongly empirical models have many parameters, a lot of data is needed to fit them well; otherwise, the models may be overfit, and do a poor job of predicting performance except away from the training data. Gathering a lot of data may be appropriate for cases where the model is used as the basis for auto-tuning a commonly-used kernel for a particular machine architecture, for example. But performance experiments often aren’t cheap – or at least they aren’t cheap in the regime where people most care about performance – and so a simple, theory-grounded model is often preferable. There’s an art to balancing what should be modeled and what should be treated semi-empirically, in performance analysis as in the rest of science and engineering.\n\n3.4.1 Applications, benchmarks, and kernels\nThe performance of application codes is usually what we really care about. But application performance is generally complicated. The main computation may involve alternating phases, each complex in its own right, in addition to time to load data, initialize any data structures, and post-process results. Because there are so many moving parts, its also hard to use measurements of the end-to-end performance of a given code on a given machine to infer anything about the speed expected of other codes. Sometimes it’s hard even to tell how the same code will run on a different machine!\nBenchmark codes serve to provide a uniform way to compare the performance of different machines on “characteristic” workloads. Usually benchmark codes are simplified versions of real applications (or of the computationally expensive parts of real applications); examples include the NAS parallel benchmarks, the Graph 500 benchmarks, and (on a different tack) Sandia’s Mantevo package of mini-applications.\nKernels are frequently-used subroutines such as matrix multiply, FFT, breadth-first search, etc. Because they are building blocks for so many higher-level codes, we care about kernel performance a lot; and a kernel typically involves a (relatively) simple computation. A common first project in parallel computing classes is to time and tune a matrix multiplication kernel.\nParallel patterns (or “dwarfs”) are abstract types of computation (like dense linear algebra or graph analysis) that are higher level than kernels and more abstract than benchmarks. Unlike a kernel or a benchmark, a pattern is too abstract to benchmark. On the other hand, benchmarks can elucidate the performance issues that occur on a given machine with a particular type of computation.\n\n\n3.4.2 Model composition\nCounting floating-point operations is generally a bad way to estimate performance, because the “cost per flop” is so poorly defined. But the performance of larger building blocks may be much more stable, so that call counts (as opposed to operation counts) are a reasonable basis for performance modeling. For example, we frequently describe the performance of iterative solvers for sparse systems of linear equations in terms of the number of matrix-vector products; and for a particular matrix size, the time for a matrix-vector product will usually be about constant4. If all other operations in the solver are cheap compared to the cost of the matrix-vector product, just measuring the number of products and separately building a model for the time \\(t_{\\mathrm{matvec}}\\) for one product (or measuring it) may be adequate for predicting performance.\nIf we decide that a particular iterative solver should be faster, we might try to make matrix-vector products run faster, perhaps by rearranging the data structure that represents the matrix. This might be accomplished with some setup cost \\(t_{\\mathrm{setup}}\\) to analyze the input matrix and rearrange the data. If the rearranging the multiplication speeds it up by a factor of \\(S'\\), then the cost of the original code vs the initial code is \\[\\begin{aligned}\nt_{\\mathrm{original}} &= n t_{\\mathrm{matvec}} \\\\\nt_{\\mathrm{rearranged}} &= t_{\\mathrm{setup}} + n t_{\\mathrm{matvec}} / S'\n\\end{aligned}\\] The rearranged computation will be faster if \\[\n  t_{\\mathrm{setup}} &lt; t_{\\mathrm{matvec}} n (S'-1)/S'.\n\\] Even a fairly large setup cost may be worthwhile if it is amortized over enough iterations. Even if rearranging the data structure does not make sense for a single linear solve, it may make sense to rearrange if we are solving many similar systems (in which case we might be able to pay the setup cost just once rather than for every solve).\nAlternately, we could try to find an alternative method that requires fewer matrix-vector products, though this might involve more expensive iterations. This can be a net win even if each iteration is “less efficient” in terms of the rate of floating point operations. It is worth remembering that the key measure is not the arithmetic rate, but the time to completion (or time to adequate accuracy). In each case, we can reason about the performance with the same general strategy of reasoning about the performance of smaller building blocks.\n\n\n3.4.3 Modeling with kernels\nWhen we study dense matrix computations ?sec-nla-ch, we will write many of our algorithms in “block” fashion, so that almost all the work is done in level 3 BLAS operations. Because these have high operational intensity, it is possible (though difficult) to write these operations to run at pretty close to the peak arithmetic rate, at least for a single core. Fortunately, these operations are so common that we can usually rely on someone else to have done the hard work of writing a fast implementation in this setting. Hence, in dense matrix computations we often write that something requires a certain number of floating point operations that are “mostly level 3 BLAS,” and assume that the reader understands that for this problem, the number of floating point operations times the peak flop rate (or some adjusted version thereof) is a reasonable estimate for the runtime.\nMore generally, for codes that rely on common well-tuned kernel operations like the level 3 BLAS or tuned FFTs, we can sometimes get away with assuming that the kernel runs at an appropriate “speed of light” for the hardware.\n\n\n3.4.4 The Roofline model\nIf we are going to assume kernels that run at an appropriate “speed of light” for hardware, we need to understand what that speed is. The Roofline model is a simple model of peak performance for different computational kernels (Williams, Waterman, and Patterson (2009)). The model consists of a log-log plot of the operational intensity \\(I\\) (in floating point operations / byte) vs the operation rate (floating point operations per second). The actual operation rate always sits under a “roofline,” the minimum of the limit imposed by the peak memory bandwidth \\(\\beta\\) (operation rate is less than \\(\\beta \\times\nI\\), a diagonal line on the plot) and the peak operational rate attainable by the hardware (a horizontal line on the plot). More elaborate versions of the roofline plot can include additional performance ceilings (e.g. the ceiling without the use of vector instructions, or without the use of multiple cores) and bandwidth ceilings (e.g. associated with multiple levels of cache).\n\n\n3.4.5 Amdahl’s law\nWe measure improvements to performance of a code on a fixed problem size by the speedup: \\[\n  S = \\frac{T_{\\mathrm{baseline}}}{T_{\\mathrm{improved}}}.\n\\] Suppose we have a code involving two types of work: some fraction \\(\\alpha\\) is work that we do not know how to speed up, and the remaining fraction \\((1-\\alpha)\\) we think we can improve. Let \\(S'\\) be the speedup for the part that we know how to improve; then \\[\n\\begin{aligned}\n  S &=\n  \\frac{T_{\\mathrm{baseline}}}\n       {\\alpha T_{\\mathrm{baseline}} +\n       (1-\\alpha) T_{\\mathrm{baseline}}/S'} \\\\\n  &=   \\frac{1}{\\alpha + (1-\\alpha)/S'} \\\\\n  &=   \\frac{S'}{1 + \\alpha (S'-1)}\n\\end{aligned}\n\\] No matter how big the speedup \\(S'\\) for the part we know how to improve, the overall speedup is bounded by \\(1/\\alpha\\). This observation is known as Amdahl’s law.\nAmdahl’s law is best known in the context of parallel computing, where we are interested in speedup for a fixed problem size as a function of the number of processors \\(p\\): \\[\n  S(p) = \\frac{T_{\\mathrm{serial}}}{T_{\\mathrm{parallel}}(p)}.\n\\] Studying the speedup \\(S(p)\\) in this setting (or the parallel efficiency \\(S(p)/p\\)) is known as a strong scaling study. In this setting, \\(\\alpha\\) is the fraction of serial work. If the rest of the computation can be perfectly sped up (i.e. \\(S' = p\\)), then \\[\n  S(p) = \\frac{p}{1+\\alpha (p-1)} \\leq \\frac{1}{\\alpha}.\n\\] In practice, this is usually a generous estimate: some overheads usually grow with the number of processors, so that past a certain number of processors the speedup often doesn’t just level off, but actually decreases.\n\n\n3.4.6 Gustafson’s law\nAmdahl’s law is an appropriate modeling tool when we are trying to improve the runtime to solve problems of a fixed size, whether by parallel computing or by tuning code. Sometimes, though, we want to improve runtime because we want to solve bigger problems! This leads to a different scaling relationship.\nIn weak scaling studies in parallel computing, we usually consider the scaled speedup \\[\nS(p) = \\frac{T_{\\mathrm{serial}}(n(p))}\n            {T_{\\mathrm{parallel}}(n(p),p)}\n\\] where \\(n(p)\\) is a family of problem sizes chosen so that the work per processor remains constant. For weak scaling studies, the analog of Amdahl’s law is Gustafson’s law; if \\(a\\) is the amount of serial work and \\(b\\) is the parallelizable work, then \\[\nS(p) \\leq \\frac{a + bP}{a + b} = p-\\alpha(p-1)\n\\] where \\(\\alpha = a/(a+b)\\) is the fraction of serial work.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#sec-performance-principles",
    "href": "00-Background/02-Performance.html#sec-performance-principles",
    "title": "3  Performance Basics",
    "section": "3.5 Performance principles",
    "text": "3.5 Performance principles\n\nThere is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the seed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say 97% of the time: premature optimization is the root of all evil.\nYet, we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified. It is often a mistake to make a priori judgements about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail. …\n– Donald Knuth (from Knuth (1974))5\n\nWe want our codes to run fast, but not at great cost in terms of maintainability or development time. To do this, it is useful to keep some principles in mind.\n\n3.5.1 Think before you write\nBack-of-the-envelope performance models are often6 enough to give us a sense of the “big computations,” parts of a code will be critical to performance. We might also have a sense in advance of what the natural algorithmic variants are for this code are. If we think a particular routine might be a performance bottleneck that we will want to play with, it is probably worth thinking about how to code that routine so it is easy to experiment with it (or replace it).\nWhen thinking about performance, it is worthwhile remembering to explicitly account for I/O. Memory accesses may move at a snail’s pace compared to arithmetic; but I/O is positively glacial compared to either.\nBefore writing any code, it is also useful to\n\nThink through the implications of algorithms and data structures. Sometimes it is possible to just make a code do much less work without too much efort by using the right standard tools.\nThink about data layouts, if only to make sure that the code does not become too wedded to a specific data layout in memory.\nDecide if an approximation is good enough.\n\n\n\n3.5.2 Time before you tune\nWhen codes are slow, it is often because a large amount of time is spent in a few key bottlenecks. In this case, our goal is to find and fix those bottleneck computations7.\nFor removing bottlenecks – or even for deciding that a code needs to be tuned in the first place – we need to pay attention to some practical points about the design of timing experiments:\n\nFor codes that show any data-dependent performance, it is important to profile on something realistic, as the time breakdown will depend on the use case.\nWe also need to be aware that wall-clock time is not the only type of time we can measure – for example, many systems have some facility to monitor “CPU time,” in which only the time that a task is using the processor is counted. Wall-clock time is typically what we care about.\nThe system wall-clock time resolution is often much coarser than the CPU cycle time. Consequently, we need to make sure that enough work is done in a timing experiment or we might not get good data. A typical approach to this is to put code to be timed into a loop.\nThe same routine run repeatedly may run faster after a first iteration that warms up the cache.\nThere will probably be other processes running on the system, and cross-interference with other tasks can affect timing.\n\nProfiling involves running a code and measuring how much time (and resources) are used in different parts of the code. One can profile with different levels of detail. The simplest case often involves manually instrumenting a code with timers. There are also tools that automatically instrument either the source code or binary objects to record timing information. Sampling profilers work differently; they use system facilities to interrupt the program execution periodically and measure where the code is. It is also possible to use hardware counters to estimate the number of performance-relevant events (such as cache misses or flops) that have occurred in a given period of time. We will discuss these tools in more detail as we get into the class (and we’ll use some of them on our codes).\nAs with everything else, there are tradeoffs in running a profiler: methods that provide fine-grained information can produce a lot of data, enough that storing and processing profiling data can itself be a challenge. There is also an issue that very fine-grained measurement can interfere with the software being measured. It is often helpful to start with a relatively crude, lightweight profiling technology in order to first find what’s interesting, then focus on the interesting bits for more detailed experimentation.\n\n\n3.5.3 Shoulders of giants\nA good computational kernel is a general building block with a simple interface that does a fair amount of work. We want kernels to be general in order to amortize the work of tuning. We also ideally like kernels with high operational intensity, though not all kernels will have this property.\nWe have already discussed the BLAS as an example of kernel design, and noted that the high arithmetic intensity of level 3 BLAS (e.g. matrix-matrix multiplication) makes it a particularly useful building block for high-performance code. Other common kernel operations include\n\nApplying a sparse matrix to a vector (or powers of a sparse matrix)\nComputing a discrete Fourier transform\nSorting a list\n\nCode written using common kernel interfaces is transportable: the implementation will differ from plarform to platform depending on architectural details, but the common interface means that code that uses the different kernel implementations will run the same way. It is critical to get properly tuned implementations of these kernels — for example, linear algebra codes run with the reference BLAS library invariably have disappointing performance.\nThere are some cases when we want to be careful with general-purpose kernels. For example, for some types of structured matrices, the general matrix-matrix multiply routines from the BLAS may have higher operational complexity than a specialized implementation. Whether “higher operational complexity” means “more run time” depends on the size of the problem and the arithmetic rate that a specialized implementation attains — it is sometimes worthwhile to write the more specialized code, but not always! As another example, in some situations (like finite element codes or computer graphics), we may want to do many operations with small matrices (e.g. 3-by-3 or 4-by-4), and the BLAS calls may not be as efficient for such small \\(n\\).\n\n\n3.5.4 Help tools help you\nCompilers, including the Julia compiler, are often quite effective at local optimizations, particularly those restricted to a “basic block” (straight-line code with no conditionals or loops). Loop optimizations are somewhat harder, and global optimizations that cross function boundaries are much harder. In practice, this means that very local optimizations are usually only effective when the programmer has information that the compiler might not have, but humans can help the compiler much more with figuring out global optimizations.\nTo take some concrete examples: compilers are better than humans at register allocation and instruction scheduling, and usually at branch joins and jump elimination. Indeed, at this level it is hard for humans to even attempt to interfere with what the compiler is doing! For operations like constant folding and propagation or the elimination of common subexpressions, the compiler might do a good job if it can figure out that there are no side effects (like I/O or changing the contents of an array) – but the compiler might not easily be able to establish that there are no side effects without some help8. Compilers are often good at simple loop transformations involving loop invariant code motion, unrolling, and vectorization. But compilers are also usually conservative about using algebraic identities to transform code9, for example.\nApart from using tools for writing our code in performance-friendly Julia (as discussed in Section 3.6), we can help the compiler in two ways. First, we recognize that the compiler mostly looks at a little bit of the code at a time, and does not know the higher-level semantics of that code. Complex algebraic transformations are usually up to the programmer. Second, the compiler is really good at dealing with simple code without too many dependencies. In particular, we want to avoid very complex loops or conditional statements, as well as tricky use of functional programming constructs in performance-sensitive inner loops.\n\n\n3.5.5 Tune data structures\nAs we have seen, memory access and communication patterns are critical to performance. The system is designed to make it fast to process small amounts of data that fit in cache, ideally by going through it in memory order (a “unit-stride” access pattern). But this is not the most natural access pattern for all the codes we might want to write! Consequently, tuning often involves looking not at code but at the data that the code manipulate: rearranging arrays for unit stride, simplifying structures with many levels of indirection,using single precision for high-volume floating point data, etc. With a proper interface abstraction, the fastest way to high performance often involves replacing a low-performance data structure with an equivalent high-performance structure.\nWhile we are mainly concerned with accessing data structures (reads and writes), access is not the only cost. Allocation and deallocation also cost something, as does garbage collection. It is easy to end up paying hidden allocation costs (often followed by hidden copying costs). Fortunately, Julia provides us with diagnostics to identify memory allocations; and with some care, we can write code to pre-allocate key data structures.\nMatrices are a key data structure in many numerical codes. In Julia, as in MATLAB and Fortran, matrices are stored in a column-major format, with each column layed out consecutively in memory. Consequently, we usually prefer to process data column-by-column (rather than row-by-row). For example, the code\nfor j = 1:n\n    for i = 1:m\n        y[i] += A[i,j]*x[j]\n    end\nend\nwill run significantly faster than\nfor i = 1:m\n    for j = 1:n\n        y[i] += A[i,j]*x[j]\n    end\nend\nAs another example, consider the layout of many instances of a particular structure type. We could organize this as an array of structures, which is good for locality of access to the data for one item; or we could organize a structure of arrays, which may be more friendly to vectorization. Particularly for read-only access patterns, we might also consider a copy optimization where we keep around both data structures10, and use whichever data structure gives us the best performance in context.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#sec-performance-julia",
    "href": "00-Background/02-Performance.html#sec-performance-julia",
    "title": "3  Performance Basics",
    "section": "3.6 Performance in Julia",
    "text": "3.6 Performance in Julia\nSo far, we have focused on general performance issues that are mostly relevant across languages. However, there are some things that are more specific to Julia.\n\n3.6.1 Measurement tools\nWe repeat the advice of Section 3.5: you should measure your code before tuning it! Fortunately, Julia provides several macros that measure runtime and memory allocation:\n\n@time prints out runtime and allocation information\n@timev prints out a more verbose version of the timing and allocation information\n@elapse prints just the runtime\n@timed returns a structure that includes run time, memory allocation, and information about garbage collection\n@elapsed returns the elapsed time\n@allocated returns the total number of bytes allocated by the expression\n@allocations returns the numbre of allocations in the expression\n\nFor more elaborate timing with tabular outputs, we can use the TimerOutputs.jl package. The @time macros and even the more elaborate TimerOutputs.jl package time a single run of a function. The BenchmarkTools.jl package runs a code multiple times in order to get more accurate timing information.\nThe Julia Profile package implements a statistical profiler. Statistics are gathered by running an expression with the @profile macro. There are several different visualization interfaces that can be used to view the profiling results, including a particularly nice visualizer built into VS Code.\n\n\n3.6.2 Avoiding boxing\nIn dynamically-typed languages, the types of values are often only known at runtime. Hence, the system keeps “boxes” that contain both type information and the value. The coding for this pair varies from system to system11, but the practice of boxing in general can cause problems with performance. The run-time system has to switch between different operations depending on the type, which introduces branches in the code for many operations and makes it effectively impossible for the compiler to do code transformations like vectorization. Boxed values also generally take more storage than unboxed values would.\nThough Julia is a dynamically-typed language, it can often produce low-level code that avoids boxing. To do this, Julia uses a just-in-time (JIT) compiler that instantiates specialized versions of methods depending on the concrete type signature of the inputs12. The just-in-time compilation process is necessary in part because it would be ridiculously expensive to produce specialized implementations of generic methods for every possible compatible type; however, most functions are only invoked with a small number of type signatures in any given program, so most conceivable implementations never need to be generated in practice. For a type stable method, the system can determine the concrete type of the return value from the concrete input type signature. Ideally, we would like type stability not just for the return value, but for other intermediate values and variables as well. If the JIT compiler can reliably determine the concrete types of quantities, it can work with that data directly without boxes.\nThe @code_warntype macro in Julia does type inference for a particular method call and colors in red any expressions that represent a performance hazard because the type system can only determine an abstract type. In situations where it is convenient to allow abstract types on input but performance still matters, the Julia performance tips recommends putting the inner part of the function into its own separate kernel function that is type stable (the “function barrier technique”).\nSeveral other performance issues in Julia are associated with the potential need for boxing when a concrete type cannot be inferred. For example, the Julia manual recommends avoiding untyped globals, because there is possibility that they might be assigned to different types, so they might be boxed; this also poses is a hazard to the type stability of any method that uses them. We should also avoid containers of abstract types, since the items in the container then have to be boxed; and, for the same reason, we should be careful with structures with abstract types for fields. Finally, while Julia’s functional programming features are convenient, when creating a closure it is easy to run into unanticipated boxing of captured variables from the surrounding environment.\n\n\n3.6.3 Temporary issues\nVector operations in Julia (addition and scalar multiplication) produce temporaries. For example, if x and y are vector variables, then the function\nf1(x,y) = x.^2 + 2x.*y + y.^2\nwill generate temporaries for x.^2, 2x, 2x.*y, and y.^2 (as well as storage for the final result). If we use dotted operations throughout, then Julia fuses the operations and does not produce temporaries:\nf2(x,y) = x.^2 .+ 2.0 .* x .* y .+ y.^2\nAn equivalent, but less verbose, option is to use the @. macro to make a fused broadcast version of the whole expression:\nf3(x,y) = @. x^2 + 2x*y + y^2\nWe could also use a broadcast call to a helper function, which again will avoid allocating temporaries:\nf4scalar(x, y) = x^2 + 2x*y + y^2\nf4(x, y) = f4scalar.(x, y)\nOn the machine on which this is being written, the function f1 takes about five times the memory and six times the run time of any of the other versions for input vectors x and y of length a million.\nIf we pre-allocate storage for the result, we do not need any allocations:\nfunction f5!(result, x, y)\n    @. result = x^2 + 2x*y + y^2\nend\nWhen x and y are long vectors, there is negligible runtime performance difference between writing to a pre-allocated vector and producing a newly-allocated output vector. If the vectors are short, though, we may start to notice the cost of allocating (and garbage collecting) these temporaries, particularly if the code has a large working set and more vectors leads to more cache pressure. On the other hand, if the input vectors are short and their sizes are known at compile time, we may also want to consider declaring them to be static arrays (using the StaticArrays.jl package).\nAnother place where Julia allocates new storage is in slicing operations used to specify a subarray. This happens even if the “slice” is the whole array. For example, if x is a vector in Julia, writing\nz = x\nassigns the name z to the same vector object, while writing\nz = x[:]\ncreates a new copy of x. If we want to refer to the entries of a subarray without creating a copy, we can create a “view” object (a SubArray) by either calling the view function or using the @views macro:\nz1 = x[1:10]        # Copy of the start of x\nz2 = view(x, 1:10)  # View of the start of x\n@views z2 = x[1:10] # View of the start of x\nz1[1] = 100         # Does not alter x, only z1\nz2[1] = 100         # Now x[1] == 100\nIn general, many of the linear algebra functions in Julia provide a mutating version that writes results into user-provided storage. This includes factorization routines (e.g. cholesky!), solves (with ldiv!), and matrix-vector or matrix-matrix products (with mul!). But using these mutating operations is more error-prone, and we do not recommend it until a timing experiment or a performance model suggests that allocating new storage might cause a problem.\n\n\n3.6.4 Performance annotations\nJulia provides several macros to tell the compiler to try certain types of transformations. For example,\n\n@inbounds: tells the compiler that there is no need to check that array accesses are in bounds\n@fastmath: tells the compiler that it is OK to apply certain floating point transformations that are not equivalent to the original code (e.g. by allowing the compiler to pretend floating point addition and multiplication are associative)\n@simd: tells the compiler to try vectorizing the code through SIMD instructions. @simd ivdep promises that loop iterations are independent and references are free from aliasing.\n\nThere are a number of packages that provide additional transformations (e.g. @turbo from LoopVectorization.jl.\nWe do not recommend using performance annotation macros until and unless a timing experiment or performance model suggests that it addresses a bottleneck. These macros effectively prompting the compiler to do certain transformations it might not otherwise do. If the compiler can deduce that these transformations are equivalent to the original code and will result in a performance improvement, the macros will often be unnecessary. If the compiler cannot deduce that these transformations are equivalent to the original code, but you as the human author think that they are obviously equivalent, you may want to double-check your assumptions of what is “obvious” before proceeding.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#sec-performance-misconceptions",
    "href": "00-Background/02-Performance.html#sec-performance-misconceptions",
    "title": "3  Performance Basics",
    "section": "3.7 Misconceptions and deceptions",
    "text": "3.7 Misconceptions and deceptions\n\nIt ain’t ignorance causes so much trouble; it’s folks knowing so much that ain’t so.\n– Josh Billings\n\nOne of the common findings in pedagogy research is that an important part of learning an area is overcoming common misconceptions about the area; see, e.g. Leonard, Kalinowski, and Andrews (2014), Sadler et al. (2013), Muller (2008). And there are certainly some common misconceptions about high performance computing! Some misconceptions are exacerbated by bad reporting, which leads to deceptions and delusions about performance.\n\n3.7.1 Incorrect mental models\n\n3.7.1.1 Algorithm = implementation\nWe can never time algorithms. We only time implementations, and implementations vary in their performance. In some cases, implementations may vary by orders of magnitude in their performance!\n\n\n3.7.1.2 Asymptotic cost is always what matters\nWe can’t time algorithms, but we can reason about their asymptotic complexity. When it comes to scaling for large \\(n\\), the asymptotic complexity can matter a lot. But comparing the asymptotic complexity of two algorithms for modest \\(n\\) often doesn’t make sense! QuickSort may not always be the fastest algorithm for sorting a list with ten elements…\n\n\n3.7.1.3 Simple serial execution\nHardware designers go to great length to present us with the interface that modern processor cores execute a instructions sequentially. But this interface is not the actual implementation. Behind the scenes, a simple stream of x86 instructions may be chopped up into micro-instructions, scheduled onto different functional units acting in parallel, and executed out of order. The effective behavior is supposed to be consistent with sequential execution – at least, that’s what happens on one core – but that illusion of sequential execution does not extend to performance.\n\n\n3.7.1.4 Flops are all that count\nData transfers from memory to the processor are often more expensive than the computations that are run on that data.\n\n\n3.7.1.5 Flop rates are what matter\nWhat matters is time to solution. Often, the algorithms that get the best flop rates are not the most asymptotically efficient methods; as a consequence, a code that uses the hardware less efficiently (in terms of flop rate) may still give the quickest time to solution.\n\n\n3.7.1.6 All speedup is linear\nSee the comments above about Amdahl’s law and Gustafson’s law. We rarely achieve linear speedup outside the world of embarrassingly parallel applications.\n\n\n3.7.1.7 All applications are equivalent\nPerformance depends on the nature of the computation, the nature of the implementation, and the nature of the hardware. Extrapolating performance from one computational style, implementation, or hardware platform to another is something that must be done very carefully.\n\n\n\n3.7.2 Deceptions and self-deceptions\nThe article “Twelve Ways to Fool the Masses When Giving Performance Results on Parallel Computers” (Bailey (1991)) is a classic in performance analysis. It’s still worth reading (as are various follow-up pieces – see the Further Reading section), and highlights issues that we still see now. To summarize slightly, here’s my version of the list of common performance deceptions:\n\n3.7.2.1 Unfair comparisons and strawmen\nA common sin in scaling studies is to compare the performance of a parallel code on \\(p\\) processors against the performance of the same code with \\(p = 1\\). This ignores the fact that the parallel code may have irrelevant overheads, or (worse) that there may be a better organization for a single processor. Consequently, the speedups no longer reflect the reasonable expectation of the reader that this is the type of performance improvement they might see when going to a good parallel implementation from a good serial implementation. Of course, it’s also possible to see great speedups by comparing a bad serial implementation to a correspondingly bad parallel implementation: a lot of unnecessary work can hide overheads.\nA similar issue arises when computing with accelerators. Enthusiasts of GPU-accelerated codes often claim order of magnitude (or greater) performance improvements over using a CPU alone. Often, this comes from explicitly tuning the GPU code and not the CPU code (see, e.g., Vuduc et al. (2010)).\n\n\n3.7.2.2 Using the wrong measures\nIf what you care about is time to solution, you might not really care so much about watts per GFlop (though you certainly do if you’re responsible for supplying power for an HPC installation). More subtlely, you don’t necessarily care about scaled speedup if the natural problem size is fixed (e.g. in some graph processing applications).\n\n\n3.7.2.3 Deceptive plotting\nThere are so many ways this can happen:\n\nUse of a log scale when one ought to have a linear scale, and vice-versa;\nChoosing an inappropriately small range to exaggerate performance differences between near-equivalent options;\nNot marking data points clearly, so that there is no visual difference between data falling on a straight line because it closely follows a trend and data falling on a straight line because there are two points.\nHiding poor scalability by plotting absolute time vs numbers of processors so that nobody can easily see that the time for 100 processors (a small bar relative to the single-processor time) is equivalent to the time for 200 processors.\nAnd more!\n\nPlots allow readers to absorb trends very quickly, but it also makes it easy to give wrong impressions.\n\n\n3.7.2.4 Too much faith in models\nAny model has limits of validity, and extrapolating outside those limits leads to nonsense. Treat with due skepticism claims that – according to some model – a code will run an order of magnitude faster in an environment where it has not yet been run.\n\n\n3.7.2.5 Undisclosed tweaks\nThere are many ways to improve performance. Sometimes, better hardware does it; sometimes, better tuned code; sometimes, algorithmic improvements. Claiming that a jump in performance comes from a new algorithm without acknowledging differences in the level of tuning effort, or acknowledging non-algorithmic changes (e.g. moving from double precision to single precision) is deceptive, but sadly common. Hiding tweaks in the fine print in the hopes that the reader is skimming doesn’t make this any less deceptive!\n\n\n\n3.7.3 Rules for presenting performance results\nIn the introduction to Bailey, Lucas, and Williams (2010), David Bailey suggests nine guidelines for presenting performance results without misleading the reader. Paraphrasing only slightly, these are:\n\nFollow rules on benchmarks\nOnly present actual performance, not extrapolations\nCompare based on comparable levels of tuning\nCompare wall clock times (not flop rates)\nCompute performance rates from consistent operation counts based on the best serial codes.\nSpeedup should compare to best serial version. Scaled speedup plots should be clearly labeled and explained.\nFully disclose information affecting performance: 32/64 bit, use of assembly, timing of a subsystem rather than the full system, etc.\nDon’t deceive skimmers. Take care not to make graphics, figures, and abstracts misleading, even in isolation.\nReport enough information to allow others to reproduce the results. If possible, this should include\n\nThe hardware, software and system environment\nThe language, algorithms, data types, and coding techniques used\nThe nature and extent of tuning\nThe basis for timings, flop counts, and speedup computations\n\n\n\n\n\n\nBailey, David H. 1991. “Twelve Ways to Fool the Masses When Giving Performance Results on Parallel Comptuers.” Supercomputing Review, August. https://www.davidhbailey.com/dhbpapers/twelve-ways.pdf.\n\n\nBailey, David H., Robert F. Lucas, and Samuel Williams, eds. 2010. Performance Tuning of Scientific Applications. CRC Press. https://doi.org/10.1201/b10509.\n\n\nHennessey, John L., and David A. Patterson. 2017. Computer Architecture: A Quantitative Approach. Sixth. Elsevier.\n\n\nKnuth, Donald E. 1974. “Structured Programming with go to Statements.” Computing Surveys 6 (4).\n\n\nLeonard, Mary J., Steven T. Kalinowski, and Tessa C. Andrews. 2014. “Misconceptions Yesterday, Today, and Tomorrow.” CBE – Life Sciences Education 13 (2). https://doi.org/10.1187/cbe.13-12-0244.\n\n\nMuller, Derek A. 2008. “Designing Effective Multimedia for Physics Education.” PhD thesis, University of Sydney.\n\n\nSadler, Philip M., Gerhard Sonnert, Harold P. Coyle, Nancy Cook-Smith, and Jaimie L. Miller. 2013. “The Influence of Teachers’ Knowledge on Student Learning in Middle School Physical Science Classrooms.” American Educational Research Journal 50 (5). https://doi.org/10.3102/0002831213477680.\n\n\nVuduc, Richard, Aparna Chandramowlishwaran, Jee Choi, Kenneth Czechowski, Marat Guney, Logan Moon, and Aashay Shringarpure. 2010. “On the Limits of GPU Acceleration.” In HotPar’10: Proceedings of the 2nd USENIX Conference on Hot Topics in Parallelism. Berkeley, CA. https://doi.org/10.5555/1863086.1863099.\n\n\nWilliams, Samuel, Andrew Waterman, and David Patterson. 2009. “Roofline: An Insightful Visual Performance Model for Multicore Architectures.” Communications of the ACM 52 (4). https://doi.org/10.1145/1498765.1498785.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/02-Performance.html#footnotes",
    "href": "00-Background/02-Performance.html#footnotes",
    "title": "3  Performance Basics",
    "section": "",
    "text": "Strassen’s algorithm for matrix-matrix multiplication has better asymptotic complexity than the standard algorithm, at \\(O(n^{\\log_2 7})\\) running time. Despite the better asymptotic complexity, Strassen’s algorithm is at best an improvement for rather large matrices, and is not used in most BLAS libraries.↩︎\nThese numbers are for a Firestorm core (performance core) on an Apple M1 Pro, taken from a review on Anandtech. These cores have four floating point pipelines, each of which can execute operations on 128-bit vectors (two double-precision floating point numbers). At 3.2 GHz, this corresponds to a processor bandwidth of 25.6 floating point operations (adds or multiplies) per nanosecond. We estimate the latency to main memory at about 100 ns, so about 2560 floating point operations in the time to get the first byte of a memory transfer from memory. Memory bandwidth to one core seems to be about 60 GB/s, or about 7.5 double-precision floating point numbers per nanosecond.↩︎\nIn shared-memory parallelism, we are also concerned about coherence misses when multiple processors write to locations in memory that are close to each other, and thus the local cache lines must be invalidated to ensure that all processors are viewing memory in the same ways. This is important, but beyond the scope of the current treatment.↩︎\nFor small problems, the cost of the first matrix-vector product in a series of such computations may be slower than subsequent products. This is because the first product might suffer compulsory cache misses, but afterward the cache is “warmed.”↩︎\nThe quote about “premature optimization” in Knuth (1974) is probably the best known quote from this paper, but the rest of the paper is worth reading as well.↩︎\nThis assumes that our back-of-the-envelope performance models are at least somewhat correct.↩︎\nSome wag once called this process “deslugging.”↩︎\nA function that always runs the same way with no side effects is called a pure function. Though Julia has some support for functional programming, unlike some languages, it does not assume functions are pure by default. There is a @pure macro in Julia for declaring functions are pure, but it is easily abused (enough so that the Julia developers have declared that it should only be used in the Base packages).↩︎\nWe want compilers to be conservative about applying some types of algebraic transformations to floating point code, for reasons we will discuss in Chapter 9.↩︎\nKeeping multiple data structures representing the same data is sometimes good for performance, but violates the “don’t repeat yourself” advice from Chapter 2.↩︎\nA common representation of boxed values is via a tagged union: a structure with an tag (usually stored as an int) and then a union whose bits can be interpreted as an integer, floating point number, pointer to a more complex object, etc. Some language implementations use more elaborate compact representations to pack both data and type information into a 64-bit word (e.g. by using NaN encodings in floating point to signal non-floating-point types, or using the low-order bits of aligned pointers as a type tag).↩︎\nThe JIT compiler operates on functions. The system does not compile script-style code, or code entered at the REPL that is not inside a function. If you want performance, put it in a function!↩︎",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Basics</span>"
    ]
  },
  {
    "objectID": "00-Background/03-LA.html",
    "href": "00-Background/03-LA.html",
    "title": "4  Linear Algebra",
    "section": "",
    "text": "4.1 Vector spaces\nWe will not get far in this book without a strong foundation in linear algebra. That means knowing how to compute with matrices, but it also means understanding the abstractions of linear algebra well enough to use them effectively. We assume prior familiarity with linear algebra at the level of an undergraduate course taught from Strang (2023) or Lay, Lay, and McDonald (2015). It will be helpful, but not necessary, to have a more advanced perspective as covered in Axler (2024) or even Lax (2007). Even for readers well-versed with linear algebra, we suggest skimming this chapter, as some ideas (quasimatrices, the framework of canonical forms) are not always standard in first (or even second) linear algebra courses.\nWhile our treatment will be fairly abstract, we try to keep things concrete by regularly giving examples in Julia. Readers unfamiliar with Julia may want to review Chapter 2; it is also possible to simply skim past the code examples.\nWe do not assume a prior course in functional analysis. This will not prevent us from sneaking a little functional analysis into our discussion, or treating some linear algebra concepts from a functional analysis perspective.\nA vector space (or linear space) over a field \\(\\mathbb{F}\\) is a set with elements (vectors) that can be added or scaled in a sensible way – that is, addition is associative and commutative and scaling (by elements of the field) is distributive. We will always take \\(\\mathbb{F}\\) to be \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\). We generally denote vector spaces by script letters (e.g. \\(\\mathcal{V}, \\mathcal{W}\\)), vectors by lower case Roman letters (e.g. \\(v, w\\)), and scalars by lower case Greek letters (e.g. \\(\\alpha, \\beta\\)). But we feel free to violate these conventions according to the dictates of our conscience or in deference to other conflicting conventions.\nThere are many types of vector spaces. Apart from the ubiquitous concrete vector spaces \\(\\mathbb{R}^n\\) and \\(\\mathbb{C}^n\\), the most common vector spaces in applied mathematics are different types of function spaces. These include \\[\\begin{aligned}\n  \\mathcal{P}_d &=\n  \\{ \\mbox{polynomials of degree at most $d$} \\}; \\\\\n  L(\\mathcal{V}, \\mathcal{W}) &=\n  \\{ \\mbox{linear maps $\\mathcal{V}\\rightarrow \\mathcal{W}$} \\}; \\\\\n  \\mathcal{C}^k(\\Omega) &=\n  \\{\\mbox{ $k$-times differentiable functions on a set $\\Omega$} \\}; \\\\\n  \\ell^1 &= \\{ \\mbox{absolutely summable sequences } x_1, x_2, \\ldots \\} \\\\\n  \\ell^2 &= \\{ \\mbox{absolutely square-summable sequences } x_1, x_2, \\ldots \\} \\\\\n  \\ell^\\infty &= \\{ \\mbox{bounded sequences } x_1, x_2, \\ldots \\}\n\\end{aligned}\\] and many more. A finite-dimensional vector space can be put into 1-1 correspondence with some concrete vector space \\(\\mathbb{F}^n\\) (using a basis). In this case, \\(n\\) is the dimension of the space. Vector spaces that are not finite-dimensional are infinite-dimensional.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "00-Background/03-LA.html#vector-spaces",
    "href": "00-Background/03-LA.html#vector-spaces",
    "title": "4  Linear Algebra",
    "section": "",
    "text": "4.1.1 Polynomials\nWe compute with vectors in \\(\\mathbb{F}^n\\) (\\(\\mathbb{R}^n\\) and \\(\\mathbb{C}^n\\)), which we represent concretely by tuples of numbers in memory, usually stored in sequence. To keep a broader perspective, we will also frequently describe examples involving the polynomial spaces \\(\\mathcal{P}_d\\).\nThe Julia Polynomials.jl package provides a variety of representations of and operations on polynomials.\n\nusing Polynomials\n\nFor example, we can construct a polynomial in standard form from its coefficients in the monomial basis or from its roots, or we can keep it in factored form internally.\n\nPolynomial([2, -3, 1])\n\n2 - 3∙x + x2\n\n\n\nfromroots([1, 2])\n\n2 - 3∙x + x2\n\n\n\nfromroots(FactoredPolynomial, [1, 2])\n\n(x - 1) * (x - 2)\n\n\nThe fact that there are several natural representations for polynomials is part of what makes them a good example of an abstract vector space.\n\n\n4.1.2 Dual spaces\nA vector space in isolation is a lonely thing1. Fortunately, every vector space has an associated dual space of linear functionals2, that is \\[\n  \\mathcal{V}^* =\n  \\{ \\mbox{linear functions $\\mathcal{V} \\rightarrow \\mathbb{F}$} \\}.\n\\] Dual pairs are everywhere in applied mathematics. The relation between random variables and probability densities, the theory of Lagrange multipliers, and the notion of generalized functions are all examples. It is also sometimes convenient to recast equations in terms of linear functionals, e.g. using the fact that \\(v = 0\\) iff \\(\\forall w^* \\in \\mathcal{V}^*,\nw^* v = 0\\).\nIn matrix terms, we usually associate vectors with columns and dual vectors with columns (ordinary vectors) and rows (dual vectors). In Julia, we construct a dual vector by taking the conjugate transpose of a column, e.g.\n\ntypeof([1.0; 2.0; 3.0])   # Column vector in R^3\n\n\nVector{Float64} (alias for Array{Float64, 1})\n\n\n\n\ntypeof([0.0; 1.0; 0.0]')  # Row vector in R^3\n\nAdjoint{Float64, Vector{Float64}}\n\n\nApplying a linear functional is concretely just “row times column”:\n\nlet\n    v = [1.0; 2.0; 3.0]   # Column vector in R^3\n    w = [0.0; 1.0; 0.0]'  # Row vector in R^3\n    w*v\nend\n\n2.0\n\n\nIn order to try to keep the syntax uniform between concrete vectors and operations on abstract polynomial spaces, we will define a DualVector type representing a (linear) functional\n\nstruct DualVector\n    f :: Function\nend\n\nWe overload the function evaluation and multiplication syntax so that we can write application of a dual vector \\(w^*\\) to a vector \\(v\\) as w(v) or as w*v.\n\n(w :: DualVector)(v) = w.f(v)\nBase.:*(w :: DualVector, v) = w(v)\n\nBecause dual vectors still make up a vector space, we would like to be able to add and scale them:\n\nBase.:+(w1 :: DualVector, w2 :: DualVector) =\n    DualVector(v -&gt; w1(v) + w2(v))\nBase.:-(w1 :: DualVector, w2 :: DualVector) =\n    DualVector(v -&gt; w1(v) - w2(v))\nBase.:-(w :: DualVector) =\n    DualVector(v -&gt; -w(v))\nBase.:*(c :: Number, w :: DualVector) =\n    DualVector(v -&gt; c*w(v))\nBase.:*(w :: DualVector, c :: Number) = c*w\nBase.:/(w :: DualVector, c :: Number) =\n    DualVector(v -&gt; w(v)/c)\n\nEvaluating a polynomial at a point is one example of a functional in \\(\\mathcal{P}_d^*\\); another example is a definite integral operation:\n\nlet\n    p = Polynomial([2.0; -3.0; 1.0])\n    w1 = DualVector(p -&gt; p(0.0))\n    w2 = DualVector(p -&gt; integrate(p, -1.0, 1.0))\n    w1*p, w2*p\nend\n\n(2.0, 4.666666666666666)\n\n\n\n\n4.1.3 Subspaces\nGiven a vector space \\(\\mathcal{V}\\), a (nonempty3) subset \\(\\mathcal{U}\\subset \\mathcal{V}\\) is a subspace if it is closed under the vector space operations (addition and scalar multiplication). We usually take \\(\\mathcal{U}\\) to inherit any applicable structure from \\(\\mathcal{V}\\), such as a norm or inner product. Many concepts in numerical linear algebra and approximation theory are best thought of in terms of subspaces, whether that is nested subspaces used in approximation theory, subspaces that are invariant under some linear map, or subspaces and their orthogonal complements.\nThere are a few standard operations involving subspaces:\n\nThe span of a subset \\(S \\subset \\mathcal{V}\\) is the smallest subspace that contains \\(S\\), i.e. the set of all finite linear combinations \\(\\sum_i c_i s_i\\) where each \\(s_i \\in S\\) and \\(c_i \\in \\mathbb{F}\\).\nGiven a collection of subspaces \\(\\mathcal{U}_i\\), their sum is the subspace \\(\\mathcal{U}\\) consisting of all sums of elements \\(\\sum_i u_i\\) where only finitely many \\(u_i \\in \\mathcal{U}_i\\) are nonzero. We say the sum is a direct sum if the decomposition into vectors \\(u_i \\in \\mathcal{U}_i\\) is unique.\nThe annihilator for a subspace \\(\\mathcal{U}\\subset \\mathcal{V}\\) is the set of linear functionals that are zero on \\(\\mathcal{U}\\). That is: \\[\n  \\mathcal{U}^\\perp = \\{ w^* \\in \\mathcal{V}^* : \\forall u \\in \\mathcal{U}, w^* u = 0 \\}.\n\\] In inner product spaces, the annihilator of a subspace is identified with the orthogonal complement (Section 4.1.7).\nGiven a subspace \\(\\mathcal{U}\\subset \\mathcal{V}\\), the quotient space \\(\\mathcal{V}\n/ \\mathcal{U}\\) is a vector space whose elements are equivalence classes under the relation \\(v_1 \\sim v_2\\) if \\(v_1-v_2 \\in \\mathcal{U}\\). We can always find a “complementary” subspace \\(\\mathcal{U}^c \\subset \\mathcal{V}\\) isomorphic to \\(\\mathcal{V}/ \\mathcal{U}\\) such that each equivalence class in \\(\\mathcal{V}/ \\mathcal{U}\\) contains a unique representative from \\(\\mathcal{U}^c\\). Indeed, we can generally find many such spaces! For any complementary subspace \\(\\mathcal{U}^c\\), \\(\\mathcal{V}\\) is a direct sum \\(\\mathcal{U}\n\\oplus \\mathcal{U}^c\\).\nIn the infinite-dimensional setting, the closure of a subspace consists of all limit points in the subspace. This only makes sense in topological vector spaces where we have enough structure that we can take limits. We will always consider the even stronger structure of normed vector spaces.\n\n\n\n4.1.4 Quasimatrices\nMatrix notation is convenient for expressing linear algebra over concrete vector spaces like \\(\\mathbb{R}^n\\) and \\(\\mathbb{C}^n\\). To get that same convenience for abstract vector spaces, we introduce the more general notion of quasimatrices, matrix-like objects that involve more than just ordinary numbers. This includes letting columns or rows belong to an abstract vector space instead of a concrete space like \\(\\mathbb{R}^m\\) or \\(\\mathbb{R}^n\\), or using linear maps between subspaces as the “entries” of a matrix.\n\n4.1.4.1 Column quasimatrices\nA (column) quasimatrix is a list of vectors in some space \\(\\mathcal{V}\\) interpreted as columns of a matrix-like object, i.e. \\[\n  U = \\begin{bmatrix} u_1 & u_2 & \\ldots & u_n \\end{bmatrix}.\n\\] We write a linear combination of the columns of \\(U\\) as \\[\n  Uc = \\sum_{i=1}^n u_i c_i\n\\] for some coefficient vector \\(c \\in \\mathbb{F}^n\\). The range space \\(\\mathcal{R}(U) = \\{ Uc : c \\in \\mathbb{F}^n \\}\\) (or column space) is the set of all possible linear combinations of the columns; this is the same as the span of the set of column vectors. The key advantages of column quasimatrices over sets of vectors is that we assign indices to name the vectors and we allow for the possibility of duplicates.\nWhen \\(\\mathcal{V}\\) is a concrete vector space like \\(\\mathbb{R}^m\\) or \\(\\mathbb{C}^m\\), the notion of a quasimatrix coincides with the idea of an ordinary matrix, and taking a linear combination of columns corresponds to ordinary matrix-vector multiplication. We will typically use the word “quasimatrix” when the columns correspond to vectors in an abstract vector space like \\(\\mathcal{P}_d\\).\nWe can represent a quasimatrix with columns in \\(\\mathcal{P}_d\\) by an ordinary Julia array of Polynomials, e.g.\n\nUdemo = let\n    p1 = Polynomial([1.0])\n    p2 = Polynomial([-1.0, 1.0])\n    p3 = Polynomial([2.0, -3.0, 1.0])\n    transpose([p1; p2; p3])\nend\n\n1×3 transpose(::Vector{Polynomial{Float64, :x}}) with eltype Polynomial{Float64, :x}:\n Polynomial(1.0)  Polynomial(-1.0 + 1.0*x)  Polynomial(2.0 - 3.0*x + 1.0*x^2)\n\n\nWe can use these together with standard matrix operations involving concrete vectors, e.g.\n\nUdemo*[1.0 1.0; 1.0 1.0; 1.0 0.0]\n\n1×2 transpose(::Vector{Polynomial{Float64, :x}}) with eltype Polynomial{Float64, :x}:\n Polynomial(2.0 - 2.0*x + 1.0*x^2)  Polynomial(1.0*x)\n\n\nWe also want to be able to multiply a dual vector by a quasimatrix to get a concrete row vector:\n\nBase.:*(w :: DualVector, U :: AbstractMatrix) = map(w, U)\n\nFor example,\n\nDualVector(p-&gt;p(0.0)) * Udemo\n\n1×3 transpose(::Vector{Float64}) with eltype Float64:\n 1.0  -1.0  2.0\n\n\nWe sometimes want to refer to a subset of the columns. In mathematical writing, we will use notation similar to that in Julia or MATLAB: \\[\n  U_{:,p:q} = \\begin{bmatrix} u_p & u_{p+1} & \\ldots & u_q \\end{bmatrix}.\n\\] We will also sometimes (horizontally) concatenate two quasimatrices with columns in the same \\(\\mathcal{V}\\), e.g. \\[\n  U = \\begin{bmatrix} U_{:,1:p} & U_{:,p+1:n} \\end{bmatrix}.\n\\] The range of the horizontal concatenation of two quasimatrices is the same as the sum of the ranges, i.e. \\[\n  \\mathcal{R}(U) = \\mathcal{R}(U_{:,1:p}) + \\mathcal{R}(U_{:,p+1:n}).\n\\]\n\n\n4.1.4.2 Row quasimatrices\nEvery vector space \\(\\mathcal{V}\\) over a field \\(\\mathbb{F}\\) has a natural dual space \\(\\mathcal{V}^*\\) of linear functions from \\(\\mathcal{V}\\) to \\(\\mathbb{F}\\) (aka linear functionals). A row quasimatrix is a list of vectors in a dual space \\(\\mathcal{V}^*\\) interpreted as the rows of a matrix-like object, i.e. \\[\n  W^* = \\begin{bmatrix} w_1^* \\\\ w_2^* \\\\ \\vdots \\\\ w_m^* \\end{bmatrix}.\n\\] We write linear combinations of rows as \\[\n  c^* W^* = \\sum_{i=1}^m \\bar{c}_i w_i^*;\n\\] the set of all such linear combinations is the row space of the quasimatrix. We can refer to a subset of rows with colon notation as in Julia or MATLAB: \\[\n  (W^*)_{p:q,:} = \\begin{bmatrix} w_p^* \\\\ w_{p+1}^* \\\\ \\vdots \\\\ w_q^* \\end{bmatrix}.\n\\] We can also combine row quasimatrices via horizontal concatenation, e.g. \\[\n  W^* = \\begin{bmatrix} (W^*)_{1:p,:} \\\\ (W^*)_{p+1:m,:} \\end{bmatrix} .\n\\]\nIn Julia, analogous to the column quasimatrices over spaces \\(\\mathcal{P}_d\\), we can define a structure representing row quasimatrices over spaces \\(\\mathcal{P}_d^*\\).\n\nWdemo = DualVector.(\n    [p -&gt; p(0.0);\n     p -&gt; p(1.0);\n     p -&gt; integrate(p, -1.0, 1.0)])\n\n3-element Vector{DualVector}:\n DualVector(var\"#27#30\"())\n DualVector(var\"#28#31\"())\n DualVector(var\"#29#32\"())\n\n\n\n\n4.1.4.3 Duality relations\nFor a column quasimatrix \\(U\\) with columns in \\(\\mathcal{V}\\), the map \\(c \\mapsto Uc\\) takes a concrete vector \\(c \\in \\mathbb{F}^n\\) to an abstract vector \\(\\mathcal{V}\\). Alternately, if \\(w^* \\in \\mathcal{V}^*\\) is a linear functional, we have \\[\n  d^* = w^* U =\n  \\begin{bmatrix} w^* u_1 & w^* u_2 & \\ldots & w^* u_n \\end{bmatrix},\n\\] representing a concrete row vector such that \\(d^* c = w^* (Uc)\\). So \\(U\\) can be associated either with a linear map from a concrete vector space \\(\\mathbb{F}^n\\) to an abstract vector space \\(\\mathcal{V}\\), or with a linear map from the abstract dual space \\(\\mathcal{V}^*\\) to a concrete (row) vector space associated with linear functionals on \\(\\mathbb{F}^n\\).\n\n# Example: map concrete [1; 1; 1] to a Polynomial\nUdemo*[1; 1; 1]\n\n2.0 - 2.0∙x + 1.0∙x2\n\n\n\n# Example: map DualVector to a concrete row\nDualVector(p-&gt;p(0))*Udemo\n\n1×3 transpose(::Vector{Float64}) with eltype Float64:\n 1.0  -1.0  2.0\n\n\nSimilarly, a row quasimatrix \\(W^*\\) can be interpreted as a map from a concrete row vector to an abstract dual vector in \\(\\mathcal{V}^*\\), or as a map from an abstract vector in \\(\\mathcal{V}\\) to a concrete vector in \\(\\mathbb{F}^m\\).\n\n# Example: map concrete row to an abstract dual vector\ntypeof([1.0; 1.0; 1.0]'*Wdemo)\n\nDualVector\n\n\n\n# Example: map Polynomial to a concrete vector\nWdemo.*Polynomial([2.0, -3.0, 1.0])\n\n3-element Vector{Float64}:\n 2.0\n 0.0\n 4.666666666666666\n\n\nComposing the actions of a row quasimatrix with a column quasimatrix gives us an ordinary matrix representing a mapping between two abstract vector spaces: \\[\n  W^* U =\n  \\begin{bmatrix}\n  w_1^* u_1 & \\ldots & w_1^* u_n \\\\\n  \\vdots &  & \\vdots \\\\\n  w_m^* u_1 & \\ldots & w_m^* u_n\n  \\end{bmatrix}\n\\] For example,\n\nWdemo*Udemo\n\n3×3 Matrix{Float64}:\n 1.0  -1.0  2.0\n 1.0   0.0  0.0\n 2.0  -2.0  4.66667\n\n\nConversely, composing the actions of a column quasimatrix with a row quasimatrix (assuming \\(m = n\\)) gives us an operator mapping \\(\\mathcal{V}\\) to \\(\\mathcal{V}\\): \\[\n  U W^* = \\sum_{k=1}^n u_k w_k^*.\n\\] In Julia, we can write this mapping as\n\nUWdemo(p) = Udemo*(Wdemo.*p)\n\nUWdemo (generic function with 1 method)\n\n\nThese two compositions correspond respectively to the inner product and outer product interpretations of matrix-matrix multiplication.\n\n\n4.1.4.4 Infinite quasimatrices\nWe may (rarely) discuss “infinite” quasimatrices with a countable number of columns. We will be analysts about it instead of algebraists. In the case of column quasimatrices:\n\nWe assume the columns lie in some complete vector space.\nWhen we take a linear combination \\(Uc\\), we assume the sequence \\(c\\) is restricted to make sense of \\[\n  Uc = \\lim_{n\\rightarrow \\infty} U_{:,1:n} c_{1:n}.\n\\] Usually, this will mean \\(c\\) is bounded (\\(\\ell^\\infty\\)), absolutely summable (\\(\\ell^1\\)), or absolutely square summable (\\(\\ell^2\\)).\nWe take the “range” of such a quasimatrix to be the set of allowed limits of linear combinations.\n\nWe can similarly define infinite versions of row-wise quasimatrices.\n\n\n\n4.1.5 Bases\nA basis set for a finite-dimensional vector space \\(\\mathcal{V}\\) is a finite set \\(S \\subset \\mathcal{V}\\) that spans \\(\\mathcal{V}\\) and whose elements are linearly independent — that is, no nonzero linear combination of entries of \\(S\\) is equal to zero. We can equivalently define a basis quasimatrix \\(U\\) such that \\(\\mathcal{R}(U) = \\mathcal{V}\\) and the mapping \\(c\n\\mapsto Uc\\) is one-to-one. That is, we think of a basis quasimatrix as an invertible linear mapping from a concrete vector space \\(\\mathbb{F}^n\\) to an abstract space \\(\\mathcal{V}\\). While we may refer to either the set or the quasimatrix as “a basis” when terminology is unambiguous, by default we use quasimatrices.\nThinking of dual spaces as consisting of rows, we generally represent the basis of \\(\\mathcal{V}^*\\) by a row quasimatrix \\(W^*\\). The map \\(d^* \\mapsto d^* W^*\\) is then an invertible linear map from the set of concrete row vectors wo \\(\\mathcal{V}^*\\). If \\(U\\) and \\(W^*\\) are bases of \\(\\mathcal{V}\\) and \\(\\mathcal{V}^*\\), the matrix \\(X = W^* U\\) represents the composition of two invertible maps (from \\(\\mathbb{F}^n\\) to \\(\\mathcal{V}\\) and back via the two bases), and so is invertible. The map \\(U^{-1} =\nX^{-1} W^*\\) is the dual basis associated with \\(U\\), which satisfies that \\(U^{-1} U\\) is the \\(n\\)-by-\\(n\\) identity matrix and \\(U\nU^{-1}\\) is the identity map on \\(\\mathcal{V}\\).\nIf \\(U\\) and \\(\\hat{U} = UX\\) are two bases for \\(\\mathcal{V}\\), a vector can be written as \\(v = Uc\\) or \\(v = \\hat{U} \\hat{c}\\) where \\(\\hat{c} =\nX^{-1} c\\). Because we use \\(X\\) to change from the \\(U\\) basis to the \\(\\hat{U}\\) basis and use \\(X^{-1}\\) to change from the \\(c\\) coefficients to the \\(\\hat{c}\\) coefficients, we say the coefficient vector is contravariant (it changes in contrast to the basis). In contrast, an abstract dual vector \\(w^* \\in \\mathcal{V}^*\\) can be written in terms of the \\(U\\) and \\(\\hat{U}\\) vectors with the concrete row vectors \\(d^* = w^*\nU\\) or \\(\\hat{d}^* = w^* \\hat{U} = d^* X\\). Because the transformations from \\(U\\) to \\(\\hat{U}\\) and from \\(d^*\\) to \\(\\hat{d}^*\\) both involve the same operation with \\(X\\), we say these coefficient vector is covariant (it changes together with the basis).\n\n4.1.5.1 Component conventions\nWe will generally follow the convention common in many areas of numerical analysis: a vector \\(v \\in \\mathcal{V}\\) is associated with a column, a dual vector \\(w^* \\in \\mathcal{V}^*\\) is associated with a row, and writing a dual vector followed by a vector (\\(w^* v\\)) indicates evaluation. However, there are some shortcomings to this convention, such as situations where we want to apply a dual vector to a vector but some other notational convention prevents us from writing the two symbols adjacent to each other. This issue becomes particularly obvious in multilinear algebra (tensor algebra), as we will see in Section 4.5.\nAn alternate convention that addresses some of these issues is frequently used in areas of physics where tensors are particularly common. In this convention (“Einstein notation”), vectors with respect to a basis \\[\n  V = \\begin{bmatrix} \\mathbf{v}_1 & \\ldots & \\mathbf{v}_n \\end{bmatrix}\n\\] are written as \\[\n  \\mathbf{v} = \\mathbf{v}_i c^i\n\\] where the summation convention is in effect, i.e. there is an implicit summation over repeated indices. Dual vectors are written with respect to the basis \\[\n  V^{-1} = \\begin{bmatrix} \\mathbf{w}^1 \\\\ \\vdots \\\\ \\mathbf{w}^n \\end{bmatrix}\n\\] as \\[\n  \\mathbf{w}^* = d_i \\mathbf{w}^i.\n\\] Therefore \\[\n  \\mathbf{w}^* \\mathbf{v} = d_i c^i.\n\\] Note that ordering of the symbols in the product makes no difference in this notation: \\(d_i c^i\\) and \\(c^i d_i\\) indicate the same scalar value.\nThe convention of using subscripts to denote vector coefficients (contravariant coefficients) and superscripts to denote dual coefficients (covariant coefficients) gives us a mechanism for “type checking” expressions: summation over a repeated index should involve one superscript (a covariant index) and one subscript (a contravariant index). At the same time, many authors who follow the summation convention only use subscript indices in order to save the superscript space for other purposes. Caveat lector!4\nIndices are useful for matching functions (dual vectors) and arguments (vectors) without explicit reference to position in an expression. At the same time, indicial notation presumes a basis, with particularly basis-centric authors sometimes writing statements like “a vector is an array that transforms like a vector.” While we usually use bases in our computation, we prefer a basis-independent perspective for mathematical arguments when possible. With this said, we will revert to indicial notation when the situation merits it.\n\n\n4.1.5.2 Polynomial bases\nAs an example of different choices of bases, we consider the polynomial spaces \\(\\mathcal{P}_d\\). A common choice of basis for \\(\\mathcal{P}_d\\) is the power basis \\[\n  X_{0:d} = \\begin{bmatrix} 1 & x & x^2 & \\ldots & x^d \\end{bmatrix}.\n\\] That is, we write polynomials \\(p \\in \\mathcal{P}_d\\) in terms of the basis as \\[\n  p = X_{0:d} c = \\sum_{j=0}^d c_j x^j.\n\\] A common basis for \\(\\mathcal{P}_d^*\\) is a basis of point evaluation functionals, i.e. \\[\n  Y^* p = \\begin{bmatrix} p(x_0) \\\\ p(x_1) \\\\ \\vdots \\\\ p(x_d) \\end{bmatrix}\n\\] for distinct points \\(x_0, x_1, \\ldots, x_d\\). The matrix \\(Y^* X_{0:d}\\) is the Vandermonde matrix \\[\n  V = \\begin{bmatrix}\n  1 & x_0 & x_0^2 & \\ldots & x_0^d \\\\\n  1 & x_1 & x_1^2 & \\ldots & x_1^d \\\\\n  1 & x_2 & x_2^2 & \\ldots & x_2^d \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & x_d & x_d^2 & \\ldots & x_d^d\n  \\end{bmatrix}.\n\\] The basis \\(V^{-1} W^*\\) is the dual basis associated with the monomial basis. For example, for quartic polynomials with equally spaced polynomials on \\([-1, 1]\\), we can write\n\nlet\n    p = Polynomial([1.0; 2.0; 3.0; 4.0; 5.0])\n    X = range(-1.0, 1.0, length=5)\n    V = [xi^j for xi in X, j in 0:4]\n    c = V\\p.(X)\nend\n\n5-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n\n\nThat is, if \\(p(x) = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + c_4 x^4\\), and let \\(p_X\\) denote the vector of samples \\(p(-1), p(-0.5), p(0), p(0.5),\np(1)\\), then \\(c = V^{-1} p_X\\). This is, alas, not a particularly good numerical approach to recovering a polynomial from point values, a topic that we will return to in Chapter 10.\nThe power basis is not the only basis for \\(\\mathcal{P}_d\\). Other common choices include Newton or Lagrange polynomials with respect to a set of points. Along with Vandermonde matrices, we will discuss these topics in Chapter 10. We will sometimes use the (first kind) Chebyshev5 polynomial basis \\(T_{0:d}\\) with columns \\(T_0(x), \\ldots, T_d(x)\\) given by the recurrence \\[\\begin{aligned}\n  T_0(x) &= 1 \\\\\n  T_1(x) &= x \\\\\n  T_{j+1}(x) &= 2 x T_{j}(x) - T_{j-1}(x), \\quad j \\geq 1.\n\\end{aligned}\\] The Chebyshev polynomials can also be written on \\([-1, 1]\\) in terms of trigonometric functions: \\[\n  T_m(\\cos(\\theta)) = \\cos(m \\theta).\n\\] This connection between Chebyshev polynomials and trigonometric functions is part of their utility, and allows us to use transform methods for working with Chebyshev polynomials (Chapter 19).\nThe related second kind Chebyshev polynomial basis \\(U_{0:d}\\) satisfies \\[\\begin{aligned}\nU_0(x) &= 1 \\\\\nU_1(x) &= 2x \\\\\nU_{j+1}(x) &= 2x U_j(x) - U_{j-1}(x), \\quad j \\geq 1.\n\\end{aligned}\\] These polynomials satisfy \\[\n  U_m(\\cos(\\theta)) \\sin(\\theta) = \\sin(m \\theta).\n\\] These polynomials are useful in part because of the derivative relationship \\(T_m'(x) = m U_{m-1}(x)\\).\nIn matrix terms, we can write the relationship between the Chebyshev polynomials and the power basis as \\(T_{0:d} = X_{0:d} M\\) where the matrix \\(M\\) is computed via the three-term recurrence:\n\nfunction chebyshev_power_coeffs(d, kind=1)\n    n = max(d+1, 2)\n    M = zeros(n, n)\n    M[1,1] = 1                          # p0(x) = 1\n    M[2,2] = kind                       # p1(x) = kind*x\n    for j = 2:d                         # p_{j+1}(x) =\n        M[2:end,j+1] .= 2*M[1:end-1,j]  #   2x p_j(x)\n        M[:,j+1] .-= M[:,j-1]           #   -p_{j-1}(x)\n    end\n    UpperTriangular(M[1:d+1,1:d+1])\nend\n\nchebyshev_power_coeffs (generic function with 2 methods)\n\n\nFor example, for the quadratic case, we have\n\nchebyshev_power_coeffs(2)\n\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 1.0  0.0  -1.0\n  ⋅   1.0   0.0\n  ⋅    ⋅    2.0\n\n\nand the coefficient vectors \\(d\\) and \\(c\\) with respect to the Chebyshev and power bases are such that \\(p = T_{0:d} d = X_{0:d} Md = X_{0:d} c\\), so \\(Md = c\\). For example, to write the polynomial \\(p(x) = 2 - 3x + x^2\\) in the Chebyshev basis, we solve the triangular linear system:\n\nchebyshev_power_coeffs(2) \\ [2.0; -3.0; 1.0]\n\n3-element Vector{Float64}:\n  2.5\n -3.0\n  0.5\n\n\nIf we would like to form a representation of the Chebyshev basis, we could use the power basis expansion\n\nfunction chebyshev_basis(d, kind=1)\n    M = chebyshev_power_coeffs(d, kind)\n    transpose([Polynomial(M[1:j,j]) for j=1:d+1])\nend\n\nchebyshev_basis(2)           \n\n1×3 transpose(::Vector{Polynomial{Float64, :x}}) with eltype Polynomial{Float64, :x}:\n Polynomial(1.0)  Polynomial(1.0*x)  Polynomial(-1.0 + 2.0*x^2)\n\n\nHowever, the Chebyshev polynomials are natively supported in the Polynomials.jl package, and we can explicitly write polynomials in terms of Chebyshev polynomials. For example,\n\nChebyshevT([2.5, -3.0, 0.5])\n\n2.5⋅T_0(x) - 3.0⋅T_1(x) + 0.5⋅T_2(x)\n\n\n\nconvert(Polynomial, ChebyshevT([2.5, -3.0, 0.5]))\n\n2.0 - 3.0∙x + 1.0∙x2\n\n\nThe coefficients satisfy exactly the equations from above.\nThe bases \\(T_{0:d}\\) and \\(U_{0:d}\\) are also both bases for \\(\\mathcal{P}_d\\), so there must be a change of basis matrix that converts between the two. We can look up or derive that \\[\n  T_n(x) = \\frac{1}{2} \\left( U_n(x) - U_{n-2}(x) \\right),\n\\] or, in terms of a matrix representation of the change of basis, \\[\n  T_{0:d} = U_{0:d} B\n\\] where \\(B\\) is the upper triangular matrix \\[\n  B =\n  \\begin{bmatrix}\n  1 &             & -\\frac{1}{2} \\\\\n    & \\frac{1}{2} &              & -\\frac{1}{2} \\\\\n    &             & \\frac{1}{2}  &              & \\ddots \\\\\n    &             &              & \\frac{1}{2}  & \\ddots & -\\frac{1}{2} \\\\\n    &             &              &              & \\ddots & \\\\\n    &             &              &              &        & \\frac{1}{2}\n  \\end{bmatrix}.\n\\] Alternately, in code we have\n\nfunction change_U_to_T(d)\n    B = Matrix(0.5*I, d+1, d+1)\n    B[1,1] = 1.0\n    for j = 3:d+1\n        B[j-2,j] = -0.5\n    end\n    B\nend\n\nchange_U_to_T (generic function with 1 method)\n\n\nThat is, we should have\n\nchebyshev_basis(5) ==\n    chebyshev_basis(5,2) * change_U_to_T(5)\n\ntrue\n\n\nIn addition to the Chebyshev polynomials, we will sometimes want the Legendre polynomial basis \\(P_{0:d}(x)\\) with columns \\(P_j(x)\\) given by the recurrence \\[\\begin{aligned}\n  P_0(x) &= 1 \\\\\n  P_1(x) &= x \\\\\n  (j+1) P_{j+1}(x) &= (2j+1) x P_j(x) - j P_{j-1}(x).\n\\end{aligned}\\] As with the Chebyshev polynomials, we can define an upper triangular matrix whose columns are coefficients for the Legendre polynomials in the power basis:\n\nfunction legendre_power_coeffs(d)\n    n = max(d+1, 2)\n    M = zeros(n, n)\n    M[1,1] = 1\n    M[2,2] = 1\n    for j = 2:d\n        M[2:end,j+1] .= (2*j-1)*M[1:end-1,j]\n        M[:,j+1] .-= (j-1)*M[:,j-1]\n        M[:,j+1] ./= j\n    end\n    UpperTriangular(M[1:d+1,1:d+1])\nend\n\nlegendre_power_coeffs (generic function with 1 method)\n\n\nIf we want a representation in terms of a list of Polynomial objects (representing the basis polynomials), we can simply convert the columns:\n\nfunction legendre_basis(d)\n    M = legendre_power_coeffs(d)\n    transpose([Polynomial(M[1:j,j]) for j=1:d+1])\nend\n\nlegendre_basis(2)           \n\n1×3 transpose(::Vector{Polynomial{Float64, :x}}) with eltype Polynomial{Float64, :x}:\n Polynomial(1.0)  Polynomial(1.0*x)  Polynomial(-0.5 + 1.5*x^2)\n\n\nEach basis we have discussed has a different use case. Sometimes the “obvious” choice of basis (e.g. the standard basis in \\(\\mathbb{R}^n\\) or the power basis in \\(\\mathcal{P}_d\\)) is not the best choice for numerical computations.\n\n\n4.1.5.3 Block bases\nConsider the direct sum \\[\n  \\mathcal{V}= \\mathcal{U}_1 \\oplus \\mathcal{U}_2 \\oplus \\ldots \\oplus \\mathcal{U}_p.\n\\] Given basis quasimatrices \\(U_j\\) for the spaces \\(\\mathcal{U}_j\\), there is an associated basis quasimatrix \\(V\\) for \\(\\mathcal{V}\\) defined by horizontal concatenation: \\[\n  V = \\begin{bmatrix} U_1 & U_2 & \\ldots & U_p \\end{bmatrix}.\n\\] Conversely, we can think of partitioning the columns of a basis \\(V\\) for \\(\\mathcal{V}\\) into disjoint subsets; then each subset is itself a basis for some subspace, and \\(\\mathcal{V}\\) is a direct sum of those subspaces. This sort of “block thinking” will generally be useful in our treatment of numerical linear algebra.\n\n\n4.1.5.4 Infinite bases\nThe notion of a basis can be generalized to the case of an infinite-dimensional space \\(\\mathcal{V}\\) in two ways. The algebraists version (the Hamel basis) is a set of linearly independent vectors such that any vector is a finite linear combination of basis vectors. Such a basis always exists if we assume the axiom of choice, but is not particularly useful for our purposes. More useful for topics such as approximation theory and probability is the analyst’s notion of a basis: an infinite quasimatrix that represents a map between some sequence space (e.g. \\(\\ell^1\\), \\(\\ell^2\\), or \\(\\ell^\\infty\\)) and an abstract vector space.\n\n\n\n4.1.6 Vector norms\nA norm \\(\\|\\cdot\\|\\) measures vector lengths. It is positive definite, (absolutely) homogeneous, and sub-additive: \\[\\begin{aligned}\n  \\|v\\| & \\geq 0 \\mbox{ and } \\|v\\| = 0 \\mbox{ iff } v = 0 \\\\\n  \\|\\alpha v\\| &= |\\alpha| \\|v\\| \\\\\n  \\|u+v\\| & \\leq \\|u\\| + \\|v\\|.\n\\end{aligned}\\] The three most common vector norms we work with in \\(\\mathbb{R}^n\\) are the Euclidean norm (aka the 2-norm), the \\(\\infty\\)-norm (or max norm), and the \\(1\\)-norm: \\[\\begin{aligned}\n  \\|v\\|_2 &= \\sqrt{\\sum_j |v_j|^2} \\\\\n  \\|v\\|_\\infty &= \\max_j |v_j| \\\\\n  \\|v\\|_1 &= \\sum_j |v_j|\n\\end{aligned}\\] These are all available via Julia’s norm function:\n\nlet\n    x = [3.0; 4.0]\n    norm(x),      # Euclidean norm (also written norm(x,2))\n    norm(x,Inf),  # Max norm\n    norm(x,1)     # 1-norm\nend\n\n(5.0, 4.0, 7.0)\n\n\nMany other norms can be related to one of these three norms. In particular, a “natural” norm in an abstract vector space will often look strange in the corresponding concrete representation with respect to some basis function. For example, consider the vector space of polynomials with degree at most 2 on \\([-1,1]\\). This space also has a natural Euclidean norm, max norm, and \\(1\\)-norm; for a given polynomial \\(p(x)\\) these are \\[\\begin{aligned}\n  \\|p\\|_2 &= \\sqrt{ \\int_{-1}^1 |p(x)|^2 \\, dx } \\\\\n  \\|p\\|_\\infty &= \\max_{x \\in [-1,1]} |p(x)| \\\\\n  \\|p\\|_1 &= \\int_{-1}^1 |p(x)| \\, dx.\n\\end{aligned}\\] These norms are not built into the Polynomials package, but they are not difficult to compute for real polynomials:\n\nreal_roots(p) = sort(real(filter(isreal, roots(p))))\nreal_roots(p, a, b) = filter(x -&gt; (a &lt;= x &lt;= b),\n                             real_roots(p))\n\nnormL2(p) = sqrt(integrate(p*p, -1, 1))\n\nfunction normL∞(p)\n    nodes = [-1.0; real_roots(derivative(p), -1, 1); 1.0]\n    maximum(abs.(p.(nodes)))\nend\n\nfunction normL1(p)\n    nodes = [-1.0; real_roots(p, -1, 1); 1.0]\n    sum(abs(integrate(p, nodes[j], nodes[j+1]))\n        for j = 1:length(nodes)-1)\nend\n\nlet\n    p = Polynomial([-0.5, 0.0, 1.0])\n    normL2(p),\n    normL∞(p),\n    normL1(p)\nend\n\n(0.483045891539648, 0.5, 0.60947570824873)\n\n\nThe same norms can also be approximated from samples using the midpoint rule (?sec-quad-diff-ch):\n\nlet\n    n = 100\n    p = Polynomial([-0.5, 0.0, 1.0])\n    X = range(-1.0, 1.0, length=n+1)\n    X = (X[1:end-1] + X[2:end])/2\n    pX = p.(X)\n    norm(pX)*sqrt(2/n),\n    norm(pX, Inf),\n    norm(pX, 1)*2/n\nend\n\n(0.48297688971626773, 0.4999, 0.6093599999999999)\n\n\nOf course, when we write \\(p(x)\\) in terms of the coefficient vector with respect to the power basis (for example), the max norm of the polynomial is not the same as the max norm of the coefficient vector.\nIn a finite-dimensional vector space, all norms are equivalent: that is, if \\(\\|\\cdot\\|\\) and \\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert \\cdot\n    \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}\\) are two norms on the same finite-dimensional vector space, then there exist constants \\(c\\) and \\(C\\) such that for any \\(v\\) in the space, \\[\n  c \\|v\\| \\leq {\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert v\n    \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert} \\leq C \\|v\\|.\n\\] Of course, there is no guarantee that the constants are small! In infinite-dimensional spaces, not all norms are equivalent.\nA normed vector space is a metric space, and so we can use it to define things like Cauchy sequences and limits. In a finite-dimensional normed space, any Cauchy sequence \\(\\{v_k\\}_{k=1^\\infty}\\) in a finite-dimensional vector space \\(\\mathcal{V}\\) converges to a limit \\(v_* \\in \\mathcal{V}\\). A normed infinite-dimensional vector space where Cauchy sequences converge (i.e. a space that is complete under the norm) is called a Banach space.\nFor any Banach space \\(\\mathcal{V}\\), we can define the dual norm for continuous functionals \\(w^*\\) in the dual space \\(\\mathcal{V}^*\\) by \\[\n  \\|w^*\\| = \\sup_{v \\neq 0} \\frac{w^* v}{\\|v\\|} = \\sup_{\\|v\\| = 1} w^* v.\n\\] In the finite-dimensional case, the supremum is always achieved, i.e. for any \\(w^*\\) there exists a \\(v\\) of unit norm (\\(\\|v\\|=1\\)) such that \\(w^* v =\n\\|w^*\\|\\). Conversely, for any \\(v\\) there exists a \\(w^*\\) of unit norm such that \\(w^* v = \\|v\\|\\). The \\(\\infty\\)-norm and the \\(1\\)-norm are dual norms of each other, and the Euclidean norm is its own dual.\n\n\n4.1.7 Inner products\nAn inner product \\(\\langle \\cdot, \\cdot \\rangle\\) is a function from two vectors into the real numbers (or complex numbers for an complex vector space). It is positive definite, linear in the first slot, and symmetric (or Hermitian in the case of complex vectors); that is: \\[\\begin{aligned}\n  \\langle v, v \\rangle & \\geq 0 \\mbox{ and }\n  \\langle v, v \\rangle = 0 \\mbox{ iff } v = 0 \\\\\n  \\langle \\alpha u, w \\rangle &= \\alpha \\langle u, w \\rangle\n  \\mbox{ and }\n  \\langle u+v, w \\rangle = \\langle u, w \\rangle + \\langle v, w \\rangle \\\\\n  \\langle u, v \\rangle &= \\overline{\\langle v, u \\rangle},\n\\end{aligned}\\] where the overbar in the latter case corresponds to complex conjugation. The standard inner product on \\(\\mathbb{C}^n\\) is \\[\n  x \\cdot y = y^* x = \\sum_{j=1}^n \\bar{y}_j x_j.\n\\] A vector space with an inner product is sometimes called an inner product space or a Euclidean space.\n\n4.1.7.1 The Riesz map\nThe inner product defines an (anti)linear map (the Riesz map) from vectors \\(v\\) to dual vectors \\(v^*\\) via \\(v^* u = \\langle u, v \\rangle\\). In terms of the Julia code in this chapter,\n\nriesz(dot) = v -&gt; DualVector(u -&gt; dot(u, v))\n\nAccording to the Riesz representation theorem, in finite-dimensional spaces, this defines a 1-1 correspondence between vectors and dual vectors. For \\(\\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\) under the standard inner product, the Riesz map is simply the (conjugate) transpose: \\[\n  \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\mapsto\n  \\begin{bmatrix} \\bar{c}_1 & \\bar{c}_2 & \\ldots & \\bar{c}_n \\end{bmatrix}.\n\\] Going beyond single vectors at a time, we can think of the Riesz map as inducing a mapping from colum quasimatrices to row quasimatrices.\nIn infinite-dimensional inner product spaces, we have a 1-1 correspondence between vectors and continuous dual vectors (which is all dual vectors for a complete inner product space). A (not-necessarily finite-dimensional) Euclidean space that is complete under the Euclidean norm is called a Hilbert space.\n\n\n4.1.7.2 The Gram matrix\nSuppose \\(\\mathcal{V}\\) is an arbitrary space with an inner product and \\(V\\) is a basis for that space. Then for vectors expressed in that basis, the inner product is \\[\\begin{aligned}\n  \\langle Vc, Vd \\rangle\n  &= \\left\\langle \\sum_j v_j c_j, \\sum_i v_i d_i \\right\\rangle \\\\\n  &= \\sum_{i,j} \\bar{d}_i c_j \\langle v_j, v_i \\rangle \\\\\n  &= d^* M c = \\langle c, d \\rangle_M,\n\\end{aligned}\\] where \\(M = V^* V\\) is the Gram matrix whose entries are inner products of basis elements. In Julia code, we have\n\ngram(dot, V) = Symmetric([dot(vj, vi) for vi in V[:], vj in V[:]])\n\nThe Gram matrix is Hermitian and (because \\(V\\) is a basis) positive definite. We call the expression \\(d^* M c\\) the \\(M\\)-inner product between the concrete vectors \\(c\\) and \\(d\\).\nWe can also write the Riesz map in terms of \\(V\\) and \\(M\\). Given \\(w^* \\in \\mathcal{V}^*\\), the Riesz map is \\(w = V M^{-1} (w^* V)^*\\); we can verify this based on the expression of the \\(\\mathcal{V}\\)-inner product as an \\(M\\)-inner product over coefficient vectors: \\[\n  \\langle u, w \\rangle =\n  \\langle V^{-1} u, M^{-1} (w^* V)^* \\rangle =\n  w^* V M^{-*} M V^{-1} u = w^* u.\n\\]\n\n\n4.1.7.3 Euclidean norms\nEvery inner product defines a corresponding norm \\[\n  \\|v\\| = \\sqrt{ \\langle v, v \\rangle}.\n\\] The inner product can also be recovered from the norm. In general, we can expand \\[\\begin{aligned}\n  \\|u+v\\|^2\n  &= \\langle u + v, u + v \\rangle \\\\\n  &= \\langle u, u \\rangle + \\langle v, u \\rangle + \\langle u, v \\rangle\n     + \\langle v, v \\rangle \\\\\n  &= \\|u\\|^2 + 2 \\Re \\langle u, v \\rangle + \\|v\\|^2\n\\end{aligned}\\] Therefore \\[\\begin{aligned}\n  \\Re \\langle u, v \\rangle\n  &= \\frac{1}{2} \\left( \\|u+v\\|^2 - \\|u\\|^2 - \\|v\\|^2 \\right) \\\\\n  \\Im \\langle u, v \\rangle = \\Re \\langle u, iv \\rangle\n  &= \\frac{1}{2} \\left( \\|u+iv\\|^2 - \\|u\\|^2 - \\|v\\|^2 \\right).\n\\end{aligned}\\] Of course, for real vector spaces only the first equation is needed.\n\n\n4.1.7.4 Angles and orthogonality\nThe inner product and the associated norm satisfy the Cauchy-Schwarz inequality \\[\n  |\\langle u, v \\rangle| \\leq \\|u\\| \\|v\\|.\n\\] We define the cosine of the angle between nonzero real vectors \\(u\\) and \\(v\\) to be \\[\n  \\cos \\theta = \\frac{\\langle v, w \\rangle}{\\|v\\| \\|w\\|}.\n\\] Because of the Cauchy-Schwarz inequality, the cosine is always at most one in absolute value. For a real inner product space, the expansion of \\(\\|u+v\\|^2\\) and the definition of the cosine gives us the law of cosines: for any nonzero \\(u\\) and \\(v\\), \\[\n  \\|u+v\\|^2 = \\|u\\|^2 + 2 \\|u\\| \\|v\\| \\cos(\\theta) + \\|v\\|^2.\n\\] We say two vectors \\(u\\) and \\(v\\) are orthogonal with respect to an inner product if \\(\\langle u, v \\rangle = 0\\). If \\(u\\) and \\(v\\) are orthogonal, we have the Pythagorean theorem: \\[\n  \\|u+v\\|^2 = \\|u\\|^2 + \\|v\\|^2.\n\\]\nIf \\(\\mathcal{U}\\) is any subspace of an inner product space \\(\\mathcal{V}\\), we can define the orthogonal complement \\(\\mathcal{U}^\\perp\\): \\[\n  \\mathcal{U}^\\perp =\n  \\{ v \\in \\mathcal{V}: \\forall u \\in \\mathcal{U}, \\langle u, v \\rangle = 0 \\}.\n\\] We can always write \\(\\mathcal{V}\\) as the direct sum \\(\\mathcal{U}\\oplus \\mathcal{U}^\\perp\\). In other words, the orthogonal complement is isomorphic to the quotient space \\(\\mathcal{V}/ \\mathcal{U}\\), and in inner product spaces these two are often identified with each other.\nVectors that are mutually orthogonal and have unit length in the Euclidean norm are said to be orthonormal. A basis \\(V\\) for an \\(n\\)-dimensional space \\(\\mathcal{V}\\) is an orthonormal basis if all its columns are orthonormal. In this case, \\(V^* V\\) is the \\(n\\)-by-\\(n\\) identity matrix and \\(V V^*\\) is the identity map on \\(\\mathcal{V}\\). That is, for an orthonormal basis quasimatrix, the Riesz map and the dual basis are the same, i.e. \\(U^* = U^{-1}\\).\n\n\n4.1.7.5 Einstein notation\nFor physicists working with Einstein notation on real vector spaces, the Gram matrix (aka the metric tensor) is sometimes written in terms of the components \\(g_{ij} = \\langle \\mathbf{v}_j, \\mathbf{v}_i \\rangle\\); the inverse of the Gram matrix is written with components \\(g^{ij}\\). In this convention, the Riesz map from \\(\\mathcal{V}^*\\) to \\(\\mathcal{V}\\) is associated with using the metric tensor to “raise an index”: \\[\n  v^i = g^{ij} w_j.\n\\] Conversely, the inverse Riesz map from \\(\\mathcal{V}\\) to \\(\\mathcal{V}^*\\) is associated with using the metric tensor to “lower an index”: \\[\n  w_i = g_{ij} v^j.\n\\] And the Euclidean norm is written as \\[\n  \\|\\mathbf{v}\\|^2 = g_{ij} v^i v^j\n\\] When an orthonormal basis is used, the Gram matrix is just the identity, and raising or lowering indices appears to be a purely formal matter.\n\n\n4.1.7.6 The \\(L^2\\) inner product\nReturning to our example of a vector space of polynomials, the standard \\(L^2([-1,1])\\) inner product is \\[\n  \\langle p, q \\rangle_{L^2([-1,1])} = \\int_{-1}^1 p(x) \\bar{q}(x) \\, dx.\n\\] In Julia code, we can write this as\n\ndotL2(p, q) = integrate(p*conj(q), -1, 1)\n\nThe Gram matrix for this inner product with the power basis (using zero-based indexing) is \\[\n  a_{ij} = \\int_{-1}^1 x^{i-1} x^{j-1} \\, dx =\n  \\begin{cases}\n   2/(i+j+1), & i+j \\mbox{ even} \\\\\n   0, & \\mbox{ otherwise}\n  \\end{cases}\n\\] which we can implement in Julia as\n\ngram_L2_power(d) = [(i+j)%2 == 0 ? 2.0/(i+j+1) : 0.0\n                    for i=0:d, j=0:d]\n\nFor example, for quadratics we have\n\ngram_L2_power(2)\n\n3×3 Matrix{Float64}:\n 2.0       0.0       0.666667\n 0.0       0.666667  0.0\n 0.666667  0.0       0.4\n\n\nHence we can compute \\(\\langle 1 + x + x^2, 2 -3x + x^2 \\rangle\\) either via\n\nlet\n    p = Polynomial([1.0; 1.0; 1.0])\n    q = Polynomial([2.0; -3.0; 1.0])\n    dotL2(p, q)\nend\n\n4.4\n\n\nor via\n\n[1.0; 1.0; 1.0]'*gram_L2_power(2)*[2.0; -3.0; 1.0]\n\n4.3999999999999995\n\n\nFor the Chebyshev basis, the Gram matrix has entries \\[\n  \\int_{-1}^1 T_{i}(x) T_j(x) \\, dx.\n\\] This can also be computed using the relations \\[\\begin{aligned}\nT_m(x) T_n(x) &= \\frac{1}{2} \\left( T_{m+n}(x) + T_{|m-n|}(x) \\right) \\\\\n\\int_{-1}^1 T_n(x) \\, dx &=\n\\begin{cases}\n  \\frac{2}{1-n^2}, & n \\mbox{ even} \\\\\n  0, & n \\mbox{ odd}.\n\\end{cases}\n\\end{aligned}\\]\n\nfunction gram_L2_chebyshev(d)\n    tint(j) = j%2 == 0 ? 2.0/(1-j^2) : 0.0\n    m(i,j) = (tint(i+j) + tint(abs(i-j)))/2.0\n    [m(i,j) for i=0:d, j=0:d]\nend\n\ngram_L2_chebyshev (generic function with 1 method)\n\n\nFor example, the Gram matrix for \\(T_{0:4}\\) is\n\ngram_L2_chebyshev(4)\n\n5×5 Matrix{Float64}:\n  2.0        0.0       -0.666667   0.0       -0.133333\n  0.0        0.666667   0.0       -0.4        0.0\n -0.666667   0.0        0.933333   0.0       -0.361905\n  0.0       -0.4        0.0        0.971429   0.0\n -0.133333   0.0       -0.361905   0.0        0.984127\n\n\nWe can sanity check our integration by comparing to another routine:\n\ngram_L2_chebyshev(4) ≈ gram(dotL2, chebyshev_basis(4)[:])\n\ntrue\n\n\nFor the Legendre basis, the Gram matrix is\n\ngram(dotL2, legendre_basis(4)[:])\n\n5×5 Symmetric{Float64, Matrix{Float64}}:\n 2.0  0.0       0.0  0.0       0.0\n 0.0  0.666667  0.0  0.0       0.0\n 0.0  0.0       0.4  0.0       0.0\n 0.0  0.0       0.0  0.285714  0.0\n 0.0  0.0       0.0  0.0       0.222222\n\n\nThat is, the Gram matrix for the Legendre basis is diagonal, with diagonal entries \\(2/1, 2/3, 2/5, 2/7, 2/9, \\ldots\\). In other words, the Legendre basis vectors are orthogonal with respect to the \\(L^2([-1,1])\\) inner product. The normalized Legendre polynomials are \\[\n  \\sqrt{\\frac{2j+1}{2}} P_j,\n\\] or, in code\n\nnormalized_legendre_basis(d) =\n    transpose([sqrt((2j+1)/2)*Pj\n               for (Pj,j) = zip(legendre_basis(d), 0:d)])\n\nnormalized_legendre_basis (generic function with 1 method)\n\n\nand these form an orthonormal basis for the \\(\\mathcal{P}_d\\) spaces under the \\(L^2([-1,1])\\) inner product. Hence, the Gram matrix for a normalized Legendre basis (using the \\(L^2([-1,1])\\) inner product) is just the identity:\n\nlet\n    P̄ = normalized_legendre_basis(3)\n    gram(dotL2, P̄)\nend\n\n4×4 Symmetric{Float64, Matrix{Float64}}:\n 1.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0\n 0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  1.0\n\n\nUsing an orthonormal basis \\(U\\) makes it straightforward to evaluate the Riesz map. For a given functional \\(w^* \\in \\mathcal{V}^*\\) the associated \\(w \\in \\mathcal{V}\\) under the Riesz map is \\[\n  w = U (w^* U)^*,\n\\] where \\((w^* U)^*\\) represents the ordinary conjugate transpose of the concrete row vector \\(w^* U\\). Then taking the inner produce with \\(w\\) is equivalent to evaluating \\(w^*\\), since \\[\n  \\langle Uc, w \\rangle =\n  \\langle Uc, U (w^* U)^* \\rangle =\n  (w^* U) c = w^* (Uc).\n\\] In the case of the normalized Legendre polynomials, for example, this gives us a way of computing a vector \\(\\delta_x\\) associated with the evaluation functional \\(\\delta_x^*\\) such that \\(\\delta_x^* p =\np(x)\\):\n\nfunction dualL2_δx(x, d)\n    P̄ = normalized_legendre_basis(d)\n    c = [P̄j(x) for P̄j in P̄]'\n    P̄*c[:]\nend\n\ndualL2_δx (generic function with 1 method)\n\n\nFor example, evaluating \\(2 -3x + x^2\\) at \\(x = 0.5\\) gives \\(p(0.5) =\n\\langle p, \\delta_{0.5} \\rangle = 0.75\\); in code, we have\n\nlet\n    p = Polynomial([2.0; -3.0; 1.0])\n    δhalf = dualL2_δx(0.5, 2)\n    p(0.5) ≈ dotL2(p, δhalf)\nend\n\ntrue",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "00-Background/03-LA.html#maps-and-forms",
    "href": "00-Background/03-LA.html#maps-and-forms",
    "title": "4  Linear Algebra",
    "section": "4.2 Maps and forms",
    "text": "4.2 Maps and forms\nA central object in linear algebra is a linear map between two spaces. If \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\) are two vector spaces, we denote the vector space of linear maps6 between them by \\(L(\\mathcal{V}, \\mathcal{U})\\).\nClosely associated with linear maps between two vector spaces are certain maps from two vector spaces. A bilinear form is a map \\(a : \\mathcal{U}\\times \\mathcal{V}\\rightarrow \\mathbb{F}\\) which is linear in both arguments. Such a form can be identified with a linear map from \\(\\mathcal{V}\\) to \\(\\mathcal{U}^*\\) by partial evaluation: \\(v \\mapsto a(\\cdot, v)\\). A sesquilinear form is linear in one argument and antilinear (conjugate linear) in the second argument, and can be identified with an antilinear map from \\(\\mathcal{V}\\) to \\(\\mathcal{U}^*\\). We are generally interested in bilinear forms over real spaces and sesquilinear forms over complex spaces. When we want to talk about real bilinear forms and complex sesquilinear forms at the same time, we will usually just say “sesquilinear forms.”\nOn inner product spaces, each sesquilinear form \\(a : \\mathcal{V}\\times \\mathcal{W}\n\\rightarrow \\mathbb{F}\\) is associated with a unique linear map \\(L \\in\nL(\\mathcal{V}, \\mathcal{W})\\) such that \\[\n  a(v, w) = \\langle Lv, w \\rangle_{\\mathcal{U}}.\n\\] That is, sesquilinear forms can be identified with linear maps and vice-versa, and so we will address them together.\nA linear map from a space to itself is called a linear operator. Linear operators have enough additional structure that we will deal with them separately.\n\n4.2.1 Dual and adjoint maps\nAny linear map \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) comes with an associated dual map in \\(L(\\mathcal{U}^*, \\mathcal{V}^*)\\) with the action \\(u^* \\mapsto u^* L\\). If both spaces have inner products, then we compose the dual map with the Riesz map on both sides to get the adjoint \\(L^* \\in\nL(\\mathcal{U}, \\mathcal{V})\\) with the action \\(u \\mapsto (u^* L)^*\\). Equivalently, the adjoint map has the property that \\[\n  \\langle Lv, u \\rangle_{\\mathcal{U}} = \\langle v, L^* u \\rangle_{\\mathcal{V}}.\n\\] In concrete spaces with the standard inner product, the adjoint map is represented by the conjugate transpose of the matrix of the original map.\n\n\n4.2.2 Matrices\n\n4.2.2.1 Matrices for maps\nThe usual representation of a linear map \\(L \\in L(\\mathbb{F}^n, \\mathbb{F}^m)\\) is via an \\(m\\)-by-\\(n\\) matrix \\(A \\in \\mathbb{F}^{m \\times n}\\). Applying the function \\(y = Lx\\) is equivalent to the matrix multiplication \\(y = Ax\\): \\[\n  y_i = \\sum_{j=1}^n a_{ij} x_j.\n\\]\nMore generally, if \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) and \\(V\\) and \\(U\\) are bases for \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\), then there is an associated matrix representation \\[\n  A = U^{-1} L V.\n\\] The action of the matrix \\(A\\) comes from converting from \\(\\mathbb{F}^n\\) to \\(\\mathcal{V}\\) (via \\(V\\)), applying \\(L\\) to get a vector in \\(\\mathcal{U}\\), and mapping from \\(\\mathcal{U}\\) back to the concrete space \\(\\mathbb{F}^m\\). Equivalently, we can write the abstract operator as \\[\n  L = U A V^{-1},\n\\] i.e. we map from \\(\\mathcal{V}\\) to a concrete vector in \\(\\mathbb{F}^n\\), map that to a concrete vector in \\(\\mathbb{F}^m\\) by matrix multiplication with \\(A\\), and map \\(\\mathbb{F}^m\\) to \\(\\mathcal{U}\\) via the \\(U\\) basis.\nIf we consider the case where \\(V'\\) and \\(U'\\) are alternate bases for \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\), there is an associated matrix representation \\[\n  A' = U'^{-1} L V' = (U'^{-1} U) A (V^{-1} V')\n\\] Here the (invertible) matrix \\(V^{-1} V'\\) represents a change of basis: if \\(v = V'c\\) is a representation in the \\(V'\\) basis and \\(v = Vd\\) is a representation in the \\(V\\) basis, then \\(d = V^{-1} V' c\\). Similarly, the matrix \\(U'^{-1} U\\) converts the coefficients in the \\(U\\) basis to coefficients in the \\(U'\\) basis.\n\n\n4.2.2.2 Matrices for sesquilinear forms\nSuppose \\(a : \\mathcal{V}\\times \\mathcal{W}\\rightarrow \\mathbb{F}\\) is a sesquilinear form and \\(V\\) and \\(W\\) are bases for \\(\\mathcal{V}\\) and \\(\\mathcal{W}\\). Then \\[\n  a(Vx, Wy) = \\sum_{j,i} x_j \\bar y_i a(v_j, w_i) = y^* A x\n\\] where \\(A\\) is the matrix with entries \\(a_{ij} = a(v_j, w_i)\\). In an inner product space where we can write \\(a\\) in terms of a linear operator \\(L\\), we have \\[\n  a(Vx, Wy) = \\langle L Vx, Wy \\rangle_{\\mathcal{W}}\n            = y^* W^* L V x,\n\\] i.e. we can also think of \\(A\\) as \\(W^* L V\\).\nWe note that the matrix representation of the sesquilinear form is not identical to the matrix representation of \\(L\\) as a linear map, except in the special case where we constrain \\(V\\) and \\(W\\) to be orthonormal bases. For more general matrices, the expression \\(W^* L\nV\\) involves both the matrix for the linear map (\\(W^{-1} L V\\)) and the Gram matrix (\\(M = W^* W)\\); that is, we can also write \\[\n  y^* W^* L V x\n  = y^* (W^* W) W^{-1} L V x\n  = \\langle W^{-1} L V x, y \\rangle_M.\n\\]\n\n\n4.2.2.3 Representing derivatives\nAs a concrete example of matrix representations, we consider the derivative operator \\(D : \\mathcal{P}_d \\rightarrow \\mathcal{P}_{d-1}\\). With respect to the power basis for both \\(\\mathcal{P}_d\\) and \\(\\mathcal{P}_{d-1}\\), we write the relation \\[\n  \\frac{d}{dx} x^n = n x^{n-1}\n\\] as \\[\n  D X_{0:d} = X_{0:d-1} A\n\\] where \\[\n  A = \\begin{bmatrix}\n  0 & 1 \\\\\n    & & 2 \\\\\n    & & & 3 \\\\\n    & & & & \\ddots \\\\\n    & & & & & d\n  \\end{bmatrix}.\n\\] In code, we have\n\nfunction derivative_power(d)\n    A = zeros(d,d+1)\n    for i=1:d\n        A[i,i+1] = i\n    end\n    A\nend\n\nderivative_power (generic function with 1 method)\n\n\nAlternately, we can write \\(D = X_{0:d-1} A X_{0:d}^{-1}\\) or \\(A = X_{0:d-1}^{-1} D X_{0:d}\\).\nA similar relationship holds for the first and second kind Chebyshev polynomials: \\[\n  \\frac{d}{dx} T_n(x) = n U_{n-1}(x),\n\\] and so if we use the first-kind polynomial basis \\(T_{0:d}\\) for \\(\\mathcal{P}_d\\) and the second-kind polynomial basis \\(U_{0:d-1}\\) for \\(\\mathcal{P}_{d-1}\\), we have \\[\n  D T_{0:d} = U_{0:d-1} A\n\\] where \\(A\\) is the same matrix as before. Using the change of basis, we have \\[\n  D T_{0:d} = T_{0:d-1} B^{-1} A.\n\\] In code, the matrix representation \\(T_{0:d-1}^{-1} D T_{0:d} = B^{-1} A\\) can be computed by\n\nderivative_chebyshevT(d) =\n    change_U_to_T(d-1)\\derivative_power(d)\n\nderivative_chebyshevT (generic function with 1 method)\n\n\nIt is always useful to compare against another implementation of the same computation, e.g.\n\nlet\n    Dmatrix = zeros(5,6)\n    DT = derivative.(chebyshev_basis(5))\n    for j = 1:length(DT)\n        for ci = collect(pairs(convert(ChebyshevT, DT[j])))\n            Dmatrix[ci[1]+1,j] = ci[2]\n        end\n    end\n    Dmatrix ≈ derivative_chebyshevT(5)\nend\n\ntrue\n\n\nNow consider a sesquilinear form on \\(\\mathcal{P}_d \\times \\mathcal{P}_{d-1}\\) given by \\[\n  a(u, w) = \\int_{-1}^1 \\bar{w}(x) u'(x) \\, dx.\n\\] If we use the Chebyshev basis for both arguments, we have \\[\\begin{aligned}\n  a(T_{0:d} x, T_{0:d-1} y)\n  &= y^* T_{0:d-1}^* D T_{0:d} x \\\\\n  &= y^* T_{0:d-1}^* T_{0:d-1} B^{-1} A x,\n\\end{aligned}\\] that is, the matrix is \\(T_{0:d-1}^* T_{0:d-1} B^{-1} A\\). In terms of codes we have written, we have (for \\(d = 5\\))\n\nderivative_chebyshevT_bilinear(d) =\n    gram_L2_chebyshev(d-1)*derivative_chebyshevT(d)\n\nderivative_chebyshevT_bilinear (generic function with 1 method)\n\n\nAs before, we can sanity check against an alternate computation\n\nlet\n    T = chebyshev_basis(5)\n    a(p, q) = dotL2(derivative(p), q)\n\n    derivative_chebyshevT_bilinear(5) ≈\n        [a(T[j+1], T[i+1]) for i=0:4, j=0:5]\nend\n\ntrue\n\n\nWith respect to the Legendre basis, one can differentiate the Bonnet recurrence relation for \\(P_n\\) and rearrange to get a recurrence relation for the derivatives: \\[\n  P_{n+1}'(x) = (2n+1) P_n(x) + P_{n-1}'(x).\n\\] Together with the initial conditions \\(P_0'(x) = 0\\) and \\(P_1'(x) = P_0(x)\\), we have the following expression of \\(D\\) with respect to the Legendre bases for \\(\\mathcal{P}_d\\) and \\(\\mathcal{P}_{d-1}\\), i.e. \\(D P_{0:d} = P_{0:d-1} C\\):\n\nfunction derivative_legendre(d)\n    DP = zeros(d,d+1)\n    if d &gt; 0\n        DP[1,2] = 1\n    end\n    for j = 1:d-1\n        DP[:,  j+2] = DP[:,j]\n        DP[j+1,j+2] = 2j+1\n    end\n    DP\nend\n\nderivative_legendre (generic function with 1 method)\n\n\nAs usual, we sanity check our computation:\n\nderivative.(legendre_basis(3)) ==\n    legendre_basis(2)*derivative_legendre(3)\n\ntrue\n\n\nIf we want the derivative for the normalized Legendre polynomials, it is convenient to write them as \\(P_{0:d} S_d\\) where \\(S_d\\) is the diagonal matrix with elements \\(\\sqrt{(2j+1)/2}\\) for \\(j=0\\) to \\(d\\). Then \\[\n  D P_{0:d} S_d = P_{0:d-1} S_{d-1} (S_{d-1}^{-1} C S_d).\n\\] In code, we have\n\nderivative_nlegendre(d) =\n    sqrt.(2.0./(2*(0:d-1).+1)) .*\n    derivative_legendre(d) .*\n    sqrt.((2*(0:d).+1)/2)'\n\nderivative_nlegendre (generic function with 1 method)\n\n\nUnlike some of our earlier computations, there is a little difference between different approaches to computing the normalized Legendre polynomials, so we check whether the results are close rather than exactly the same:\n\nlet\n    err = normalized_legendre_basis(2)*derivative_nlegendre(3) -\n        derivative.(normalized_legendre_basis(3))\n    all(iszero.(truncate.(err, atol=1e-12)))\nend\n\ntrue\n\n\n\n\n\n4.2.3 Block matrices\nSometimes we are interested in \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) where the two spaces are direct sums: \\[\\begin{aligned}\n  \\mathcal{V}&= \\mathcal{V}_1 \\oplus \\mathcal{V}_2 \\oplus \\ldots \\oplus \\mathcal{V}_n \\\\\n  \\mathcal{U}&= \\mathcal{U}_1 \\oplus \\mathcal{U}_2 \\oplus \\ldots \\oplus \\mathcal{U}_m.\n\\end{aligned}\\] In this case, we can write \\(u = Lv\\) with the subspace decompositions \\(u = u_1 + \\ldots + u_m\\) and \\(v = v_1 + \\ldots + v_n\\) as \\[\n  u_i = \\sum_{j=1}^n L_{ij} v_j\n\\] where \\(L_{ij} \\in L(\\mathcal{V}_j, \\mathcal{U}_i)\\). We can write this in block quasimatrix notation as \\[\n  \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_m \\end{bmatrix} =\n  \\begin{bmatrix}\n  L_{11} & \\ldots & L_{1n} \\\\\n  \\vdots & & \\vdots \\\\\n  L_{m1} & \\ldots & L_{mn}\n  \\end{bmatrix}\n  \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{bmatrix}.\n\\] Now suppose we have associated partitioned bases \\[\\begin{aligned}\n  V &= \\begin{bmatrix} V_1 & V_2 & \\ldots & V_n \\end{bmatrix} \\\\\n  U &= \\begin{bmatrix} U_1 & U_2 & \\ldots & U_m \\end{bmatrix}.\n\\end{aligned}\\] Then the matrix \\(A = U^{-1} L V\\) can be thought of as a block matrix with blocks \\(A_{ij} = U_i^{-1} L_{ij} V_j\\) associated with each of the \\(L_{ij}\\). Concretely, if \\(w_i = U_i d_i\\) and \\(v_j = V_j c_j\\) for appropriately-sized concrete vectors \\(d_i\\) and \\(c_i\\), we have \\[\n  \\begin{bmatrix} d_1 \\\\ \\vdots \\\\ d_m \\end{bmatrix} =\n  \\begin{bmatrix}\n  A_{11} & \\ldots & A_{1n} \\\\\n  \\vdots & & \\vdots \\\\\n  A_{m1} & \\ldots & A_{mn}\n  \\end{bmatrix}\n  \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix}.\n\\]\n\n\n4.2.4 Canonical forms\nA canonical form for a map \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) is a “simple as possible” matrix representation associated with a particular choice of bases. This also makes sense when \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\) are concrete vector spaces, in which case the canonical form is a sort of matrix decomposition.\nIn general, we will consider two types of canonical forms: those associated with general choices of bases and those associated with orthonormal bases (when \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\) are inner product spaces). The canonical forms are essentially the same whether we are working with linear maps or sesquilinear forms; we will focus on the linear map case.\n\n4.2.4.1 Block decomposition\nThe first step to the canonical forms for a general linear map is to decompose \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\) into special direct sums. For \\(\\mathcal{V}\\), we decompose into the kernel (or null space) of \\(L\\), i.e. \\(\\mathcal{N}(L) =\n\\{v \\in \\mathcal{V}: Lv = 0\\}\\), along with any complementary space: \\[\n  \\mathcal{V}= \\mathcal{V}_1 \\oplus \\mathcal{V}_2, \\quad \\mathcal{V}_2 = \\mathcal{N}(L).\n\\] For \\(\\mathcal{U}\\), we decompose into the range space \\(\\mathcal{R}(L) = \\{Lv : v \\in \\mathcal{V}\\}\\) and any complementary space: \\[\n  \\mathcal{U}= \\mathcal{U}_1 \\oplus \\mathcal{U}_2, \\quad \\mathcal{U}_1 = \\mathcal{R}(L).\n\\] The null space \\(\\mathcal{N}(L)\\) and the range \\(\\mathcal{R}(L)\\) are sometimes also known as the kernel and the image of \\(L\\). The space \\(\\mathcal{U}_2\\) is isomorphic to \\(\\mathcal{U}/ \\mathcal{R}(L)\\), sometimes called the cokernel, while the space \\(\\mathcal{V}_1\\) is isomorphic to \\(\\mathcal{V}/ \\mathcal{N}(L)\\), sometimes called the coimage. These four spaces (the kernel, cokernel, range, and corange) play a key role in linear algebra. The rank \\(r\\) is thus the both the dimension of the range and the dimension of the coimage.\nWith respect to such a decomposition, we have the block quasimatrix \\[\n  L = \\begin{bmatrix} L_{11} & 0_{r \\times (n-r)} \\\\ 0_{(m-r)\\times r}\n  & 0_{(m-r)\\times(n-r)} \\end{bmatrix}.\n\\] We have indicated the dimensions of the zero blocks here for clarity. The structure of this quasimatrix comes directly from the definition of the null space (which guarantees that the second block column should be zero) and the range space (which guarantees that the second block row should be zero). The block \\(L_{11} \\in L(\\mathcal{V}_1, \\mathcal{U}_1)\\) is one-to-one (because all null vectors of \\(L\\) are in the \\(\\mathcal{V}_2\\) space) and onto (because \\(\\mathcal{U}_1\\) is the range space). Therefore, \\(L_{11}\\) is invertible.\nThe block decomposition of a map is closely connected to the Fredholm alternative theorem. Consider the equations \\(Lx = b\\), and suppose we decompose our spaces in terms of kernel, cokernel, range, and corange as above to get the quasimatrix equation \\[\n  \\begin{bmatrix} L_{11} & 0_{r \\times (n-r)} \\\\ 0_{(m-r)\\times r}\n  & 0_{(m-r)\\times(n-r)} \\end{bmatrix}\n  \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} =\n  \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}.\n\\] Then one of two conditions must hold; the system is either\n\nConsistent (\\(b_2 = 0\\)): There is a solution to the system. In fact, if \\(x_*\\) is any solution (a particular solution), then the set of all possible solutions is \\(\\{ x_* + v : Lv = 0 \\}\\). more generally, an \\((n-r)\\)-dimensional set of solutions\nInconsistent (\\(b_2 \\neq 0\\)): There is not a solution to the system (\\(b_2 \\neq 0\\)), but there does exist \\(y^* \\in \\mathcal{U}^*\\) such that \\(y^* L = 0\\) and \\(y^* b \\neq 0\\).\n\nWithout the decomposition of \\(\\mathcal{U}\\) and \\(\\mathcal{V}\\), the statement of the Fredholm alternative (for finite dimensional spaces) is a little mysterious. With the decomposition, it is almost a matter of inspection.\nThe decomposition of a map based on range and cokernel (for \\(\\mathcal{U}\\)) and kernel and coimage (for \\(\\mathcal{V}\\)) works equally well in the infinite-dimensional setting. Of course, in this setting some of the block dimensions will be infinite! But in some cases when the kernel and cokernel are finite-dimensional, we can define the index of \\(L\\) to be the difference between the dimension of the kernel and the dimension of the cokernel (or the codimension of the range space, which is the same thing). In the finite-dimensional picture, this is \\[\n  \\dim(\\mathcal{V}_2) - \\dim(\\mathcal{U}_2) = (m-r)-(n-r) = m-n.\n\\] In other words, the index generalizes the notion of “how many more equations than variables” there are.\n\n\n4.2.4.2 General bases\nLet \\(\\mathcal{V}= \\mathcal{V}_1 \\oplus \\mathcal{V}_2\\) and let \\(\\mathcal{U}_1 \\oplus \\mathcal{U}_2\\) as above (i.e. \\(\\mathcal{U}_1\\) is the range space and \\(\\mathcal{V}_2\\) is the null space). For any basis \\(V_1\\) for \\(\\mathcal{V}_1\\), the quasimatrix \\(U_1 =\nLV_1\\) is a basis for \\(\\mathcal{U}_1\\). Concatenating any bases \\(V_2\\) and \\(U_2\\) for \\(\\mathcal{V}_2\\) and \\(\\mathcal{U}_2\\), we have bases \\(V = \\begin{bmatrix} V_1 & V_2 \\end{bmatrix}\\) for \\(\\mathcal{V}\\) and \\(U = \\begin{bmatrix} U_1 & U_2 \\end{bmatrix}\\) for \\(\\mathcal{U}\\). With respect to these bases, we have the matrix representation \\[\n  U^{-1} L V = \\begin{bmatrix} I_{r \\times r} & 0 \\\\ 0 & 0 \\end{bmatrix}.\n\\] This matrix representation is completely characterized by the dimensions of \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\) and the rank \\(r\\).\n\n\n4.2.4.3 Orthonormal bases\nIn a general vector space, we can use any complementary subspaces to the range and null space of \\(L\\). But in an inner product space, we prefer to use the orthogonal complements: \\[\\begin{aligned}\n  \\mathcal{V}_1 &= \\mathcal{N}(L)^\\perp & \\mbox{coimage} \\\\\n  \\mathcal{V}_2 &= \\mathcal{N}(L) & \\mbox{kernel / null space} \\\\\n  \\mathcal{U}_1 &= \\mathcal{R}(L) & \\mbox{range / image} \\\\\n  \\mathcal{U}_2 &= \\mathcal{R}(L)^\\perp & \\mbox{cokernel}\n\\end{aligned}\\] These are sometimes referred to as the “four fundamental subspaces” for a linear map7 (see Strang (2023)).\nOur canonical form in this setting involves a special choice for the bases \\(V_1\\) and \\(U_1\\). Specifically, we choose unit-length vectors \\(v_1, \\ldots, v_r\\) by maximizing \\(\\|Lv_j\\|\\) over all vectors in \\(\\mathcal{V}\\) orthogonal to \\(v_1, \\ldots, v_{j-1}\\). As we will discuss in Chapter 18, the vectors \\(Lv_1, \\ldots, Lv_r\\) themselves are then (nonzero) orthogonal vectors. The vectors \\(v_j\\) are the right singular vectors, the norms \\(\\sigma_j = \\|Lv_j\\|\\) are the singular values, and the vectors \\(u_j = Lv_j/\\sigma_j\\) are the left singular vectors. Concatenating the orthonormal bases \\(V_1 = \\begin{bmatrix} v_1 & \\ldots & v_r \\end{bmatrix}\\) and \\(U_1 = \\begin{bmatrix} u_1 & \\ldots & u_r \\end{bmatrix}\\) with orthonormal bases \\(V_2\\) and \\(U_2\\) for the orthogonal complements, we have the canonical form \\[\n  U^* L V = \\Sigma = \\begin{bmatrix} \\Sigma_{11} & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\] where \\(\\Sigma_{11}\\) is the diagonal matrix with the singular values \\(\\sigma_1, \\ldots, \\sigma_r\\) on the diagonal. By construction, these singular values are nonzero and are listed in descending order. Frequently, we write this canonical form as the singular value decomposition (SVD) of \\(L\\): \\[\n  L = U \\Sigma V^* = U_1 \\Sigma_{11} V_1^*.\n\\]\n\n\n4.2.4.4 Decomposed derivatives\nWe return again to our example of the derivative mapping from \\(\\mathcal{P}_d\\) to \\(\\mathcal{P}_{d-1}\\). For this example, the range space is all of \\(\\mathcal{P}_{d-1}\\) and the null space is the constant vectors. Decomposing with respect to the \\(L^2([-1,1])\\) inner product, the cokernel and coimage are the empty set and the mean zero vectors. Therefore, we can find general bases in which the matrix is \\[\n  \\begin{bmatrix} I_{d \\times d} & 0_{d \\times 1} \\end{bmatrix}.\n\\]\nMore interesting is the singular value decomposition in the \\(L^2([-1,1])\\) inner product. Here it is easiest to compute using the matrix representation of the derivative with respect to orthonormal bases like the normalized Legendre polynomials \\(\\bar{P}_{0:d}\\). Then taking \\(C = U \\Sigma V^T\\) as the singular value decomposition of the matrix representing the derivative, the singular value decomposition of the derivative map is \\[\n  D = (\\bar{P}_{0:d-1} U) \\Sigma (\\bar{P}_{0:d} V)^T.\n\\] That is, the orthonormal basis vectors that diagonalize \\(D\\) are given by \\(\\hat{U} = \\bar{P}_{0:d-1} U\\) and \\(\\bar{P}_{0:d} V\\).\nThe singular values of the derivative mapping from \\(\\mathcal{P}_d\\) to \\(\\mathcal{P}_{d-1}\\) depend on the dimension \\(d\\) of the space. It is illuminating to look at the behavior of these values in ascending order:\n\nlet\n    dmax = 10\n\n    # Compute the singular values for each degree d\n    σs = zeros(dmax,dmax)\n    for d = 1:dmax\n        F = svd(derivative_nlegendre(d))\n        σs[1:d,d] .= F.S[end:-1:1]\n    end\n\n    # Draw one line for the ith smallest singular value as a function of d\n    p = plot(1:dmax, σs[1,:], legend=:false, marker=:star, xlabel=\"d\")\n    for i = 2:dmax\n        plot!(i:dmax, σs[i,i:dmax], marker=:star)\n    end\n    p\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we increase \\(d\\), we get more singular values; but each singular value (ordered from smallest to biggest) appears to fairly quickly converge from above to a limiting value. Indeed, we can make sense of these limits.\nAs an example, consider the smallest singular value and the associated basis vectors for the derivative acting on \\(\\mathcal{P}_{10}\\) with the \\(L^2([-1,1])\\) inner product. We expect that regardless of the degree, we should have \\(Dv_{\\min} = u_{\\min} \\sigma_{\\min}\\) where \\(\\sigma_{\\min}\\) is the smallest (nonzero) singular value and \\(u_{\\min}\\) and \\(v_{\\min}\\) are the associated singular vectors (columns in the \\(U\\) and \\(V\\) bases). By \\(d = 10\\), we also expect to be close to the limiting behavior: the smallest singular value should converge to \\(\\pi/2\\), with associated singular vectors \\(v_{\\min}(x) \\rightarrow \\cos(x\\pi/2)\\) and \\(u_{\\min}(x) \\rightarrow \\sin(x\\pi/2)\\). This anticipated behavior is borne out in numerical experiment:\n\nlet\n    # Compute the SVD of D acting on P_10\n    d = 10\n    F = svd(derivative_nlegendre(d))\n    U = normalized_legendre_basis(d-1)*F.U\n    V = normalized_legendre_basis(d)*F.V\n    σs = F.S\n\n    # Get the singular triple associated with\n    # the smallest (nonzero) singular value\n    σ, u, v = σs[end], U[end], V[end]\n\n    # Check various relations (use a mesh of 100 points)\n    xx = range(-1,1,length=100)\n    s = sign(u(0))\n    println(\"\"\"\n- |D*u-v*σ| = $(norm((derivative(v)-u*σ).(xx), Inf))\n- |sin(π/2*x)-v| = $(norm(s*sin.(π/2*xx)-v.(xx), Inf))\n- |cos(π/2*x)-u| = $(norm(s*cos.(π/2*xx)-u.(xx), Inf))\n- |σmin-π/2| = $(abs(σ-π/2))\n\"\"\")\nend\n\n- |D*u-v*σ| = 5.076326047387471e-14\n- |sin(π/2*x)-v| = 4.919919749379886e-9\n- |cos(π/2*x)-u| = 1.3086657329447118e-7\n- |σmin-π/2| = 3.552713678800501e-15\n\n\n\nIn general, the \\(k\\)th smallest singular value converges to \\(k\\pi/2\\), and the singular vectors converge to sines and cosines.\n\n\n\n4.2.5 (Pseudo)inverses\nSuppose \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) has a canonical form with respect to a general basis of \\[\n  L = U \\begin{bmatrix} I_{r \\times r} & 0_{r \\times (n-r)} \\\\\n  0_{(m-r) \\times r} & 0_{(m-r) \\times (n-r)} \\end{bmatrix}\n  V^{-1}.\n\\] If \\(r \\neq m\\) and \\(r \\neq n\\), then \\(L\\) does not have an inverse. However, we can define an associated pseudoinverse \\(M \\in L(\\mathcal{V}, \\mathcal{U})\\) by \\[\n  M = V \\begin{bmatrix} I_{r \\times r} & 0_{r \\times (m-r)} \\\\\n  0_{(n-r) \\times r} & 0_{(n-r) \\times (m-r)} \\end{bmatrix}\n  U^{-1}.\n\\] Let \\(P_V = ML\\) and \\(P_U = LM\\). In terms of the bases, we have \\[\n  P_V = V \\begin{bmatrix} I_{r \\times r} & 0_{r \\times (n-r)} \\\\\n  0_{(n-r) \\times r} & 0_{(n-r) \\times (n-r)} \\end{bmatrix} V^{-1}\n\\] and \\[\n  P_U = U \\begin{bmatrix} I_{r \\times r} & 0_{r \\times (m-r)} \\\\\n  0_{(m-r) \\times r} & 0_{(m-r) \\times (m-r)} \\end{bmatrix} U^{-1}.\n\\] Both \\(P_V\\) and \\(P_U\\) are projectors, i.e. \\(P_V^2 = P_V\\) and \\(P_U^2 =\nP_U\\). The \\(P_V\\) projector is the identity on the subspace \\(\\mathcal{V}_1\\) that is the complementary to the kernel, and the null space is \\(\\mathcal{V}_1\\). The \\(P_U\\) projector is the identity on the range space \\(\\mathcal{U}_1\\), and the null space is \\(\\mathcal{U}_2\\). If \\(L\\) is one-to-one, \\(P_V\\) is simply the identity; and if \\(L\\) is onto, \\(P_U\\) is the identity.\nThe pseudoinverse depends on the decompositions \\(\\mathcal{V}= \\mathcal{V}_1\n\\oplus \\mathcal{N}(L)\\) and \\(\\mathcal{U}= \\mathcal{R}(L) \\oplus \\mathcal{U}_2\\), though it is independent of the details of the bases chosen for the four subspaces. In an inner product space, we most often choose to decompose \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\) into orthogonal complements. This gives the Moore-Penrose pseudoinverse (often just called “the pseudoinverse” when the context does not specify some other pseudoinverse). The Moore-Penrose pseudoinverse plays a central role in least squares and minimal norm problems, as we discuss in Chapter 17. In terms of the singular value decomposition, the Moore-Penrose pseudoinverse is \\[\n  L^\\dagger =\n  V \\begin{bmatrix} \\Sigma_{11}^{-1} & 0 \\\\ 0 & 0 \\end{bmatrix} U^*.\n\\] For the Moore-Penrose pseudoinverse, the associated projectors are \\(P_V = V_1 V_1^*\\) and \\(P_U = U_1 U_1^*\\). These are orthogonal projectors, since their range spaces are orthogonal complements of their null spaces. Projectors that are not orthogonal are said to be oblique projectors.\nAs an example, the Moore-Penrose pseudoinverse of the derivative operator from \\(\\mathcal{P}_d\\) to \\(\\mathcal{P}_{d-1}\\) is the indefinite integral operator \\(\\mathcal{P}_{d-1}\\) to \\(\\mathcal{P}_d\\) where the constant is chosen to make the result have zero mean. The associated projector on \\(\\mathcal{P}_d\\) is \\(q(x) \\mapsto q(x) - \\bar{q}\\) where \\(\\bar{q} = \\frac{1}{2} \\int_{-1}^1 q(x) \\, dx\\).\n\n\n4.2.6 Norms\nThe space \\(L(\\mathcal{V}, \\mathcal{U})\\) is a vector space; and, as with other vector spaces, it makes sense to talk about norms. In particular, we frequently want norms that are consistent with vector norms on the range and domain spaces; that is, for any \\(w\\) and \\(v\\), we want \\[\n  u = Lv \\implies \\|u\\| \\leq \\|L\\| \\|v\\|.\n\\] Particularly useful are the norms induced by vector norms on \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\): \\[\n  \\|A\\| = \\sup_{v \\neq 0} \\frac{\\|Av\\|}{\\|v\\|} = \\sup_{\\|v\\|=1} \\|Av\\|.\n\\] In a finite-dimensional space8, the supremum is achieved, i.e. there exists a maximizing \\(v\\).\nSuppose \\(S \\in L(\\mathcal{U}, \\mathcal{V})\\) and \\(T \\in L(\\mathcal{V}, \\mathcal{W})\\), and we have consistent norms on the three vectors spaces \\(\\mathcal{U},\n\\mathcal{V}, \\mathcal{W}\\) and the spaces of linear maps \\(L(\\mathcal{U}, \\mathcal{V})\\) and \\(L(\\mathcal{V}, \\mathcal{W})\\). Then for all \\(u \\in \\mathcal{U}\\), consistency implies \\[\n  \\|TSu\\| \\leq \\|T\\| \\|Su\\| \\leq \\|T\\| \\|S\\| \\|u\\|.\n\\] Taking the supremum over \\(\\|u\\| = 1\\) implies that the induced norm on \\(L(\\mathcal{U}, \\mathcal{W})\\), we have \\[\n  \\|TS\\| \\leq \\|T\\| \\|S\\|.\n\\] This fact is most often used when the norms on \\(L(\\mathcal{U}, \\mathcal{V})\\) and \\(L(\\mathcal{V}, \\mathcal{W})\\) are also induced norms.\nFor inner product spaces, we can also define norms in terms of the singular value decomposition. Some examples include\n\nThe spectral norm (the largest of the singular values)\nThe Frobenius norm (the two-norm of the singular values)\nThe nuclear norm (the sum of the singular values)\n\nAll of these are also examples of Ky Fan norms (the Ky Fan \\(k\\)-norm is the sum of the \\(k\\) largest singular values) and Schatten norms (the Schatten norm is the \\(p\\)-norm of the vector of singular values). The spectral norm is the norm induced by the Euclidean structure on the two spaces. All the other Ky Fan and Schatten norms are consistent, though they are not induced norms.\nFor concrete vector spaces, we can write the Frobenius norm as \\[\n  \\|A\\|_F = \\sqrt{ \\sum_{i,j} |a_{ij}|^2 }.\n\\] The induced norms corresponding to the vector 1-norm and \\(\\infty\\)-norm are \\[\\begin{aligned}\n  \\|A\\|_1 &= \\max_j \\sum_i |a_{ij}| \\quad \\mbox{(max absolute column sum)}\\\\\n  \\|A\\|_\\infty &= \\max_i \\sum_j |a_{ij}| \\quad \\mbox{(max absolute row sum)}\n\\end{aligned}\\] The norm induced by the standard Euclidean norm (the spectral norm) is harder to compute without appealing to the singular value decomposition.\n\n\n4.2.7 Isometries\nA linear map that preserves norms is called an isometry. Isometries are always one-to-one, since any null vector must have zero norm. If an isometry is also onto, it is sometimes called a global isometry. Global isometries are invertible, and the inverse is also an isometry.\nFor general norms, there are not always many interesting isometries. For example, the only isometries from \\(\\mathbb{R}^n\\) to itself preserving the 1-norm or the \\(\\infty\\)-norm are (signed) permutations. For inner product spaces, though, there is a much richer group of isometries, and we will focus on this case.\nIsometries between inner product spaces necessarily preserve inner products, since the inner product can be recovered from the associated norm. For global isometries between inner product spaces, the adjoint and the inverse are the same. A global isometry from a space to itself is called an orthogonal operator in the real case, or a unitary operator in the complex case.\nA unitary operator maps an orthonormal basis to another orthonormal basis. Therefore, if \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) and \\(W\\) and \\(Z\\) are unitary operator operators on \\(\\mathcal{U}\\) and \\(\\mathcal{V}\\), respectively, then \\(L\\) and \\(W L Z\\) have the same singular values. This also means that any norms that can be defined in terms of singular values (including the spectral norm, the Frobenius norm, and the nuclear norm) are the same for \\(L\\) and \\(W L Z\\). This properties is known as unitary invariance (or orthogonal invariance in the real case).\n\n\n4.2.8 Volume scaling\nIn an inner product space, we usually say that a paralleliped associated with an orthonormal basis (i.e. a unit cube) has unit volume. A unitary operator between two spaces maps a unit cube in one space to a unit cube in another space, i.e. it preserves volume. More generally, if \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) and \\(V\\) and \\(U\\) are orthonormal bases associated with the singular value canonical form, then we can see \\(L = U \\Sigma V^*\\) as\n\nA volume-preserving transformation \\(V^*\\) from \\(\\mathcal{V}\\) into \\(\\mathbb{R}^n\\) (or \\(\\mathbb{C}^n\\)).\nA diagonal scaling operation on \\(\\mathbb{R}^n\\) that change each side length from \\(1\\) to \\(\\sigma_j\\). The volume of the scaled paralleliped in \\(\\mathbb{R}^n\\) is then \\(\\prod_{j=1}^n \\sigma_j\\).\nA volume-preserving transformation \\(U\\) from \\(\\mathbb{R}^n\\) into \\(\\mathcal{V}\\).\n\nTherefore, if \\(\\Omega \\subset \\mathcal{V}\\) is a region with volume \\(\\operatorname{vol}(\\Omega)\\), then \\[\n  \\operatorname{vol}(L \\Omega) =\n  \\left( \\prod_{j=1}^n \\sigma_j \\right) \\operatorname{vol}(\\Omega).\n\\] In other words, the product of the singular values gives the magnitude of the determinant of \\(L\\). We will discuss determinants and signed volumes in Section 4.5.5.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "00-Background/03-LA.html#operators",
    "href": "00-Background/03-LA.html#operators",
    "title": "4  Linear Algebra",
    "section": "4.3 Operators",
    "text": "4.3 Operators\nAn operator is a map \\(L \\in L(\\mathcal{V}, \\mathcal{V})\\). Everything we know about general maps also works for operators, but we have a lot of additional structure coming from the fact that the space is mapped to itself.\nMost of our discussion of operators focuses on three connected ideas:\n\nA key feature of an operator \\(L\\) is its invariant subspaces, i.e subspaces \\(\\mathcal{U}\\subset \\mathcal{V}\\) such that \\(L \\mathcal{U}= \\{ Lu : u \\in\n\\mathcal{U}\\}\\) lies within \\(\\mathcal{U}\\). Given an invariant subspace, we can analyze the associated restriction \\(L|_{\\mathcal{U}}\\) of \\(L\\) to \\(\\mathcal{U}\\). One-dimensional invariant subspaces play a particular notable role, as they contain the eigenvectors of \\(L\\); the restrictions are the associated eigenvalues.\nWe can sensibly talk about polynomials in \\(L\\): if \\(p(z) = \\sum_{j=0}^d c_j z^j\\), then \\(p(L) = \\sum_{j=0}^d c_j L^j\\). Such polynomials are useful for designing and analyzing numerical methods (e.g. Krylov subspace methods for linear systems and eigenvalue problems). They are also useful as a more purely theoretical tool.\n\nIn Chapter 5, we will also discuss an alternative approach to analyzing operators by applying the tools of complex analysis to the resolvent \\(R(z) = (zI-L)^{-1}\\), a rational function on \\(\\mathbb{C}\\) whose poles correspond to eigenvalues. This perspective is useful both for computation and theory. It is particularly useful if we want to generalize from the finite-dimensional to the infinite-dimensional setting.\n\n4.3.1 Minimal polynomial\nIf \\(\\mathcal{V}\\) is an \\(n\\)-dimensional space, then \\(L(\\mathcal{V}, \\mathcal{V})\\) is an \\(n^2\\)-dimensional space, and so there must be a linear dependency between the set of \\(n^2+1\\) operators \\(\\{L^j\\}_{j=0}^{n^2}\\). Consequently, there must be an annihilating polynomial of degree at most \\(n^2\\), i.e. a polynomial \\(p\\) such that \\(p(L) = 0\\). Among the annihilating polynomials for \\(L\\), there must be some monic9 annihilating polynomial \\(p_{\\min}\\) of minimal degree \\(m\\). We call \\(p_{\\min}\\) the minimal polynomial.\nIf \\(p\\) is any annihilating polynomial, polynomial division lets us write \\[\n  p(z) = q(z) p_{\\min}(z) + r(z), \\quad \\deg(r) &lt; m.\n\\] Because \\(p_{\\min}(L) = 0\\), we have \\[\n  p(L) = q(L) p_{\\min}(L) + r(L) = r(L),\n\\] and therefore \\(r = 0\\); otherwise \\(r\\) would be an annihilating polynomial of lower degree than \\(m\\), contradicting the minimality. Therefore any annihilating polynomial can be written as \\(q(z)\np_{\\min}(z)\\) for some other polynomial \\(q(z)\\). As a direct consequence, the minimal polynomial is unique. Also as a consequence, the minimal polynomial of \\(L|_{\\mathcal{U}}\\) for any subspace \\(\\mathcal{U}\\) divides \\(p_{\\min}\\) for \\(L\\), since \\(p_{\\min}\\) for \\(L\\) is an annihilating polynomial for \\(L|_{\\mathcal{U}}\\).\nWe can write the minimal polynomial in factored form (over \\(\\mathbb{C}\\)) as \\[\n  p_{\\min}(z) = \\prod_{j=1}^s (z-\\lambda_j)^{d_j}.\n\\] Each term \\(L-\\lambda_j I\\) must be singular; otherwise, we would have \\((L-\\lambda_j I)^{-1} p_{\\min}(L) = 0\\), contradicting minimality.\nThe \\(\\lambda_j\\) are the eigenvalues of \\(L\\), and the null spaces of each \\(L-\\lambda_j I\\) are comprised of eigenvectors. If \\(d_j = 1\\) for each eigenvalue, then there is a basis of eigenvectors; otherwise, we have to consider generalized eigenvectors as well to get a basis. In either case, the space \\(\\mathcal{V}\\) can be decomposed into a direct sum of invariant subspaces \\(\\mathcal{N}\\left( (L-\\lambda_j I)^{d_j} \\right)\\); these are the maximal invariant subspaces associated with the eigenvalues.\nIf we let \\(m_j = \\dim \\mathcal{N}\\left( (L-\\lambda_j I)^{d_j} \\right)\\), we can also define the characteristic polynomial \\[\n  p_{\\mbox{char}}(z) = \\prod_{j=1}^s (z-\\lambda_j)^{m_j}.\n\\] The dimension \\(m_j\\) is the algebraic multiplicity of the eigenvalue \\(\\lambda_j\\). The geometric multiplicity is the dimension of \\(\\mathcal{N}(L - \\lambda_j I)\\), i.e. the number of independent eigenvectors for \\(\\lambda_j\\).\nThe characteristic polynomial is often also written as \\[\n  p_{\\mbox{char}}(z) = \\det(zI-L),\n\\] but writing the characteristic polynomial in terms of determinants has the disadvantage that we need an appropriate definition of determinants first!\n\n\n4.3.2 Canonical forms\nAs in the case of maps between different spaces, we are interested in canonical matrix representations for operators \\(L \\in L(\\mathcal{V},\n\\mathcal{V})\\), i.e. choices of bases that make the matrix representation “as simple as possible.” We consider two such forms: one involving an arbitrary basis, and the other involving an orthonormal basis.\n\n4.3.2.1 Jordan form\nAs we noted above, the space \\(\\mathcal{V}\\) can be written as \\[\n  \\mathcal{V}= \\mathcal{V}_1 \\oplus \\mathcal{V}_2 \\oplus \\ldots \\oplus \\mathcal{V}_s,\n\\] where \\(\\mathcal{V}_s\\) is the maximal invariant subspace associated with the eigenvalue \\(\\lambda_s\\), i.e. \\[\n  \\mathcal{V}_j = \\mathcal{N}((L-\\lambda_j I)^{d_j}).\n\\] With respect to this partitioning of \\(\\mathcal{V}\\), we have the block diagonal quasimatrix representation \\[\n  \\begin{bmatrix}\n    L_{11} \\\\ & L_{22} \\\\ & & \\ddots \\\\ & & & L_{ss}\n  \\end{bmatrix}\n\\] where \\(L_{jj}\\) represents the restriction \\(L|_{\\mathcal{V}_j}\\). With a choice of bases for each of the \\(\\mathcal{V}_j\\), this leads to a block diagonal matrix representation.\nWhen the minimal polynomial exponent \\(d_j\\) is 1, we have that \\(\\mathcal{V}_j = \\mathcal{N}(L-\\lambda_j I)\\), and so \\[\n  L|_{\\mathcal{V}_j} = \\lambda_j I.\n\\] Hence, if there are no repeated zeros of the minimal polynomial, we can choose bases \\(V_j\\) for each subspace \\(\\mathcal{V}_j\\), and each will satisfy \\[\n  L V_{j} = V_j (\\lambda_j I).\n\\] Putting these together, we have the concatenated basis \\[\n  V = \\begin{bmatrix} V_1 & V_2 & \\ldots & V_s \\end{bmatrix}\n\\] satisfying the relationships \\[\n  LV = V \\Lambda\n\\] where \\(\\Lambda\\) is a diagonal matrix in which each eigenvalue \\(\\lambda_j\\) appears \\(\\dim \\mathcal{V}_j\\) times in a row. With respect to the eigenvector basis \\(V\\), we therefore have the canonical matrix representation \\[\n  \\Lambda = V^{-1} L V.\n\\] Hence, such operators are diagonalizable.\nWhen the minimal polynomial has zeros with multiplicity greater than one, the matrix is not diagonalizable. In this case, we cannot form a basis of eigenvectors, but we can form a basis if we include generalized eigenvectors. In the Jordan canonical form, we find bases consisting of Jordan chains, i.e. vectors satisfying \\[\\begin{aligned}\n  (L-\\lambda_j I) u_1 &= 0 \\\\\n  (L-\\lambda_j I) u_k &= u_{k-1}, \\quad 1 &lt; k \\leq b.\n\\end{aligned}\\] Rewriting the second expression as \\(L u_k = u_{k-1} + \\lambda_j u_k\\) and defining the quasimatrix \\(U = \\begin{bmatrix} u_1 & \\ldots & u_b \\end{bmatrix}\\), we can summarize with the matrix equation \\[\n  L U = U J_{b\\times b}(\\lambda)\n\\] where \\[\n  J_{b \\times b}(\\lambda)\n  \\begin{bmatrix}\n    \\lambda_j & 1 \\\\\n    & \\lambda_j & 1 \\\\\n    & & \\ddots & \\ddots \\\\\n    & & & \\lambda_j & 1 \\\\\n    & & & & \\lambda_j\n  \\end{bmatrix}\n\\] is a \\(b\\)-dimensional Jordan block. The maximum size of any Jordan block is equal to the multiplicity \\(d_j\\) of \\(\\lambda_j\\) in the minimal polynomial; the number of Jordan blocks for \\(\\lambda_j\\) is equal to the geometric multiplicity \\(\\dim \\mathcal{N}(L-\\lambda_j I)\\); and the sum of the sizes of the Jordan blocks is equal to the algebraic multiplicity \\(\\dim \\mathcal{N}((L-\\lambda_j I)^{d_j})\\). Taking a basis formed from concatenating such Jordan chains for each space, we have the Jordan canonical form, a block diagonal matrix in which each diagonal block is a Jordan block.\nThe Jordan form is fragile: almost all operators are diagonalizable and have distinct eigenvalues, and infinitesimally small changes will usually completely destroy any nontrival Jordan blocks. Moreover, diagonalizable operators that are close to operators with nontrivial Jordan blocks may have eigenvector bases, but those eigenvector bases are not all that nice. We therefore would like an alternate canonical form that gives up some of the goal of being “as diagonal as possible.”\n\n\n4.3.2.2 Schur form\nIn our discussion of the Jordan form, we started with the decomposition of \\(\\mathcal{V}\\) into maximal invariant subspaces associated with each eigenvalue. For the Schur form, we decompose \\(\\mathcal{V}\\) in terms of nested invariant subspaces: \\[\n  \\mathcal{W}_k = \\oplus_{j=1}^k \\mathcal{V}_j.\n\\] Assuming \\(\\mathcal{V}\\) is an inner product space, we can also write \\[\n  \\mathcal{W}_k = \\oplus_{j=1}^k \\mathcal{U}_j\n\\] where \\(\\mathcal{U}_1 = \\mathcal{V}_1\\) and otherwise \\(\\mathcal{U}_j\\) is the orthogonal complement of \\(\\mathcal{W}_{j-1}\\) in \\(\\mathcal{W}_j\\). With respect to the decomposition \\(\\mathcal{V}= \\oplus_{j=1}^s \\mathcal{U}_j\\), we have the block upper triangular quasimatrix representation \\[\n  \\begin{bmatrix}\n    L_{11} & L_{12} & \\ldots & L_{1s} \\\\\n           & L_{22} & \\ldots & L_{2s} \\\\\n           &        & \\ddots & \\vdots \\\\\n           &        &        & L_{ss}\n  \\end{bmatrix}\n\\] With an appropriate choice of orthonormal basis \\(U_j\\) for each \\(\\mathcal{U}_j\\) and setting \\(U = \\begin{bmatrix} U_1 & U_2 & \\ldots & U_s\n\\end{bmatrix}\\), we have \\[\n  U^* L U = T\n\\] where \\(T\\) is an upper triangular matrix for which the eigenvalues are on the diagonal. This is a (complex) Schur canonical form.\nThe complex Schur form may in general involve complex choices of the basis \\(U\\) and a complex matrix representation \\(T\\), even if \\(\\mathcal{V}\\) is most naturally treated as a vector space over the reals. However, there is a real Schur form which involves a real orthonormal basis and a block upper triangular matrix \\(T\\) where the diagonal blocks are 1-by-1 (for real eigenvalues) or 2-by-2 (for conjugate pairs of eigenvalues).\n\n\n\n4.3.3 Similar matrices\nSo far, we have focused on matrices as representations of operators on abstract vector spaces, where we choose one basis \\(V\\) for the space \\(\\mathcal{V}\\): \\[\n  A = V^{-1} L V.\n\\] If we consider an alternate basis \\(V' = VX\\) for an invertible matrix \\(X\\), we have the associated matrix representation \\[\n  A' = V'^{-1} L V' = (V^{-1} V')^{-1} A (V^{-1} V') = X^{-1} A X.\n\\] The transformation \\(A \\mapsto X^{-1} A X\\) is a similarity transformation, and the matrices \\(A\\) and \\(A'\\) are said to be similar.\nThe properties we have discussed so far (eigenvalues, the minimal and characteristic polynomial, the Jordan form) are fundamentally properties of operators, and do not depend on the basis in which those operators are expressed. As much as possible, we try to make this clear by giving basis-free explanations when possible. But it is also possible to give matrix-based arguments in which we argue for invariance under similarity transformations. For example, if \\(\\hat{A}\n= X^{-1} A X\\), then \\[\n  \\hat{A}^j = (X^{-1} A X)^j = X^{-1} A^j X\n\\] and, more generally, \\(p(\\hat{A}) = X^{-1} p(A) X\\). Therefore, any annihilating polynomial for \\(p(A)\\) is also an annihilating polynomial for \\(\\hat{A}\\); and the minimal and characteristic polynomials for \\(A\\) will similarly be the minimal and characteristic polynomials for \\(\\hat{A}\\).\n\n\n4.3.4 Trace\nWe typically describe the trace in terms of a matrix representation: the trace of a matrix is the sum of its diagonal entries, i.e. \\[\n  \\operatorname{tr}(A) = \\sum_{i=1}^n a_{ii}.\n\\] The trace satisfies the cyclic property that for any \\(B \\in \\mathbb{F}^{m\n\\times n}\\) and \\(C \\in \\mathbb{F}^{n \\times m}\\) we have \\[\\begin{aligned}\n  \\operatorname{tr}(AB)\n  &= \\sum_{i=1}^m \\sum_{j=1}^n b_{ij} c_{ji} \\\\\n  &= \\sum_{j=1}^n \\sum_{i=1}^m c_{ji} b_{ij} \\\\\n  &= \\operatorname{tr}(BA).\n\\end{aligned}\\] Using this property, for any invertible \\(X \\in \\mathbb{F}^{n \\times n}\\) we have \\[\n  \\operatorname{tr}(X^{-1} A X) = \\operatorname{tr}(A X X^{-1}) = \\operatorname{tr}(A).\n\\] That is, the trace is invariant under similarity, and is thus a property of the underlyling linear operator and not just of a particular matrix representation. In particular, if \\(X^{-1} A X = J\\) is a Jordan canonical form, we can see that the trace is the sum of the eigenvalues (counting geometric multiplicity).\n\n\n4.3.5 Frobenius inner product\nThe trace is also useful for defining other structures on spaces of linear maps in a basis-free way. Let \\(L \\in L(\\mathcal{V}, \\mathcal{U})\\) be a linear map between two inner product spaces, and let \\(L = U \\Sigma\nV^*\\) be the singular value decomposition. Then \\[\n  \\operatorname{tr}(L^* L) = \\operatorname{tr}(V \\Sigma^2 V^*) = \\operatorname{tr}(\\Sigma^2 V^* V) =\n  \\operatorname{tr}(\\Sigma^2) = \\sum_{j=1}^n \\sigma_j^2 = \\|L\\|_F^2.\n\\] where \\(\\|L\\|_F^2\\) is the squared Frobenius norm. Also, if \\(B\\) is any matrix representation of \\(L\\) with respect to orthonormal bases, then we have \\[\n  \\|L\\|_F^2 = \\|B\\|_F^2 = \\operatorname{tr}(B^* B) = \\sum_{i,j} |b_{ij}|^2,\n\\] so it is very simple to compute the Frobenius norm of \\(L\\) given any matrix representation with respect to orthonormal bases.\nMore generally, the trace can be used to define the Frobenius inner product on the space \\(L(\\mathcal{V}, \\mathcal{W})\\) via \\[\n  \\langle L, M \\rangle_F = \\operatorname{tr}(M^* L).\n\\] The Frobenius norm is the standard Euclidean norm associated with the Frobenius inner product.\n\n\n4.3.6 Hermitian and skew\nIf \\(L \\in L(\\mathcal{V}, \\mathcal{V})\\), then \\[\n  L = H + S, \\quad H = \\frac{1}{2} (L+L^*), \\quad S = \\frac{1}{2}(L-L^*).\n\\]\nThe operators \\(H\\) and \\(S\\) are known as the Hermitian and skew-Hermitian parts of \\(L\\). We note that and \\(\\langle H, S \\rangle_F\n= 0\\), so by the Pythagorean theorem, \\[\n  \\|L\\|_F^2 = \\|H\\|_F^2 + \\|S\\|_F^2.\n\\] More generally, \\(L(\\mathcal{V}, \\mathcal{V})\\) is a direct sum of Hermitian and skew-Hermitian subspaces, and these spaces are orthogonal complements of each other.\nAny operator \\(L\\) has a Schur canonical form \\[\n  U^* L U = T\n\\] where \\(T\\) is an upper triangular matrix. For a Hermitian matrix \\(H\\), we have \\[\n  T = U^* H U = (U^* H U)^* = T^*,\n\\] which implies that \\(T\\) must be diagonal (since the subdiagonal elements of \\(T\\) are zero and the superdiagonal elements of \\(T^*\\) are zero), and that the diagonal part of \\(T\\) must be real (since \\(t_{ii} =\nt_{ii}^*\\)). Similarly, for a skew-Hermitian matrix \\(S\\), we have \\[\n  T = U^* S U = -(U^* S U)^* = -T^*,\n\\] which simplies that in this case \\(T\\) is diagonal and the diagonal part is imaginary. Hence, the decomposition of a matrix into Hermitian and skew-Hermitian parts in many ways mirrors the decomposition of a complex number into real and imaginary parts.\nThere is an even stronger number between the Hermitian / skew-Hermitian decomposition and the complex numbers for the case of \\(\\mathbb{R}^{2 \\times 2}\\). Let \\(g : \\mathbb{C}\\rightarrow \\mathbb{R}^{2 \\times 2}\\) be the map \\[\n  g(a + bi) = aI + bJ, \\quad J = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}.\n\\] Then \\[\\begin{aligned}\ng(0) &= 0 \\\\\ng(1) &= I \\\\\ng(z+w) &= g(z) + g(w) \\\\\ng(zw) &= g(z) g(w) \\\\\ng(z^{-1}) &= g(z)^{-1}.\n\\end{aligned}\\] That is, the complex matrices are isomorphic to the subset of the 2-by-2 real matrix where the symmetric part is proportional to the identity.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "00-Background/03-LA.html#hermitian-and-quadratic-forms",
    "href": "00-Background/03-LA.html#hermitian-and-quadratic-forms",
    "title": "4  Linear Algebra",
    "section": "4.4 Hermitian and quadratic forms",
    "text": "4.4 Hermitian and quadratic forms\nA sesquilinear form \\(a : \\mathcal{V}\\times \\mathcal{V}\\rightarrow \\mathbb{F}\\) is Hermitian if \\(a(v,w) = \\overline{a(w,v)}\\).\nIf \\(a(v,w) = -\\overline{a(w,v)}\\), the form is called skew-Hermitian. Any sesquilinear form on \\(\\mathcal{V}\\times \\mathcal{V}\\) can be decomposed into Hermitian and skew-Hermitian parts: \\[\\begin{aligned}\n  a(v,w) &= a^H(v,w) + a^S(v,w) \\\\\n  a^H(v,w) &= \\frac{1}{2} \\left( a(v,w) + \\overline{a(w,v)} \\right) \\\\\n  a^S(v,w) &= \\frac{1}{2} \\left( a(v,w) - \\overline{a(w,v)} \\right).\n\\end{aligned}\\] We use the terms Hermitian and skew-Hermitian generically to also refer to symmetric and skew-symmetric forms on real vector spaces.\nIf \\(\\mathcal{V}\\) is an inner product space, then for any sesquilinear form \\(a : \\mathcal{V}\\times \\mathcal{V}\\rightarrow \\mathbb{F}\\), there is an associated operator \\(L : \\mathcal{V}\\rightarrow \\mathcal{V}\\) such that \\[\n  a(v,w) = \\langle Lv, w \\rangle.\n\\] When \\(a\\) is Hermitian, we have \\[\n  \\langle Lv, w \\rangle = a(v,w) = \\overline{a(w,v)} = \\langle v, Lw \\rangle,\n\\] i.e. \\(L = L^*\\) is a self-adjoint operator. A similar argument says that if \\(a\\) is skew-Hermitian, the corresponding operator will satisfy \\(L = -L^*\\). Hence, the study of Hermitian forms is closely linked to the study of self-adjoint operators.\nA quadratic form is a function \\(\\phi : \\mathcal{V}\\rightarrow \\mathbb{F}\\) that is homogeneous of degree 2 (i.e. \\(\\phi(\\alpha v) = \\alpha^2 \\phi(v)\\)) and such that \\((u, v) \\mapsto \\phi(u+v) - \\phi(u) - \\phi(v)\\) is bilinear. The associated bilinear form is always symmetric. Hence, on real vector spaces the study of Hermitian forms is also closely linked to the study of quadratic forms.\n\n4.4.1 Matrices\nGiven a basis \\(V\\) for \\(\\mathcal{V}\\), the standard matrix representation of a sesquilinear form on \\(\\mathcal{V}\\times \\mathcal{V}\\) is a square matrix with entries \\(a_{ij} = a(v_j, v_i)\\) so that \\[\n  a(Vc, Vd) = d^* A c.\n\\] Decomposing the form into Hermitian and skew-Hermitian parts corresponds to decomposing \\(A = H+S\\) where \\(H=H^*\\) and \\(S=-S^*\\) are the Hermitian and skew-Hermitian parts of the matrix.\nA quadratic form \\(\\phi\\) has an associated symmetric bilinear form \\[\n  a(u, v) = \\frac{1}{2} \\left( \\phi(u+v) - \\phi(u) - \\phi(v) \\right).\n\\] The matrix representation for a quadratic form is \\[\n  \\phi(Vc) = c^T A c\n\\] where the entries of \\(A\\) are \\(a_{ij} = a(v_i, v_j)\\). Conversely, for any real bilinear form, \\(a\\), we have that \\(\\phi(v) := a(v,v)\\) is a quadratic form, and \\(\\phi(Vc) = c^T H c\\) where \\(H\\) is the matrix for the symmetric part of \\(a\\).\nLet \\(A\\) be a matrix representing a sesquilinear form \\(a : \\mathcal{V}\\times \\mathcal{V}\n\\rightarrow \\mathcal{F}\\) with respect to a basis \\(V\\), i.e. \\[\n  a(Vc, Vd) = d^* A v.\n\\] We can write any other basis as \\(\\hat{V} = VX\\) for some nonsingular \\(X\\); with respect to the \\(\\hat{V}\\) basis, we have \\[\n  a(\\hat{V}c \\hat{V}d) = a(VXc, VXd) = d^* (X^* A X) c.\n\\] For any nonsingular \\(X\\), we say \\(A\\) and \\(\\hat{A} = X^* A X\\) are congruent. Just as similar matrices represent the same operator under different bases, congruent matrices represent the same sesquilinear form under different bases. When we restrict our attention to orthonormal bases, the matrix \\(X\\) will be unitary; we note that \\(X^* = X^{-1}\\) for unitary \\(X\\), so a unitary congruence and a unitary similarity are the same thing.\n\n\n4.4.2 Canonical forms\nIn the case of a general basis, there exists is a basis such that the matrix representation of a Hermitian form is \\[\n  \\begin{bmatrix}\n    I_{\\nu_+} & &  \\\\\n    & 0_{\\nu_0} & \\\\\n    & & -I_{\\nu_{-}}\n  \\end{bmatrix}\n\\] The triple \\((\\nu_+, \\nu_0, \\nu_-)\\) is the inertia of the form. Inertia, like rank, is a property of the linear algebraic object (in this case the Hermitian form) rather than its representation in any specific basis. Let \\(n = \\dim(\\mathcal{V})\\); then we classify the form based on its inertia as:\n\nPositive definite if \\(\\nu = (n, 0, 0)\\)\nPositive semidefinite if \\(\\nu = (r, n-r, 0)\\)\nNegative definite if \\(\\nu = (0, 0, n)\\)\nNegative semidefinite if \\(\\nu  (0, n-r, r)\\)\nStrongly indefinite if \\(\\nu_+ &gt; 0\\) and \\(\\nu_- &gt; 0\\).\n\nA Hermitian positive definite form is an inner product.\nIf \\(\\mathcal{V}\\) is an inner product space and we restrict ourselves to orthonormal bases, we have a diagonal matrix \\(\\Lambda\\) with \\(\\nu_+\\) positive values, \\(\\nu_0\\) zeros, and \\(\\nu_-\\) negative values. If we identify \\(a(v, w)\\) with \\(\\langle Lv, w \\rangle\\), then this canonical form gives that \\(a(v_i, v_j) = \\langle Lv_i, v_j \\rangle = \\lambda_i\n\\delta_{ij}\\). The Hermitian property implies that \\(a(v_i, v_i) =\n\\overline{a(v_i, v_i)} = \\lambda\\), so \\(\\lambda\\) must be real even when the form is over \\(\\mathbb{C}\\). Given the orthonormality of the basis, this means \\[\n  Lv_j = v_j \\lambda_j,\n\\] i.e. the canonical decomposition of the Hermitian form is really the same as the eigenvalue decomposition of the associated self-adjoint operator \\(L\\).\nThe Hermitian eigenvalue problem has an enormous amount of structure that comes from the fact that it can be interpreted either in terms of a Hermitian form or the associated quadratic or in terms of an operator.\n\n\n4.4.3 A derivative example\nAs before, it is useful to consider a specific example that does not involve a concrete vector space. We will use the real symmetric form on \\(\\mathcal{P}_d\\) given by \\[\n  a(p, q) = \\left\\langle \\frac{dp}{dx}, \\frac{dq}{dx} \\right\\rangle\n\\] using the \\(L^2([-1,1])\\) inner product between polynomials. Integration by parts implies that \\[\\begin{aligned}\n  a(p,q)\n  &= \\int_{-1}^1 \\frac{dp}{dx} \\frac{dq}{dx} \\, dx \\\\\n  &= \\left. \\frac{dp}{dx} q \\right|_{-1}^1 - \\int_{-1}^1 \\frac{d^2p}{dx^2} q \\, dx \\\\\n  &= \\left\\langle \\delta_1 \\delta_1^* \\frac{dp}{dx}  -\n             \\delta_{-1} \\delta_{-1}^* \\frac{dp}{dx} -\n             \\frac{d^2 p}{dx^2}, q \\right\\rangle,\n\\end{aligned}\\] where \\(\\delta_1^*\\) and \\(\\delta_{-1}^*\\) are the evaluation functionals at \\(\\pm 1\\) and \\(\\delta_1\\) and \\(\\delta_{-1}\\) are the associated vectors under the Riesz map. Hence, the self-adjoint linear map associated with the form is \\[\n  L = \\delta_1 \\delta_1^* \\frac{d}{dx} - \\delta_{-1} \\delta_{-1}^*\n  \\frac{d}{dx} - \\frac{d^2}{dx^2}.\n\\] We can check ourselves by implementing this relation in code\n\nlet\n    # Implement the L map\n    d = 2\n    δp1 = dualL2_δx( 1.0, d)\n    δn1 = dualL2_δx(-1.0, d)\n    function L(p)\n        Dp = derivative(p)\n        D2p = derivative(Dp)\n        δp1*Dp(1) - δn1*Dp(-1) - D2p\n    end\n\n    # Test polynomials p and q\n    p = Polynomial([2.0; -3.0; 1.0])\n    q = Polynomial([1.0; 1.0; 1.0])\n\n    # Test agreement with the form and self-adjointness\n    dotL2(derivative(p), derivative(q)) ≈ dotL2(L(p), q),\n    dotL2(L(p), q) ≈ dotL2(p, L(q))\nend\n\n(true, true)\n\n\nThe form is positive semidefinite, since \\(a(1,1) = 0\\) but \\(a\\) cannot ever by negative (by positive definiteness of the inner product); the inertia is \\((d, 1, 0)\\). While we can compute the eigenvalues in terms of a matrix derived from \\(L\\), it is simpler to compute the matrix directly from the form:\n\nlet\n    d = 3\n\n    # Compute the matrix in terms of the normalized Legendre polynomials\n    P̄ = normalized_legendre_basis(d)\n    AP̄ = gram(dotL2, derivative.(P̄))\n\n    # Compute the eigendecomposition\n    F = eigen(AP̄)\n    U = P̄*F.vectors\n\n    # Check orthonormality of the U basis,\n    # and that the associated matrix is Λ\n    gram(dotL2, U) ≈ I,\n    gram(dotL2, derivative.(U)) ≈ Diagonal(F.values)\nend\n\n(true, true)\n\n\nSimilar to what we saw in our discussion of the singular value decomposition, the \\(k\\)th nonzero eigenvalue converges as a function of \\(d\\) to a limiting value of \\((k \\pi/2)^2\\). We plot the convergence of the first few eigenvalues below:\n\nlet\n    d = 15\n    P̄ = normalized_legendre_basis(d)\n    AP̄ = gram(dotL2, derivative.(P̄))\n\n    p = plot()\n    for j=2:5\n        λs = [eigvals(AP̄[1:n,1:n])[j] for n=j:d+1]\n        plot!(j:d+1, abs.(λs.-((j-1)*π/2)^2),\n              yscale=:log10, label=\"λ[$j]\")\n    end\n    p\nend",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "00-Background/03-LA.html#sec-tensor-bg-sec",
    "href": "00-Background/03-LA.html#sec-tensor-bg-sec",
    "title": "4  Linear Algebra",
    "section": "4.5 Tensors and determinants",
    "text": "4.5 Tensors and determinants\nSo far, we have dealt with linear, bilinear, sesquilinear, and quadratic functions on vector spaces. Going beyond this to functions that are linear in several arguments takes us into the land of tensors.\n\n4.5.1 Takes on tensors\nThere are several ways to approach tensors. In much of numerical analysis, only tensors of concrete spaces are considered, and tensors are treated as arrays of multi-dimensional arrays of numbers (with matrices as a special case where there are two dimensions used for indexing). For example, a tensor with three indices is represented by an element \\(a_{ijk}\\) of \\(\\mathbb{F}^{m \\times n \\times p}\\). We will see this perspective in detail later, both in our discussion of numerical linear algebra and in our discussion of tensors in latent factor models.\nHere, though, we will focus on tensors as basic objects of multilinear algebra. There are still several ways to approach this:\n\nWe can consider tensors purely in terms of bases, along with change-of-basis rules. This view, common in some corners of physics, says that “a tensor is an object that transforms like a tensor,” a perspective similar to “a vector is an object that transforms like a vector” that we mentioned earlier. We will prefer a discussion that gives changes of basis as a consequence of a more fundamental definition, rather than as the definition itself.\nWe can consider tensors from an algebraic perspective as a quotient space: the tensor product space \\(\\mathcal{V}\\otimes \\mathcal{W}\\) is a vector space spanned by elements of the Cartesian product \\(\\mathcal{V}\\times\n\\mathcal{W}\\), modulo the equivalence relations \\[\\begin{aligned}\n  \\alpha (v, w) &\\sim (\\alpha v, w) \\sim (v, \\alpha w) \\\\\n  (u+v, w) &\\sim (u,w) + (v,w) \\\\\n  (u, v+w) &\\sim (u,v) + (u,w)\n\\end{aligned}\\] We write \\(v \\otimes w\\) to denote the equivalence class associated with \\((v,w)\\). While this definition has the advantage of being very general (it can generalize to structures other than vector spaces) and it does not depending on a basis, it can be a bit overly abstract.\nWe will mostly consider tensors as multilinear forms, generalizing the notion of bilinear forms discussed above. This has the advantage of extending our discussion of bilinear and sesquilinear forms from earlier, remaining abstracted away from a specific basis while not too abstract.\n\n\n\n4.5.2 Multilinear forms\nLet \\(\\mathcal{V}\\) and \\(\\mathcal{U}\\) be two vector spaces. For any \\(u \\in \\mathcal{U}\\) and \\(w^* \\in \\mathcal{V}^*\\), the outer product \\(uw^*\\) is associated with a rank 1 map in \\(L(\\mathcal{V}, \\mathcal{U})\\) given by \\(v \\mapsto u w^* v\\). Alternately, we can see \\(uw^*\\) as a bilinear map from \\(\\mathcal{U}^* \\times\n\\mathcal{V}\\rightarrow \\mathbb{F}\\) given by \\((f^*, v) \\mapsto f^* u w^* v\\); or we can see \\(uw^*\\) as a dual map in \\(L(\\mathcal{U}^*, \\mathcal{V}^*)\\) given by \\(f^* \\mapsto f^* u w^*\\). We will use the uniform notation \\(u \\otimes w^*\\) to cover all of these possible interpretations of the outer product.\nMore generally, let \\(\\mathcal{V}_1, \\mathcal{V}_2, \\ldots, \\mathcal{V}_d\\) be vector spaces. Then for any vectors \\(v_1, v_2, \\ldots, v_d\\) in these spaces, the tensor product \\(v_1 \\otimes v_2 \\otimes \\ldots \\otimes v_d\\) can be interpreted as the multilinear map from \\(\\mathcal{V}_1^* \\times\n\\mathcal{V}_2^* \\times \\ldots \\mathcal{V}_d^* \\rightarrow \\mathbb{F}\\) given by \\[\n  (v_1 \\otimes v_2 \\otimes \\ldots \\otimes v_d)(w_1^*, w_2^*, \\ldots,\n  w_d^*) =\n  (w_1^* v_1) (w_2^* v_d) \\ldots (w_d^* v_d).\n\\] As in the case of just two vector spaces, we can define other maps by partial evaluation. For example, we can consider this as a map from \\(\\mathcal{V}_2^* \\times \\ldots \\times \\mathcal{V}_d^* \\rightarrow \\mathcal{V}_1\\) by \\[\n  (w_d^*, \\ldots, w_d^*) \\mapsto a(\\cdot, w_2^*, \\ldots, w_d^*)\n\\] where “filling in” all but the first argument gives an element of \\(\\mathcal{V}_1\\) (or technically of \\(\\mathcal{V}_1^{**}\\), but this is isomorphic10 to \\(\\mathcal{V}_1\\)).\nThe vector space of all multilinear forms \\(\\mathcal{V}_1^* \\times \\ldots\n\\mathcal{V}_d^* \\rightarrow \\mathbb{F}\\) is written as \\(\\mathcal{V}_1 \\otimes \\ldots \\otimes \\mathcal{V}_d\\). If \\(\\mathcal{V}_j\\) has a basis \\(V^{(j)}\\) and associated dual basis \\((W^{(j)})^*\\), then an arbitrary element \\(a \\in \\mathcal{V}_1 \\otimes \\ldots \\otimes\n\\mathcal{V}_d\\) can be written uniquely as \\[\n  a = \\sum_{i_1, \\ldots, i_d} a_{i_1 \\ldots i_d} \\, v_{i_1}^{(1)} \\otimes\n  \\ldots \\otimes v_{i_d}^{(d)}.\n\\] where \\[\n  a_{i_1 \\ldots i_d} = a(w_{i_1}^{(1)*}, \\ldots w_{i_d}^{(d)*}).\n\\] That is, the set of all tensor products of basis vectors for the spaces \\(\\mathcal{V}_j\\) form a basis for the tensor product space \\(\\mathcal{V}_1 \\otimes \\ldots \\otimes \\mathcal{V}_d\\).\nIn terms of the partial evaluation operation discussed above, we might consider \\(a\\) as a map from \\(\\mathcal{V}_2^* \\otimes \\ldots \\otimes\n\\mathcal{V}_d^*\\) to \\(\\mathcal{V}_1\\) by taking \\[\\begin{aligned}\n  b &= \\sum_{i_2, \\ldots, i_d} b_{i_2 \\ldots i_d} \\,\n  w_{i_2}^{(2)*} \\otimes w_{i_d}^{(d)*} \\\\\n  &\\mapsto\n  \\sum_{i_2, \\ldots, i_d} b_{i_2 \\ldots i_d} \\,\n  a(\\cdot, w_{i_2}^{(2)*}, \\ldots w_{i_d}^{(d)*}) \\\\\n  &=\n  \\sum_{i_1, i_2, \\ldots, i_d} v_{i_1}^{(1)} \\, a_{i_1 \\ldots i_d} b_{i_2 \\ldots i_d}.\n\\end{aligned}\\] This generalized partial evaluation operation is known as a tensor contraction. In general, we can contract two tensors on any pairs of “slots” in the form that take vectors from a dual pair of spaces.\nWhen a real inner product space \\(\\mathcal{V}\\) appears in a tensor, we can construct a new tensor in which \\(\\mathcal{V}^*\\) appears in the same slot by composing that slot with the inverse Riesz map (“lowering the index” in the physics parlance). Similarly, we can convert a \\(\\mathcal{V}^*\\) slot to a \\(\\mathcal{V}\\) slot via the Riesz map (“raising the index”). When a tensor is represented with respect to orthonormal bases, raising or lowering operations do not change any of the coefficients.\nIt is tempting to ask if the coefficients might be made simple by a special choices of the bases for the component spaces, analogous to the anonical forms we discussed before. Unfortunately, tensors beyond bilinear forms lack any nice canonical representation.\n\n\n4.5.3 Polynomial products\nAs an example of tensor product spaces in action, we consider the space of polynomials in three variables with maximum degree per variable of \\(d\\). In the power basis, we would write this as \\[\n  \\mathcal{P}_d \\otimes \\mathcal{P}_d \\otimes \\mathcal{P}_d =\n  \\left\\{ \\sum_{i,j,k} c_{ijk} x^i y^j z^k : c_{ijk} \\in \\mathbb{R}^{d \\times d\n  \\times d} \\right\\}.\n\\] Evaluating a polynomial \\(p \\in \\mathcal{P}_d \\otimes \\mathcal{P}_d \\otimes\n\\mathcal{P}_d\\) at a particular point \\((\\alpha, \\beta, \\gamma)\\) is associated with evaluation of a trilinear form, or equivalently with contraction against a tensor in \\(\\mathcal{P}_d^* \\otimes \\mathcal{P}_d^* \\otimes\n\\mathcal{P}_d^*\\); i.e. \\[\n  p \\cdot (\\delta_{\\alpha}^* \\otimes \\delta_{\\beta}^* \\otimes \\delta_{\\gamma}^*)\n\\] We can similarly think about contracting in some slots but not others, or contracting against other linear functionals (like integration).\nFor concrete tensors, contractions can also be used to express operations like changing basis. For example, if \\(T_{0:d} A = X_{0:d}\\), we can write \\[\\begin{aligned}\n  p(x,y,z) &= \\sum_{i,j,k} c_{ijk} \\, x^i y^j z^k \\\\\n  &= \\sum_{l,m,n} d_{lmn} \\, T_l(x) T_m(y) T_n(z) \\\\\n  d_{lmn} &=\n  \\sum_{i,j,k} c_{ijk} a_{li} a_{jm} a_{kn}\n\\end{aligned}\\] That is, we change bases by applying \\(A\\) on each “fiber” of the coefficient tensor (i.e. a vector associated with holding two indices fixed and letting the other vary). Indeed, the transformation from the \\(c_{ijk}\\) to the \\(d_{lmn}\\) coefficients is most easily coded in terms of such a matrix operation:\n\nlet\n    d = 2\n    C = rand(d+1,d+1,d+1)  # Power coeffs\n\n    # Transform to the Chebyshev coefficients\n    D = copy(C)\n    M = chebyshev_power_coeffs(d)  # A = inv(M)\n\n    # Contract with a_li\n    for j=1:d+1\n        for k = 1:d+1\n            D[:,j,k] = M\\D[:,j,k]\n        end\n    end\n\n    # Contract with a_mj\n    for i=1:d+1\n        for k=1:d+1\n            D[i,:,k] = M\\D[i,:,k]\n        end\n    end\n\n    # Contract with a_nk\n    for i=1:d+1\n        for j=1:d+1\n            D[i,j,:] = M\\D[i,j,:]\n        end\n    end\n\n    # Check correctness by comparing evaluations a random point\n    T = chebyshev_basis(d)\n    x,y,z = rand(3)\n    p1 = 0.0\n    p2 = 0.0\n    for i=1:d+1\n        for j=1:d+1\n            for k=1:d+1\n                p1 += C[i,j,k] * x^(i-1) * y^(j-1) * z^(k-1)\n                p2 += D[i,j,k] * T[i](x) * T[j](y) * T[k](z)\n            end\n        end\n    end\n    p1 ≈ p2\nend\n\ntrue\n\n\n\n\n4.5.4 Alternating forms\nAn alternating form is a multilinear form on several copies of the same vector space \\(\\mathcal{V}\\) with the property that it is zero if any input is repeated. This implies that swapping the order of any pair of arguments reverses the sign; for example, if \\(a : \\mathcal{V}\\times \\mathcal{V}\n\\times \\mathcal{V}\\rightarrow \\mathbb{F}\\) is a multilinear form, then \\[\\begin{aligned}\n  a(u, v, w) + a(w, v, u)\n  &= a(u, v, w) + a(u, v, u) + a(w, v, w) + a(w, v, u) \\\\\n  &= a(u, v, w+u) + a(w, v, w+u) \\\\\n  &= a(u+w, v, w+u) \\\\\n  &= 0.\n\\end{aligned}\\] The set of all alternating forms on \\(k\\) copies of \\(\\mathcal{V}\\) is written as \\(\\mathcal{V}^* \\wedge \\ldots \\wedge \\mathcal{V}^*\\), a subspace of \\(\\mathcal{V}^* \\otimes \\ldots \\otimes \\mathcal{V}^*\\).\nThe wedge product of two tensors over copies of the same space is \\[\n  f \\wedge g = f \\otimes g - g \\otimes f.\n\\] Alternating forms of a given number of arguments also form a vector space. For example, \\(\\mathcal{V}^* \\wedge \\mathcal{V}^* \\wedge \\mathcal{V}^*\\) has a basis of vectors \\(w_i^* \\wedge w_j^* \\wedge w_k^*\\) for each possible subset \\(\\{i, j, k\\}\\) of three distinct indices from the range \\(1, \\ldots, n\\). Hence, it has dimension \\(n\\) choose \\(3\\).\nIf \\(\\mathcal{V}\\) is \\(n\\)-dimensional, the space of \\(n\\) copies of \\(\\mathcal{V}^*\\) wedged together has \\(n\\) choose \\(n\\) dimensions — that is, there is only one alternating form on \\(n\\) inputs, up to scaling. We arbitrarily choose one such form by picking a basis \\(V\\) and letting \\(a(v_1, v_2, \\ldots, v_n) = 1\\) and declaring this to be the signed volume form. In most situations, \\(\\mathcal{V}\\) is an inner product space and we choose \\(V\\) to be an orthonormal basis.\n\n\n4.5.5 Determinants\nLet \\(f \\in \\mathcal{V}^* \\wedge \\ldots \\wedge \\mathcal{V}^*\\) be the signed volume form for a space \\(\\mathcal{V}\\). Rather than writing evaluation of \\(f\\) as \\(f(v_1, v_2, \\ldots, v_n)\\), we will collect the arguments into a quasimatrix and write \\(f(V)\\) for the volume associated with a parallelipiped whose edges are vectors \\(V\\).\nIf \\(U\\) is a basis such that \\(f(U) = 1\\), then we can write any other set of \\(n\\) vectors as \\(UA\\) for some matrix \\(A\\). The determinant of \\(A\\) is the function such that \\(f(UA) = f(U) \\det(A) = \\det(A)\\). Put differently, the determinant represents the change of volume in a mapping \\(L = UAU^{-1}\\) represented with respect to a basis quasimatrix \\(U\\) corresponding to a parallelipiped of unit (signed) volume. In order to satisfy this definition, the determinant must be\n\nAn alternating form on the concrete space \\(\\mathbb{F}^n \\otimes \\ldots\n\\otimes \\mathbb{F}^n\\) (written in terms of matrices \\(\\mathbb{F}^{n \\times n}\\))\nEqual to one for the identity matrix.\n\nThe interpretation of the determinant as representing a scaling of (signed) volumes explains various other properties, such as the important fact that \\(\\det(AB) = \\det(A) \\det(B)\\). It also explains why determinants arise in the change-of-variables formula for integration, which is one of the main places where we will see them.\nThe determinant of a singular matrix must be zero, which explains why we can write the characteristic polynomial for a matrix as \\(\\det(zI-A)\\). However, we prefer to treat both determinants and characteristic polynomials as interesting objects in their own right, and the connection through the determinant as a derived fact.\n\n\n\n\nAxler, Sheldon. 2024. Linear Algebra Done Right. Fourth. Springer.\n\n\nLax, Peter D. 2007. Linear Algebra and Its Applications. Second. Wiley.\n\n\nLay, David C., Steven R. Lay, and Judi J. McDonald. 2015. Linear Algebra and Its Applications. Fifth. Pearson.\n\n\nStrang, Gilbert. 2023. Introduction to Linear Algebra. Sixth. Wellesley-Cambridge Press.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "00-Background/03-LA.html#footnotes",
    "href": "00-Background/03-LA.html#footnotes",
    "title": "4  Linear Algebra",
    "section": "",
    "text": "“Vector spaces are like potato chips: you cannot have just one.” - W. Kahan↩︎\nIn infinite-dimensional settings, we will restrict our attention to continuous linear functionals (the continuous dual space) rather than all linear functionals (the algebraic dual space).↩︎\nThe set containing only the zero vector is the smallest subspace we ever talk about.↩︎\n“Caveat lector”: let the reader beware!↩︎\nPafnuty Chebyshev was a nineteenth century Russian mathematician, and his name has been transliterated from the Cyrillic alphabet into the Latin alphabet in several different ways. We inherit our usual spelling from one of the French transliterations, but the symbol \\(T\\) for the polynomials comes from the German transliteration Tschebyscheff.↩︎\nSome authors write \\(L(\\mathcal{V}, \\mathcal{U})\\) for the set of bounded linear maps between two spaces. This distinction matters when we are dealing with infinite-dimensional normed vector spaces — since we are mostly concerned with the finite-dimensional case, we will not worry about it.↩︎\nIn the absence of an inner product, the coimage is most naturally thought of as a subspace of \\(\\mathcal{V}^*\\) and the cokernel as a subspace of \\(\\mathcal{W}^*\\). The Riesz map allows us to move these back to subspaces of \\(\\mathcal{V}\\) and \\(\\mathcal{W}\\).↩︎\nThe notion of an induced norm makes sense in infinite-dimensional spaces as well, but there the supremum may not be achieved.↩︎\nA monic polynomial is one in which the coefficient in front of the highest-degree term is 1.↩︎\nThere is a canonical injection \\(\\mathcal{V}\\rightarrow \\mathcal{V}^{**}\\) via \\(v \\mapsto (w^* \\mapsto w^* v)\\). In finite-dimensional spaces, this is alway also surjective; in infinite-dimensional spaces, it is only surjective when the space is reflexive.↩︎",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html",
    "href": "00-Background/04-Calculus.html",
    "title": "5  Calculus and analysis",
    "section": "",
    "text": "5.1 Formulas and foundations\nWe assume the reader has standard introductory courses in single-variable and multivariable calculus. It will also be helpful to have seen some elements of analysis, including the \\(\\epsilon-\\delta\\) definition of continuity. Otherwise, it will be helpful to be familiar with the Julia programming language (Chapter 2) and with the contents of the previous chapter (Chapter 4).\nThe modern conception of “calculus” grew out of the 17th century work of Newton and Liebnitz. Many elements of calculus preceded Newton and Liebnitz; indeed, some arguments with a flavor of calculus were known to Archimedes and other Greek geometers. But Newton and Liebnitz created something new by producing “the Calculus, a general symbolic and systematic method of analytic operations, to be performed by strictly formal rules, independent of geometric meaning” (p. 84, Rosenthal 1951). The mechanical nature of calculus means that students and computers can routinely manipulate derivatives and integrals without constantly (or ever) having to think deeply about the geometry of the problem at hand. Indeed, the system of calculus has been generalized in many ways to situations that bear little resemblance to geometric problems at all, but still follow the formal patterns that make the machinery work.\nFormal calculations are power tools: easily dispatching intractable problems in the right hands, but dangerous when used incautiously. This problem is not unique to calculus: the road to “proofs” that \\(1 =\n0\\) is lined with use of the identity \\(xy/x = y\\) without checking if \\(x = 0\\). Such cautionary examples aside, many formal calculations produce valid results even when they have no right to do so, leaving mathematicians the challenge of figuring out what the “right” set of hypotheses really are.\nThe development of the calculus was a product of the seventeenth century, but the early formal system was somewhat unsound: the rules were missing important hypotheses, and could potentially lead to incorrect conclusions. Though concerns about the solidity of the foundations of calculus go back to the time of Newton and Liebnitz, many mathematicians were perfectly happy to restrict their attention to functions “nice enough” so that the rules of the calculus worked and get on with it1. The project to develop definitions and hypotheses to make the formal rules of calculus sound did not really get fully underway until the nineteenth century, with the work of Cauchy and even more of Weierstrass, the “father of modern analysis” and originator of the \\(\\epsilon-\\delta\\) definitions and arguments familiar to modern students of calculus and analysis.\nThe \\(\\epsilon-\\delta\\) world of Weierstrass has an asymptotic flavor: for a given \\(\\epsilon &gt; 0\\), we eventually get within \\(\\epsilon\\) of a target, for small enough \\(\\delta\\) for for large enough \\(N\\). Such arguments and definitions are often very convenient for establishing that a solution to some problem exists, or that it has certain properties. For numerical computation, though, we often want something stronger. Saying a sequence converges to a limit in the long run is all well and good — but fast as computers are, in the long run we are all dead2.\nIn our review of calculus and analysis, we will give a great deal of attention to the formal machinery of the calculus. But we also want some reassurance that our computations make sense, and so we will take some care to give a proper account of what regularity hypotheses are needed for our calculations to work. We will frequently be somewhat more conservative with these hypotheses than is strictly necessary. This is partly for convenience, but there is more to it. Numerical methods are inherently approximate and finite things, and so we would like strong enough hypotheses to ensure that our calculations become “right enough, fast enough” – preferably with quantifiable meanings for both “right enough” and “fast enough.”",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#metric-spaces",
    "href": "00-Background/04-Calculus.html#metric-spaces",
    "title": "5  Calculus and analysis",
    "section": "5.2 Metric spaces",
    "text": "5.2 Metric spaces\nWe will mostly be concerned with normed vector spaces, but sometimes we will want something a little more general. For the present section, we consider metric spaces and their structure, as this is the minimal structure that we really need for Weierstrass-style \\(\\epsilon-\\delta\\) arguments to make sense. A metric space is simply a set \\(X\\) with a distance function (a metric) \\(d : X \\times X \\rightarrow \\mathbb{R}\\) with three properties:\n\nSymmetry: \\(d(x,y) = d(y,x)\\);\nPositive definiteness: \\(d(x,y) \\geq 0\\), and \\(d(x,y) = 0\\) iff \\(x = y\\);\nTriangle inequality: \\(d(x,z) \\leq d(x,y) + d(y,z)\\).\n\nA normed vector space is a metric space where the distance is just the norm of the difference, i.e. \\(d(u,v) = \\|u-v\\|\\). But there are many other examples that are relevant, including the shortest distance between points in a network or on a manifold, and we will see these in our discussions of nonlinear dimensionality reduction and network analysis.\n\n5.2.1 Metric topology\nA metric space automatically comes with a metric topology3 where the open sets can be generated from unions of open balls \\[\nB_{\\delta}(x) = \\{y \\in X : d(x,y) &lt; \\delta\\}.\n\\] A set containing such a ball is sometimes called a “neighborhood” of \\(x\\). For any subset \\(\\Omega \\subset X\\), we say \\(x \\in \\Omega\\) is an interior point if \\(\\Omega\\) contains a neighborhood of \\(x\\); otherwise, it is a boundary point. The complements of open sets are closed sets. A closed set contains all its limit points; that is, if \\(C \\subset X\\) is closed, and for every \\(\\delta &gt; 0\\) there is a \\(y\n\\in C\\) with \\(d(x,y) &lt; \\delta\\), then \\(x\\) must also be in \\(C\\).\nThe idea of continuity of a function makes sense for any topological spaces, using a definition in terms of open sets: a function \\(f : X\n\\rightarrow Y\\) is continuous if for any open \\(U \\subset Y\\), the preimage \\(f^{-1}(U) = \\{ x \\in X : f(x) \\in U \\}\\) is also open. For metric spaces, this coincides with the more usual notion of continuity due to Weierstrass, i.e. that if \\(f(x) = y\\) then there is some neighborhood \\(B_\\delta(x)\\) in the preimage \\(f^{-1}(B_\\epsilon(y))\\) .\n\n\n5.2.2 Convergence\nWhen we want to numerically find the minimum or maximum of a function, or solve some system of equations, we generally have to deal with two questions:\n\nDoes the thing we want even exist?\nCan we find it (at least approximately), ideally in a reasonable amount of time?\n\nOne standard way to tackle both these questions is to construct a sequence of approximate solutions \\(x_1, x_2, \\ldots \\in X\\) in an appropriate metric space \\(X\\). The sequence converges to a limit \\(x_*\\) if for every neighborhood \\(U\\) of \\(x_*\\), the sequence eventually enters and stays in \\(U\\). Taking balls of radius \\(\\epsilon\\) as neighborhoods gives us the usual Weierstrass-style definition of convergence: \\[\n  \\forall \\epsilon &gt; 0, \\exists N : \\forall n \\geq N, d(x_n, x_*) &lt; \\epsilon.\n\\] If a sequence of approximate solutions \\(x_j\\) converges to a limit \\(x_*\\) and we have some property that guarantees that a limit of approximate solutions is an exact solution for the problem in hand, then this is a constructive way to get a handle on the thing we want.\nA convergent sequence of approximations gives us a way to compute arbitrarily good approximations to the limit \\(x_*\\) — eventually. But the word “eventually” is rarely a satisfactory answer to the question “when will we get there?” When we are focused on computation (as opposed to existence proofs), we typically seek a more quantitative answer, e.g. \\(d(x_n,x_*) \\leq \\gamma_n\\) where \\(\\gamma_n\\) is some sequence with well-defined convergence properties. Alternately, we might give some function \\(\\nu(\\epsilon)\\) such that \\(d(x_n, x_*) \\leq \\epsilon\\) for \\(n \\geq \\nu(\\epsilon)\\).\nFor example, we say \\(x_n\\) converges geometrically (or sometimes linearly) to \\(x_*\\) if \\(d(x_n,x_*) \\leq \\alpha^n d(x_0, x_*)\\) for some \\(0 \\leq \\alpha &lt; 1\\) (often called the rate constant). If \\(x_n\\) is geometrically convergent with rate \\(\\alpha &gt; 0\\), then we can guarantee \\(d(x_n,x_*) &lt; \\epsilon\\) for \\(n &gt; \\nu(\\epsilon) = \\left( \\log(\\epsilon) - \\log(d(x_0,x_*)) \\right) / \\log(\\alpha)\\).\n\n\n5.2.3 Completeness\nA Cauchy sequence \\(x_1, x_2, \\ldots\\) in a metric space \\(X\\) is a sequence such that for any \\(\\epsilon &gt; 0\\) there is an \\(N\\) such that for all \\(i, j \\geq N\\), \\(d(x_i,x_j) &lt; \\epsilon\\). We say Cauchy sequences \\(x_1, x_2, \\ldots\\) and \\(y_1, y_2, \\ldots\\) are equivalent if \\(d(x_i,y_i) \\rightarrow 0\\). We might believe that if there is any justice in the world, Cauchy sequences should converge, and equivalent Cauchy sequences should converge to the same thing. Unsurprisingly, though, sometimes we need to create our own justice.\nA complete metric space is one in which Cauchy sequences converge. Alas, not all metric spaces are complete. However, for any metric space \\(X\\) we can define a new metric space \\(\\bar{X}\\) whose elements are equivalence classes of Cauchy sequences in \\(X\\), with the metric given by \\(d(\\{x_i\\}, \\{y_i\\}) = \\lim_{i \\rightarrow \\infty}\nd(x_i,y_i)\\). There is also a canonical embedding of \\(X\\) into \\(\\bar{X}\\) which preserves the metric, given by taking \\(x \\in X\\) to the equivalence class associated with the constant sequence \\(x, x,\n\\ldots\\). The space \\(\\bar{X}\\) is the completion of \\(X\\) — and, by construction, it is complete, and every element of \\(\\bar{X}\\) can be expressed as a limit point for the embedding of \\(X\\) into \\(\\bar{X}\\).\nEvery finite-dimensional vector space over \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) is complete. There are also many complete infinite-dimensional vector spaces: for example, the space \\(C([a,b], \\mathbb{R})\\) of continuous real-valued functions on a closed interval \\([a,b]\\) is complete under the sup norm. However, there are also many examples of function spaces that are not complete, and in order to make analysis more tractable, we usually work with the completions of these spaces. A complete normed vector space is called a Banach space, and a complete inner product space is called a Hilbert space.\n\n\n5.2.4 Compactness\nIn \\(\\mathbb{R}^n\\) (or any finite-dimensional normed vector space), there is a special role for sets that are closed and bounded. We generalize this to the idea of compact sets. The standard definition of a set \\(K\\) being compact is that any open cover \\(C\\) (a collection of open sets such that every point in \\(Y\\) belongs to some \\(U \\in C\\)) has a finite subcover, i.e. \\(Y \\subset_{i=1}^n U_i\\) where each \\(U_i\\) belongs to the original cover \\(C\\). One sometimes considers covers consisting of open balls, in which case compactness of \\(K\\) is sometimes colloquiually rendered as “\\(K\\) can be guarded by a finite number of arbitrarily nearsighted watchmen.”\nCompact metric spaces have a number of useful properties. For example, the continuous image of a compact set is compact; this means, for example, that if \\(f : K \\rightarrow \\mathbb{R}\\) is continuous and \\(K\\) is compact, then \\(f(K)\\) is also compact — so closed and bounded (for \\(\\mathbb{R}\\)). Therefore, a continuous function on a compact set \\(K\\) always has a finite maximum and minimum that are achieved somewhere in \\(K\\). Compact metric spaces are also sequentially compact4, which means that if \\(K\\) is compact and \\(x_1, x_2, \\ldots\\) is a sequence in \\(K\\), then there is some subsequence \\(x_{j_1}, x_{j_2},\n\\ldots\\) that converges to a point in \\(K\\).\nIn a finite dimensional normed vector space, the closed unit ball \\(\\{v \\in \\mathcal{V}: \\|v\\| \\leq 1\\}\\) is always compact (since it is a continuous map of a closed and bounded set in \\(\\mathbb{R}^n\\) via a basis). One can use the fact that norms are continuous to show that this implies norm equivalence among finite-dimensional spaces, i.e. for any norms \\(\\|\\cdot\\|\\) and \\(\\|\\cdot\\|'\\) on a finite dimensional \\(\\mathcal{V}\\), there exist constants \\(0 &lt; m \\leq M &lt; \\infty\\) such that \\[\n  \\forall v \\in \\mathcal{V}, m \\|v\\|' \\leq \\|v\\| \\leq M \\|v\\|'.\n\\] The constants may be very big or small in general, but they exist! In contrast, for infinite dimensional spaces, the closed unit ball is generally not compact, and different norms give different topologies.\n\n\n5.2.5 Contractions\nOne of the more frequently-used theorems in numerical analysis is the Banach fixed point theorem, also known as the contraction mapping theorem.\n\nTheorem 5.1 (Banach fixed point theorem) Suppose \\(X\\) is a (non-empty) complete metric space and \\(f : X\n\\rightarrow X\\) is a contraction, i.e. there is some \\(0 \\leq \\alpha &lt;\n1\\) such that \\(d(f(x), f(y)) \\leq \\alpha d(x,y)\\). Then there exists a unique point \\(x_* \\in X\\) such that \\(f(x_*) = x_*\\).\n\n\nProof. Choose any \\(x_0 \\in X\\); there must be at least one since \\(X\\) is non-empty. Using \\(x_0\\) as a starting point, define the sequence \\(x_0,\nx_1, x_2, \\ldots\\) by the iteration \\(x_{k+1} = f(x_k)\\). By contractivity, \\(d(x_k, x_{k+1}) \\leq \\alpha^k d(x_0, x_1)\\). By the triangle inequality and a geometric series bound, for any \\(j &gt; i\\), \\[\nd(x_i, x_j)\n\\leq \\sum_{k=i}^{j-1} d(x_k, x_{k+1})\n\\leq \\sum_{k=i}^{j-1} \\alpha^k d(x_0,x_1)\n\\leq \\alpha^i \\frac{d(x_0,x_1)}{1-\\alpha}.\n\\] Therefore, for any \\(\\epsilon &gt; 0\\) we can choose an \\(N\\) such that for all \\(i,j &gt; N\\), \\(d(x_i,x_j) \\leq \\alpha^n d(x_0,x_1)/(1-\\alpha) &lt;\n\\epsilon\\), i.e. the iterates \\(x_j\\) form a Cauchy sequence, which must converge to some \\(x_* \\in X\\) by completeness.\nNow suppose \\(x_*' \\in X\\) satisfies \\(x_*' = f(x_*')\\). Then by contractivity and the fixed point condition, \\[\n  d(x_*, x_*') = d(f(x_*), f(x_*')) &lt; \\alpha d(x_*, x_*'),\n\\] which can only happen if \\(d(x_*, x_*') = 0\\). Therefore \\(x_*\\) and \\(x_*'\\) must be the same point.\n\nThe Banach fixed point theorem is useful because it gives us not only the existence of a fixed point, but also a method to compute arbitrarily good approximations to that fixed point with a rate of convergence. Consequently, this theorem is a mainstay in the convergence analysis of many iterative numerical methods.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#continuity-and-beyond",
    "href": "00-Background/04-Calculus.html#continuity-and-beyond",
    "title": "5  Calculus and analysis",
    "section": "5.3 Continuity and beyond",
    "text": "5.3 Continuity and beyond\nThe study of real analysis often devolves into a study of counterexamples: we start with something that seems like it should be true about a function, give an example where it is not, and then come up with a hypothesis under which it is true. While we make frequently resort to assuming that everything is “as nice as possible” (several times continuously differentiable, at least), it is worth a page or two to talk about more modest hypotheses and what they give us. In particular:\n\nContinuity gives us the intermediate value theorem.\nUniform continuity gives us a set of functions that is closed under (uniform) limits.\nAbsolute continuity gives us the fundamental theorem of calculus.\nBounded variation gives us a dual space to continuous functions.\n\nFor stronger control that can be used to prove approximation bounds, we might turn Lipschitz continuity (or a more general modulus of continuity).\nWe will focus on functions between normed vector spaces. But almost everything in this section, and indeed in this chapter, generalizes beyond this setting.\n\n5.3.1 Continuity\nWhen we compute, we almost always presume functions that are continuous almost everywhere. Floating point cannot exactly represent every real number, and so when we evaluate a function from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) on the computer, it helps if the small changes to the input (and the output) can be forgiven.\nLet \\(\\mathcal{V}\\) and \\(\\mathcal{W}\\) are normed vector spaces, and suppose \\(\\Omega\n\\subset \\mathcal{V}\\). We say \\(f : \\Omega \\subset \\mathcal{V}\\rightarrow \\mathcal{W}\\) is continuous at \\(x \\in \\Omega\\) if \\[\n  \\forall \\epsilon &gt; 0, \\exists \\delta &gt; 0 : \\forall y \\in \\Omega,\n  \\|x-y\\| \\leq \\delta \\implies \\|f(y)-f(x)\\| \\leq \\epsilon\n\\] In words: for any tolerance \\(\\epsilon\\) there is a corresponding tolerance \\(\\delta\\) so that getting \\(y\\) within \\(\\delta\\) of \\(x\\) means we will get \\(f(y)\\) within \\(\\epsilon\\) of \\(f(x)\\). Composition of continuous functions is continuous: if \\(g\\) is continuous at \\(f(x)\\) and \\(f\\) is continuous at \\(x\\), then \\(g \\circ f\\) is continuous at \\(x\\).\nA function is continuous on \\(\\Omega\\) if it is continuous at each point in \\(\\Omega\\). The set of such continuous functions from \\(\\Omega\n\\subset \\mathcal{V}\\) to \\(\\mathcal{W}\\) is called \\(C(\\Omega, \\mathcal{W})\\). The continuous functions \\(C(\\Omega, \\mathcal{W})\\) form a vector space: sums of continuous functions are continuous, and so are scalar multiples of continuous functions.\nFor functions from \\([a,b] \\subset \\mathbb{R}\\) to \\(\\mathbb{R}\\), the intermediate value theorem is a fundamental result: if \\(x,y \\in [a,b]\\) take on values \\(f(x)\\) and \\(f(y)\\), then for every target value \\(f_* \\in (f(x), f(y))\\), there is a \\(z \\in (x,y)\\) such that \\(f(z) = f_*\\). The intermediate value theorem is fundamentally one-dimensional, but still tells us useful things about functions between vector spaces through composition: if \\(f : \\Omega \\subset \\mathcal{V}\\rightarrow \\mathcal{W}\\) and \\(w^* \\in \\mathcal{W}^*\\) is any (bounded) linear functional, then, the function \\(s \\in [0,1] \\mapsto w^* f((1-s)x + sy)\\) is a continuous real-valued function on the interval \\([0,1]\\) for which the intermediate value theorem applies.\n\n\n5.3.2 Uniform continuity\nA function is uniformly continuous if the same \\(\\epsilon\\)-\\(\\delta\\) relation works for every \\(x\\) in \\(\\Omega\\). In a finite-dimensional space, any continuous function on a closed and bounded \\(\\Omega\\) is automatically also uniformly continuous. A sequence of functions is uniformly equicontinuous if the same \\(\\epsilon\\)-\\(\\delta\\) relation works for every \\(x \\in \\Omega\\) and for every function in the sequence.\nWe say that \\(f_k \\rightarrow f\\) uniformly if for all \\(\\epsilon &gt; 0\\) there is an \\(N\\) such that for \\(k &gt; N\\) we must have \\(\\|f_k-f\\|_{\\infty} \\leq \\epsilon\\), where \\(\\|f_k-f\\|_{\\infty} = \\sup_{x \\in \\Omega} \\|f_k(x)-f(x)\\|\\). This matters because while a pointwise limit of continuous functions does not have to be continuous, a uniform limit of uniformly continuous functions is uniformly continuous.\nIf \\(f : [a,b] \\subset \\mathbb{R}\\rightarrow \\mathbb{R}\\) is continuous (and therefore uniformly continuous), the Weierstrass approximation theorem says that \\(f\\) can be written as the uniform limit of a sequence of polynomials.\n\n\n5.3.3 Absolute continuity\nWe say \\(f : \\Omega \\subset \\mathbb{R}\\rightarrow \\mathbb{R}\\) is absolutely continuous on an interval if \\[\\begin{aligned}\n  \\forall \\epsilon &gt; 0, \\exists \\delta &gt; 0 : &\n  \\forall \\{(x_i, y_i) \\subset \\Omega\\}_{i=1}^n \\mbox{ disjoint}, \\\\\n  &\\sum_{i=1}^n |x_i-y_i| &lt; \\delta \\implies\n   \\sum_{i=1}^n |f(x_i)-f(y_i)| &lt; \\epsilon.\n\\end{aligned}\\] Absolutely continuous functions are those that are nice enough that the (Lebesgue) fundamental theorem of calculus applies; that is, the function \\(f\\) has derivatives almost everywhere, and \\[\n  f(b) = f(a) + \\int_a^b f'(x) \\, dx.\n\\] Most (perhaps all) continuous functions that we will encounter in this class will be absolutely continuous.\n\n\n5.3.4 Bounded variation\nThe total variation of a function \\(f : [a,b] \\rightarrow \\mathbb{R}\\) is a supremum over partitions \\(a = x_0 &lt; x_1 &lt; \\ldots &lt; x_{n_{\\mathcal{P}}} = b\\) of how much the function value jumps: \\[\n  V_a^b = \\sup_{\\mathcal{P}} \\sum_{i=0}^{n_{\\mathcal{P}}-1} |f(x_{i+1})-f(x_i)|.\n\\] For continuously differentiable functions, the total variation is the integral of the absolute value of the derivative. The notion generalizes to functions on more general vector spaces, though it is somewhat more technical and involves a little measure theory.\nBounded variation functions (those with finite total variation) are those that are nice enough that they can be integrated against continuous functions (in the Riemann sense). Indeed, bounded variation functions correspond to the dual space to \\(C[a,b]\\): every linear functional \\(w^* \\in (C[a,b])^*\\) can be written as \\(w^* f = \\int_a^b f(t) \\, dv(t)\\) where \\(v\\) is a bounded variation function and \\(dv(t)\\) is its (distributional) derivative.\n\n\n5.3.5 Lipschitz continuity\nA function \\(f : \\Omega \\subset \\mathcal{V}\\rightarrow \\mathcal{W}\\) is Lipschitz continuous with constant \\(C\\) if \\(\\forall x, y \\in \\Omega\\), \\[\n  \\|f(x)-f(y)\\| \\leq C\\|x-y\\|.\n\\] Unlike other notions of continuity we have considered, Lipschitz continuity involves no explicit \\(\\epsilon-\\delta\\) relationship. Also unlike other notions of continuity, Lipschitz continuity gives us control on the relationship between function values even at points that may be far from each other.\n\n\n5.3.6 Modulus of continuity\nWe have already mentioned the classic theorem of Weierstrass says that a continuous real-valued function \\(f\\) on a finite closed interval \\([a,b]\\) can be approximated arbitrarily well in the \\(L^\\infty\\) sense, i.e. \\[\n  \\forall \\epsilon &gt; 0, \\exists p \\in \\mathcal{P}_d : \\|f-p\\|_\\infty \\leq \\epsilon,\n\\] where \\(\\|f-p\\|_\\infty = \\max_{x \\in [a,b]} |f(x)-p(x)|\\). But while this is a useful theorem, it lacks the level of detail we would like if we are trying to compute. What degree polynomial is needed for a particular \\(\\epsilon\\)? How do we construct such a polynomial? We will return to some of these points later in the book, but for now we simply want to make the point that we usually want more information about our functions than simply that they are continuous. In particular, we would like to be able to say something more specific about a relationship between \\(\\epsilon\\) and \\(\\delta\\), either close to a particular point or more globally, e.g. making a statement of the form \\[\n  \\|f(x)-f(y)\\| \\leq g(x-y)\n\\] for appropriate \\(x\\) and close enough \\(y\\), where \\(g\\) is some well-understood real-valued gauge function. One example of this is Lipschitz continuity, but it useful to generalize to other gauge functions.\nThe modulus of continuity for a function \\(f\\) on some domain \\(\\Omega\\) is a non-decreasing function \\(\\omega(\\delta)\\) for \\(\\delta &gt; 0\\) given by \\[\n  \\omega(\\delta) = \\sup_{x, y \\in \\Omega : \\|x-y\\| \\leq \\delta} \\|f(x)-f(y)\\|.\n\\] Put slightly differently, the modulus of continuity is the minimal function such that \\[\n  f(x+d) = f(x) + r(x,d), \\quad \\|r(x,d)\\| \\leq \\omega(\\|d\\|).\n\\] for any \\(x\\) and \\(x+d\\) in \\(\\Omega\\). A function \\(f\\) is uniformly continuous iff \\(\\omega(\\delta) \\rightarrow 0\\) as \\(\\delta \\rightarrow 0\\).\nSeveral standard rules for differentiation have analogous inequalities involving the modulus of continuity. For example, if \\(f : \\Omega\n\\rightarrow \\mathcal{V}\\) and \\(g : \\Omega \\rightarrow \\mathcal{V}\\) are functions with moduli of continuity \\(\\omega_f\\) and \\(\\omega_g\\), then\n\n\\(f+g\\) has modulus of continuity \\(\\omega_{f+g}(\\delta) \\leq \\omega_f(\\delta) + \\omega_g(\\delta)\\).\n\\(\\alpha f\\) has modulus of continuity \\(\\omega_{\\alpha f}(\\delta) \\leq\n|\\alpha| \\omega_f(\\delta)\\)\nIf \\(w^* \\in \\mathcal{V}^*\\), then \\(\\omega_{w^* f}(\\delta) \\leq \\|w^*\\|\n\\omega_f(\\delta)\\)\nIf \\(\\mathcal{V}\\) is an inner product space, \\(x \\mapsto \\langle f(x), g(x)\n\\rangle\\) has modulus of continuity \\(\\omega_{fg}(\\delta) \\leq\n\\omega_f(\\delta) \\|g\\|_\\infty + \\|f\\|_\\infty \\omega_g(\\delta)\\) where \\(\\|g\\|_\\infty = \\sup_{x \\in \\Omega} \\|g(x)\\|\\) and similarly with \\(\\|f\\|_\\infty\\).\n\nIf \\(f : \\Omega \\rightarrow \\mathbb{R}\\), and \\(\\beta = \\inf_{x \\in \\Omega}\n|f(x)|\\), we also have \\(\\omega_{1/f}(\\delta) \\leq \\omega_f(\\delta)/\\beta^2\\). Finally, if \\(f : \\Omega \\rightarrow \\mathcal{V}\\) and \\(g : f(\\Omega)\n\\rightarrow \\mathcal{W}\\), we have \\[\n  \\omega_{g \\circ f}(\\delta) \\leq \\omega_g(\\omega_f(\\delta)).\n\\] The proof of these facts is left as an exercise for the reader.\nA function \\(f\\) is Lipschitz continuous on \\(\\Omega\\) if the modulus of continuity on \\(\\Omega\\) satisfies \\(\\omega_f(\\delta) \\leq C\\delta\\) for some \\(C\\). Put differently, a Lipschitz function satisfies \\[\n  \\forall x, y \\in \\Omega, \\|f(x)-f(y)\\| \\leq C \\|x-y\\|.\n\\] The constant \\(C\\) is a Lipschitz constant for \\(f\\).\nA more general notion than Lipschitz continuity is \\(\\alpha\\)-Hölder continuity, which is the condition that \\(\\omega_f(\\delta) \\leq\nC\\delta^\\alpha\\) This condition is interesting for \\(0 &lt; \\alpha \\leq 1\\), with \\(\\alpha = 1\\) corresponding to Lipschitz continuity. The only \\(\\alpha\\)-Hölder continuous functions for \\(\\alpha &gt; 1\\) are the constant functions.\nReturning to our discussion of approximating continuous real-valued functions on \\([a,b]\\) by polynomials, Jackson’s theorem provides a quantitative bound on the optimal error in terms of the modulus of continuity. If \\(f\\) is a continuous function on \\([a,b]\\), Jackson’s theorem gives that there exists a polynomial of degree \\(n\\) with \\[\n  \\|f-p\\|_\\infty \\leq 6 \\omega \\left( \\frac{b-a}{n} \\right).\n\\] We can directly substitute the bounds from the definitions of Lipschitz or \\(\\alpha\\)-Hölder continuous functions to get bounds for those cases.\n\n\n5.3.7 Order notation\nThe modulus of continuity provides global control over functions on some domain. However, sometimes we are satisfied with much more local notions of control. To express these, it is useful to again turn to order notation, which we visited briefly in Section 3.2. The same notation can be used to compare functions \\(f(n)\\) and \\(g(n)\\) in a limit as \\(n \\rightarrow \\infty\\) or to compare functions \\(f(\\epsilon)\\) and \\(g(\\epsilon)\\) in the limit as \\(\\epsilon \\rightarrow 0\\). We are typically interested in upper bounds when dealing with calculus; that is, we care about\n\n\\(f(\\epsilon) = O(g(\\epsilon))\\), i.e. \\(\\exists C &gt; 0, \\rho &gt; 0\\) s.t. \\(\\forall\n\\epsilon &lt; \\rho\\), \\(|f(\\epsilon)| &lt; Cg(\\epsilon)\\).\n\\(f(\\epsilon) = o(g(\\epsilon))\\), i.e. \\(\\forall C &gt; 0, \\exists \\rho &gt;\n0\\) s.t. \\(\\forall \\epsilon &lt; \\rho\\), \\(|f(\\epsilon)| &lt; Cg(\\epsilon)\\).\n\nFrom a less notationally messy perspective, we have \\[\n  \\lim \\sup_{\\epsilon \\rightarrow 0} |f(\\epsilon)|/g(\\epsilon) =\n  \\begin{cases}\n    C \\geq 0, & \\mbox{ for } f(\\epsilon) = O(g(\\epsilon)) \\\\\n    0, & \\mbox{ for } f(\\epsilon) = o(g(\\epsilon)),\n  \\end{cases}\n\\] and when \\(\\lim_{\\epsilon \\rightarrow 0} f(\\epsilon)/g(\\epsilon) = C \\neq 0\\), we say \\(f(\\epsilon) = \\Theta(g(\\epsilon))\\). When \\(f : \\mathbb{R}\\rightarrow \\mathcal{V}\\) for some normed space \\(\\mathcal{V}\\), we abuse order notation slightly to write \\[\n  \\lim \\sup_{\\epsilon \\rightarrow 0} \\|f(\\epsilon)\\|/g(\\epsilon) =\n  \\begin{cases}\n    C \\geq 0, & \\mbox{ for } f(\\epsilon) = O(g(\\epsilon)) \\\\\n    0, & \\mbox{ for } f(\\epsilon) = o(g(\\epsilon)),\n  \\end{cases}\n\\]\nWe are frequently interested in the case of functions that are \\(O(\\epsilon^r)\\) for some integer or rational \\(r\\). If \\(f(\\epsilon) = \\Theta(\\epsilon^r)\\) is positive, then near zero we expect \\(\\log(f(\\epsilon)) \\approx \\log(C\\epsilon^r) = r\n\\log(\\epsilon) + \\log(C)\\). We can check this condition for small \\(\\epsilon\\) graphically with a log-log plot:\n\nlet\n    # Should be O(ϵ^{1/2})\n    f(ϵ) = maximum(roots(Polynomial([-ϵ, 0, 1, 1])))\n\n    ϵs = 10.0.^(-10:-1)\n    fs = f.(ϵs)\n    println(\"Approximate slope: \",\n             (log(fs[2])-log(fs[1]))/\n             (log(ϵs[2])-log(ϵs[1])) )\n    plot(ϵs, fs, xscale=:log10, yscale=:log10)\nend\n\nApproximate slope: 0.4999953048691424\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis type of plot is, of course, somewhat heuristic: the asymptotic behavior as \\(\\epsilon \\rightarrow 0\\) should manifest for small enough \\(\\epsilon\\), but nothing says that \\(10^{-10}\\) (for example) is “small enough.” We can be more rigorous by including additional information, as we will discuss in later chapters.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#derivatives",
    "href": "00-Background/04-Calculus.html#derivatives",
    "title": "5  Calculus and analysis",
    "section": "5.4 Derivatives",
    "text": "5.4 Derivatives\nWe assume the reader is thoroughly familiar with differentiation in one dimension. Our focus in the rest of this section is on differentiation on functions between vector spaces.\n\n5.4.1 Gateaux and Frechet\n\n5.4.1.1 Functions from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\)\nThe standard definition for the derivative of \\(f : \\mathbb{R}\\rightarrow \\mathbb{R}\\) is \\[\n  f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}.\n\\] Alternately, we can write \\(f\\) as \\[\n  f(x+h) = f(x) + f'(x) h + o(|h|).\n\\] Of course, not all functions are differentiable. When \\(f\\) is differentiable at \\(x\\), we will sometimes also write \\(Df(x)\\) to denote the derivative.\nWhen the derivative exists and is continuous on some interval \\((a,b)\\) containing \\(x\\), we say \\(f\\) is \\(C^1\\) on \\((a,b)\\). In this case, the fundamental theorem of calculus gives that for any \\(x+h \\in (a,b)\\), \\[\\begin{aligned}\n  f(x+h)\n  &= f(x) + \\int_0^h f'(x+\\xi) \\, d\\xi \\\\\n  &= f(x) + f'(x)h + r(h) \\\\\n  r(h) &= \\int_0^h \\left( f'(x+\\xi)-f'(x) \\right) \\, d\\xi.\n\\end{aligned}\\] Here \\(r(h)\\) is the remainder term for the first-order approximation \\(f(x) + f(x)h\\). If \\(\\omega_{f'}\\) is the modulus of continuity for \\(f'\\) on \\((a,b)\\), then \\[\n  \\|f'(x+\\xi)-f'(x)\\| \\leq \\omega_{f'}(|\\xi|),\n\\] and therefore \\[\n  |r(h)| \\leq \\int_0^{|h|} \\omega_{f'}(\\xi) \\, d\\xi.\n\\] When \\(f'\\) is Lipschitz with constant \\(C\\), we have \\[\n  |r(h)| \\leq \\int_0^{|h|} \\omega_{f'}(|\\xi|) \\, d\\xi\n  \\leq \\int_0^{|h|} C\\xi \\, d\\xi = \\frac{1}{2} Ch^2.\n\\] Hence, in regions where \\(f'\\) is not only continuous but Lipschitz, the error \\(r(h)\\) in the linearized approximation is not only \\(o(|h|)\\), but it is even \\(O(h^2)\\).\nWhen a differentiable function \\(f\\) is only available by evaluation, we can approximate the derivative at \\(x\\) by finite differences, e.g. taking \\[\n  f'(x) \\approx \\frac{f(x+h)-f(x)}{h}\n\\] where \\(h\\) is “small enough.” More precisely, if \\(f\\) is \\(C^1\\) on an interval containing \\((a,b)\\), we have \\[\n  \\frac{f(x)+f'(x)h + r(h) - f(x)}{h} = f'(x) + \\frac{r(h)}{h},\n\\] where \\(r\\) is the remainder for the linear approximation about \\(x\\). When \\(f'\\) is Lipschitz with constant \\(C\\), the error in this finite difference approximation is therefore \\[\n  \\frac{|r(h)|}{|h|} \\leq \\frac{1}{2} C|h| = O(|h|).\n\\] We further analyze how the size of \\(h\\) affects accuracy, as well as other methods for approximating derivatives, in Chapter 12. For this chapter, though, it will still be useful to sanity check derivatives via finite differences:\n\nfinite_diff(f, x; h=1e-8) = (f(x+h)-f(x))/h\n\n\n\n5.4.1.2 Functions from \\(\\mathbb{R}\\) to \\(\\mathcal{W}\\)\nSuppose \\(g : \\mathbb{R}\\rightarrow \\mathcal{W}\\) for some normed linear space \\(\\mathcal{W}\\). We can define the derivative of \\(g\\) almost identically to the derivative for a function from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\), i.e. \\[\n  g(x+h) = g(x) + g'(x) h + r(h), \\quad \\|r(h)\\| = o(|h|).\n\\] The only thing that differs is that we are controlling the norm of \\(r(h)\\) rather than the absolute value. Also similarly to the real case, if \\(g'\\) is Lipschitz with constant \\(C\\), then \\[\n  \\|r(h)\\| \\leq \\frac{1}{2} Ch^2,\n\\] and we can bound the error in simple finite difference estimates of the derivative as we do in the case of functions from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\).\nThere are a handful of theorems that are specific to real-valued functions, like the mean value theorem (Section 5.4.2). These theorems usually still tell us interesting things about vector valued functions by considering functions \\(x \\mapsto w^* g(x)\\) where \\(w^* \\in \\mathcal{W}^*\\) is some dual vector.\n\n\n5.4.1.3 Functions from \\(\\mathcal{V}\\) to \\(\\mathbb{R}\\)\nSuppose \\(f : \\mathcal{V}\\rightarrow \\mathbb{R}\\) for some normed linear space \\(\\mathcal{V}\\). Then for any point \\(x \\in \\mathcal{V}\\) and nonzero vector \\(u \\in \\mathcal{V}\\), we can define a function \\(f \\circ c\\) from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) that evaluates \\(f\\) along a ray \\(c_{x,u}(s) = x+su\\). Assuming it exists, the derivative \\((f \\circ c_{x,u})'\\) is the Gateaux derivative (directional derivative) of \\(f\\) at a point \\(x\\) and in a direction \\(u\\): \\[\n  D[f(x); u] = (f \\circ c_{x,u})'(0).\n\\] Alternately, the Gateaux derivative is the number such that \\[\n  f(x+su) = f(x) + s D[f(x); u] + o(s).\n\\] We can estimate the Gateaux derivative by finite differencing in the given direction\n\nfinite_diff(f, x, u; h=1e-8) = (f(x+h*u)-f(x))/h\n\nWhen the Gateaux derivative of \\(f\\) at \\(x\\) is defined for all \\(u \\in \\mathcal{V}\\), we say \\(f\\) is Gateaux differentiable at \\(x\\).\nWhen all the Gateaux derivatives are continuously defined in a neighborhood of \\(v\\), we say \\(f\\) is \\(C^1\\) on some open set containing \\(v\\). In this case, Gateaux derivatives are related by the Frechet derivative, a functional \\(f'(x) \\in \\mathcal{V}^*\\) (also written \\(Df(x)\\)) such that \\[\n  D[f(x); u] = f'(x) u\n\\] or, equivalently, for any direction \\(u\\), \\[\n  f(x+su) = f(x) + sf'(x) u + o(s).\n\\] When the Frechet derivative is Lipschitz with some constant \\(C\\) in a consistent norm, we can bound the remainder term by \\(Cs^2/2\\), as in the one-dimensional case. Frechet differentiability is a stronger condition than Gateaux differentiability. When we say a function is “differentiable at \\(x\\)” without qualifications, we mean it is Frechet differentiable.\nThe Frechet derivative (or just “the derivative”) \\(f'(x)\\) is an element in \\(\\mathcal{V}^*\\). In a (real) inner product space, the gradient \\(\\nabla f(x) \\in \\mathcal{V}\\) is the dual of \\(f'(x)\\) given by the Riesz map, i.e. \\[\n  f'(x) u = \\langle \\nabla f(x), u \\rangle.\n\\] In \\(\\mathbb{R}^n\\) with the standard inner product, the gradient (a column vector) is the transpose of the derivative (a row vector).\n\n\n5.4.1.4 A polynomial example\nFollowing our pattern from the previous chapter, we consider an example in a polynomial space. Let \\(f : \\mathcal{P}_d \\rightarrow \\mathbb{R}\\) be given by \\(f(p) = \\frac{1}{2} \\int_{-1}^1 p(x)^2 \\, dx\\). The Frechet derivative of \\(f\\) (which is a function of the polynomial \\(p \\in \\mathcal{P}_d\\), not the indeterminate \\(x\\)) is then the functional \\(f'(p)\\) such that the action on a vector \\(q \\in \\mathcal{P}_d\\) is \\[\n  f'(p)q = \\int_{-1}^1 p(x) q(x) \\, dx.\n\\] With respect to the power basis, we can write \\(f'(q)\\) in terms of the row vector \\[\n  d^T = \\begin{bmatrix} f'(p) x^0 & f'(p) x^1 & \\ldots & f'(p) x^d \\end{bmatrix}.\n\\] The gradient with respect to the \\(L^2([-1, 1])\\) inner product is then \\[\n  \\nabla f(p) = X_{0:d} M^{-1} d\n\\] where \\(M\\) is the Gram matrix for the power basis.\nWe can compute with \\(f'(p)\\) by calling the integrate command or through the power basis. We can also compute with \\(f'(p)\\) by writing it in terms of a basis of \\(\\mathcal{P}_d^*\\) in terms of point evaluation functionals, i.e. \\[\n  f'(p) = c^T W^*, \\quad W^* = \\begin{bmatrix} \\delta_{x_0}^* \\\\ \\vdots \\\\ \\delta_{x_d}^* \\end{bmatrix}.\n\\] We can also compute the \\(c\\) coefficients by solving \\[\n  c^T A = d^T\n\\] where \\(A\\) is the Vandermonde matrix \\[\n  A = W^* X_{0:d} =\n  \\begin{bmatrix}\n    1 & x_0^1 & \\ldots & x_0^d \\\\\n    1 & x_1^1 & \\ldots & x_1^d \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_d^1 & \\ldots & x_d^d\n  \\end{bmatrix}.\n\\]\nWe can sanity check this against finite differences for a random pair of polynomials:\n\nlet\n    # Functions for f and the Frechet derivative f'(p)\n    f(p) = integrate(0.5*p*p, -1, 1)\n    Df(p) = q -&gt; integrate(p*q, -1, 1)\n\n    # Express f'(p) in the power basis\n    d(p) = [Df(p).(Polynomial(ej)) for ej in eachcol(Matrix(I,4,4))]'\n\n    # Gram matrix for the L2 inner product\n    m(k) = k % 2 == 1 ? 0.0 : 2/(k+1)\n    M = [m(i+j) for i=0:3, j=0:3]\n\n    # Compute the gradient\n    ∇f(p) = Polynomial(M\\d(p)')\n\n    # Vandermonde matrix A for four equispaced points\n    x = range(-1, 1, length=4)\n    A = [xi^j for xi=x, j=0:3]\n\n    # Compute c in terms of point evaluations\n    c(p) = d(p)/A\n\n    # Test everything\n    ptest = Polynomial(rand(4))\n    qtest = Polynomial(rand(4))\n    Dfpq_fd = finite_diff(f, ptest, qtest)     # Finite diff\n    Dfpq1 = Df(ptest)(qtest)                   # Analytic form\n    Dfpq2 = c(ptest)*qtest.(x)                 # Via evaluation fnls\n    Dfpq3 = integrate(∇f(ptest)*qtest, -1, 1)  # Via gradient\n\n    # Compare -- everything should be equal (some approx for fd)\n    isapprox(Dfpq1, Dfpq_fd, rtol=1e-6) &&\n    Dfpq1 ≈ Dfpq2 &&\n    Dfpq1 ≈ Dfpq3\n\nend\n\ntrue\n\n\n\n\n5.4.1.5 General mappings\nNow consider \\(f : \\mathcal{V}\\rightarrow \\mathcal{W}\\) where \\(\\mathcal{V}\\) and \\(\\mathcal{W}\\) are normed linear spaces. In this case, the Gateaux derivative in a direction \\(u \\in \\mathcal{V}\\) is a vector in \\(\\mathcal{W}\\), still given as \\[\n  D[f(v); u] = (f \\circ c_{v,u})'(0) \\mbox{ for }\n  c(s) = x+su\n\\] and when there is a Frechet derivative \\(Df(v) = f'(v) \\in L(\\mathcal{V}, \\mathcal{W})\\) we have \\[\n  f(v+u) = f(v) + f'(v) u + o(\\|u\\|),\n\\] and the Gateaux derivatives satisfy \\[\n  D[f(v); u] = f'(v) u.\n\\] When the Frechet derivative is Lipschitz with constant \\(C\\) (with respect to the induced norm), then the remainder for the linear approximation is bounded by \\(C\\|u\\|^2/2\\). The representation of the Frechet derivative with respect to a particular basis is the Jacobian matrix \\(J = W^{-1} f'(p) V\\).\n\n\n\n5.4.2 Mean values\nFor one-dimensional continuously differentiable functions \\(f : \\Omega \\subset \\mathbb{R}\\rightarrow \\mathbb{R}\\) for some open connected \\(\\Omega\\), the fundamental theorem of calculus tells us that for \\(a, b\n\\in \\Omega\\) we have \\[\n  f(b)-f(a)\n  = \\int_0^1 f'(zb + (1-z)a)(b-a) \\, dz.\n\\] The mean value theorem tells us that for some intermediate \\(c \\in [a,b]\\) (\\(c = \\xi b + (1-\\xi) a\\) for \\(\\xi \\in [0,1]\\)), we have \\[\\begin{aligned}\n  f'(c)(b-a) = \\int_0^1 f'(zb + (1-z)a)(b-a) \\, dz.\n\\end{aligned}\\] The fundamental theorem of calculus holds even when \\(f\\) has discontinuities in the derivative at some points, but the second equation relies on continuity of the derivative. This useful theorem generalizes to the vector space case in various ways.\nFor continuously differentiable functions \\(f : \\Omega \\subset \\mathcal{V}\n\\rightarrow \\mathbb{R}\\) where \\(\\Omega\\) is open and convex (i.e. the line segment connecting \\(a\\) and \\(b\\) lies within \\(\\Omega\\)), we have \\[\\begin{aligned}\n  f(b)-f(a)\n  &= \\int_0^1 f'(zb + (1-z)a) (b-a) \\, dz \\\\\n  &= f'(\\xi b + (1-\\xi) a) (b-a).\n\\end{aligned}\\] for some \\(\\xi \\in [0,1]\\). And for continuously differentiable functions \\(g : \\Omega \\subset \\mathcal{V}\\rightarrow \\mathcal{W}\\) where \\(\\Omega\\) is open and convex, we have that for any \\(w^* \\in\n\\mathcal{V}^*\\) we can apply the mean value theorem to the real-valued \\(f(v) =\nw^* g(v)\\); that is, there is a \\(\\xi \\in [0,1]\\) such that \\[\\begin{aligned}\n  w^* (g(b)-g(a))\n  &= \\int_0^1 w^* g(zb + (1-z)a) (b-a) \\, dz \\\\\n  &= w^* g'(\\xi b + (1-\\xi) a) (b-a).\n\\end{aligned}\\] However, the choice of \\(\\xi\\) depends on \\(w^*\\), so it is not true that we can fully generalize the mean value theorem to maps between vector spaces. On the other hand, we can get something that is almost as good, at least for the purpose of proving bounds used in numerical computing, by choosing \\(w^*\\) to be a vector such that \\(\\|w^*\\|=1\\) (using the dual norm to whatever norm we are using for \\(\\mathcal{W}\\)) and \\(w^* (g(b)-g(a)) = \\|g(b)-g(a)\\|\\). With this choice, and consistent choices of norms, we have that for \\[\\begin{aligned}\n  \\|g(b)-g(a)\\|\n  &= \\int_0^1 w^* g'(zb + (1-z)a) (b-a) \\, dz \\\\\n  &\\leq \\int_0^1 \\|w^*\\| \\|g'(zb+(1-z)a)\\| \\|b-a\\| \\, dz \\\\\n  &\\leq \\left( \\int_0^1 \\|g'(zb+(1-z)a)\\| \\, dz \\right) \\|b-a\\|.\n\\end{aligned}\\] We can also bound the average norm of the derivative on the segment from \\(a\\) to \\(b\\) by \\[\n  \\int_0^1 \\|g'(zb+(1-z)a)\\| \\, dz\n  \\leq\n  \\max_{z \\in [0,1]} \\|g'(zb+(1-z)a)\\|.\n\\] When \\(\\|g'(x)\\| \\leq C\\) for all \\(x \\in \\Omega\\), the function \\(g\\) is Lipschitz with constant \\(C\\) on \\(\\Omega\\).\n\n\n5.4.3 Chain rule\nSuppose \\(f : \\mathcal{V}\\rightarrow \\mathcal{W}\\) and \\(g : \\mathcal{U}\\rightarrow\n\\mathcal{V}\\) are affine functions, i.e. \\[\\begin{aligned}\n  g(x) &= g_0 + J_g x \\\\\n  f(y) &= f_0 + J_f y.\n\\end{aligned}\\] Then \\(h = f \\circ g\\) is another affine map \\[\n  h(x) = f(g(x)) = f(g(0)) + J_f J_g x.\n\\] The chain rule is just a generalization from affine maps to functions well approximated by affine maps (i.e. differentiable functions).\nNow let \\(f : \\mathcal{V}\\rightarrow \\mathcal{W}\\) and \\(g : \\mathcal{U}\\rightarrow \\mathcal{V}\\) be functions where \\(g\\) is differentiable at \\(u\\) and \\(f\\) is differentiable at \\(v = g(u)\\). Then \\[\\begin{aligned}\ng(u+x) &= g(u) + g'(u) x + r_g(x), & r_g(x) &= o(\\|x\\|), \\\\\nf(v+y) &= f(v) + f'(u) y + r_f(y), & r_f(y) &= o(\\|y\\|).\n\\end{aligned}\\] Let \\(h = f \\circ g\\); then setting \\(y = g'(u) x + r_g(x) = O(\\|x\\|)\\), we have \\[\\begin{aligned}\n  h(u+x) &=\n  f(v) + f'(v) y + r_f(y) \\\\\n  &= f(v) + f'(v) g'(u) x + r_h(x), \\\\\n  r_h(x) &= g'(u) r_g(x) + r_f(g'(u) x + r_g(x)) = o(\\|x\\|).\n\\end{aligned}\\] That is, \\(h\\) is differentiable at \\(u\\) with \\(h'(u) = f'(v) g'(u)\\).\n\n5.4.3.1 A polynomial example\nNow consider \\[\n  h(p) =\n  \\frac{1}{2} \\int_{-1}^1 \\left( \\frac{dp}{dx} - \\phi(x) \\right)^2 \\, dx,\n\\] which we can see as the composition of the functional \\[\n  f(r) = \\frac{1}{2} \\int_{-1}^1 r(x)^2 \\, dx\n\\] which we analyzed in the previous section, together with \\[\n  r(p) = \\frac{dp}{dx} - \\phi(x).\n\\] The function \\(r(p)\\) is affine in \\(p\\), and the derivative \\(r'(p)\\) is simply the differentiation operator \\(\\frac{d}{dx}\\). We already saw that \\[\n  f'(r) q = \\int_{-1}^1 r(x) q(x) \\, dx.\n\\] Therefore, by the chain rule we have \\[\n  h'(p) q = f'(r) r'(p) q = \\int_{-1}^1 r(x) \\frac{dq}{dx}(x) \\, dx.\n\\] As is often the case, working code is a useful check to make sure that we understand what is happening with the mathematics:\n\nlet\n    ϕ = Polynomial(rand(4))\n    r(p) = derivative(p) - ϕ\n    f(r) = 0.5*integrate(r*r, -1, 1)\n    h(p) = f(r(p))\n    Dh(p) = q -&gt; integrate(r(p)*derivative(q), -1, 1)\n    \n    ptest = Polynomial(rand(5))\n    qtest = Polynomial(rand(5))\n    isapprox(Dh(ptest)(qtest),\n             finite_diff(h, ptest, qtest), rtol=1e-6)\nend\n\ntrue\n\n\n\n\n\n5.4.4 Partial derivatives\nConsider a Frechet-differentiable function \\(f : \\mathcal{V}\\oplus \\mathcal{U}\\rightarrow \\mathcal{W}\\). For \\(x \\in \\mathcal{V}\\oplus \\mathcal{U}\\), we can write the Frechet derivative as quasimatrix with respect to the two components of the domain space: \\[\n  Df(x) = \\begin{bmatrix} (Df(x))_1 & (Df(x))_2 \\end{bmatrix},\n\\] where \\((Df(x))_1 \\in L(\\mathcal{V}, \\mathcal{W})\\) and \\((Df(x))_2 \\in L(\\mathcal{U}, \\mathcal{W})\\). However, it is often more convenient to “unpack” the input to the function into different components, i.e. thinking of \\(f : \\mathcal{V}\\times \\mathcal{U}\\rightarrow \\mathcal{W}\\). We similarly unpack \\(y\\) into a \\(\\mathcal{V}\\) component \\(y_1\\) and a \\(\\mathcal{U}\\) component \\(y_2\\). Then we write \\[\n  Df(x_1,x_2)(y_1,y_2) = D_1f(x_1,x_2) y_1 + D_2f(x_1,x_2) y_2,\n\\] where the maps \\(D_1 f(x_1,x_2) \\in L(\\mathcal{V}, \\mathcal{W})\\) and \\(D_2 f(x_1,x_2) \\in L(\\mathcal{U}, \\mathcal{W})\\) are the same as \\((Df(x))_1\\) and \\((Df(x))_2\\) above. We refer to \\(D_1 f\\) and \\(D_2 f\\) as the partial derivatives of \\(f\\) with respect to the first and second argument.\nMore generally, if \\(f\\) is a Frechet-differentiable function with \\(m\\) arguments, we write the quasimatrix expression \\[\\begin{aligned}\n  &Df(x_1, \\ldots, x_m) (y_1, \\ldots, y_m) \\\\\n  &=\n  \\begin{bmatrix}\n  D_1 f(x_1, \\ldots, x_m) &\n  \\ldots &\n  D_m f(x_1, \\ldots, x_m)\n  \\end{bmatrix}\n  \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\\\\n  &=\n  \\sum_j D_j f(x_1, \\ldots, x_m) y_j.\n\\end{aligned}\\] Though this may look like a regular matrix expression, we are deliberately not assuming that the arguments have the same type. Indeed, the arguments may belong to wildly different spaces: the first component might belong to \\(\\mathcal{P}_d\\), the second to \\(\\mathbb{R}^n\\), and so on. Nonetheless, we can formally put a (normed) vector space structure on the whole list taken as a direct sum of the argument spaces, and then treat the partial derivatives \\(D_j f\\) as pieces of the overall derivative \\(Df\\).\nA common source of confusion in partial differentiation comes from implicit function composition. For example, consider \\(f : \\mathcal{V}\n\\rightarrow \\mathbb{R}\\) where \\(\\mathcal{V}\\) is a two dimensional space with bases \\(V = \\begin{bmatrix} v_1 & v_2 \\end{bmatrix}\\) and \\(U = \\begin{bmatrix} u_1 & u_e \\end{bmatrix}\\). Then we can write \\[\\begin{aligned}\n  h_V(\\alpha, \\beta) &= (f \\circ g_V)(\\alpha, \\beta), &\n  g_V(\\alpha, \\beta) &= v_1 \\alpha + v_2 \\beta, \\\\\n  h_U(a, b) &= (f \\circ g_U)(a, b), &\n  g_U(a, b) &= u_1 a + u_2 b.\n\\end{aligned}\\] In this notation, the partials are clearly defined: \\[\\begin{aligned}\n  D_k h_V(\\alpha, \\beta) &= f'(g_V(\\alpha, \\beta)) v_k, \\\\\n  D_k h_U(\\alpha, \\beta) &= f'(g_U(\\alpha, \\beta)) v_k.\n\\end{aligned}\\] In contrast, classical notation often conflates \\(f\\), \\(f \\circ g_V\\), and \\(f \\circ g_U\\):\n\n\n\nClassical notation\nOur notation\n\n\n\n\n\\(\\partial f / \\partial \\alpha\\)\n\\(D_1 (f \\circ g_V)\\)\n\n\n\\(\\partial f / \\partial \\beta\\)\n\\(D_2 (f \\circ g_V)\\)\n\n\n\\(\\partial f / \\partial a\\)\n\\(D_1 (f \\circ g_U)\\)\n\n\n\\(\\partial f / \\partial b\\)\n\\(D_2 (f \\circ g_U)\\)\n\n\n\nWe will sometimes use symbolic labels rather than indices to refer to arguments to a function. For example, we might write \\(D_u f(u,v)\\) instead of \\(D_1 f(u,v)\\). This approach is only sensible when the symbolic label unambiguously refers to an argument of \\(f\\), and we must treat it carefully. When there is a danger of ambiguity, we will bend our notation to make clear that we are labeling a slot. For example, we can write \\(D_u f(u=a, v=b)\\) to refer to \\(D_1 f(a,b)\\).\n\n\n5.4.5 Implicit functions\nNow suppose \\(\\mathcal{V}\\) and \\(\\mathcal{W}\\) are spaces of equal dimension, and consider a function \\[\n  f : \\mathcal{V}\\times \\mathcal{U}\\rightarrow \\mathcal{W}\n\\] that is continuously differentiable on some open set including a point \\((v,u)\\) for which \\(f(v,u) = 0\\). We will write the Frechet derivative of \\(f\\) in quasimatrix form as \\[\n  f'(v,u) = \\begin{bmatrix} D_1 f(v,u) & D_2 f(v,u) \\end{bmatrix},\n\\] where \\(D_1 f\\) is the piece of the map associated with \\(v\\) and \\(D_2 f\\) is the piece associated with \\(u\\). If \\(D_1 f(v,u)\\)) is invertible, the the implicit function theorem says that there exists a function \\[\n  g : \\Omega \\subset \\mathcal{U}\\rightarrow \\mathcal{V}\n\\] defined on an open set containing \\(u\\) and such that \\(g(u) = v\\) and \\(f(g(z), z) = 0\\) for \\(z\\) in an open neighborhood about \\(u\\). By the chain rule, we must satisfy \\[\n  D_1 f(g(z), z) g'(z) + D_2 f(g(z),z) = 0.\n\\] Therefore, \\(g'(z)\\) can be computed as \\[\n  g'(z) = -\\left( D_1 f(g,z) \\right)^{-1} D_2 f(g,z),\n\\] where we have suppressed \\(z\\) as an argument to \\(g\\) to simplify notation.\nOften we are interested not in \\(g(z)\\), but some \\(h(g(z))\\) that might lie in a much lower dimensional space. If everything is differentiable, then by the chain rule we have \\[\n  (h \\circ g)'(z)\n  = -h'(g(z)) \\left[ \\left( D_1 f(g,z) \\right)^{-1} D_2 f(g,z) \\right].\n\\] We can use associativity to rewrite \\((h \\circ g)'(z)\\) as \\[\\begin{aligned}\n  \\bar{g}^* &= -h'(g(z)) \\left( D_1 f(g,z) \\right)^{-1} \\\\\n  (h \\circ g)'(z) &= \\bar{g}^* D_2 f(g,z),\n\\end{aligned}\\] where \\(\\bar{g}^* \\in \\mathcal{V}^*\\) is a dual variable. Computing \\(\\bar{g}^*\\) is sometimes called an adjoint solve, since in an inner product space this is equivalent to solving a linear system with the adjint of the derivative operator: \\[\n  \\bar{g} = \\left( D_1 f(g,z) \\right)^{-*} \\nabla h(g(z)).\n\\]\n\n\n5.4.6 Variational notation\nWe will often use a concise notation for differential calculus sometimes called variational notation (as in “calculus of variations”). If \\(f\\) is differentiable at \\(x\\) and \\[\n  y = f(x)\n\\] then in variational notation we would write \\[\n  \\delta y = f'(x) \\, \\delta x.\n\\] Here \\(\\delta x\\) and \\(\\delta y\\) are read as “variation in \\(x\\)” and “variation in \\(y\\),” and we describe the process of differentiating as “taking variations of \\(y = f(x)\\).” If we think of differentiable functions \\(\\tilde{x}(s)\\) and \\(\\tilde{y}(s)\\) with \\(x = \\tilde{x}(0)\\) and \\(y = \\tilde{y}(0)\\), then \\(\\delta x = \\tilde{x}'(0)\\) and \\(\\delta y = \\tilde{y}'(0)\\). Put differently, \\[\\begin{aligned}\n  \\tilde{x}(\\epsilon) &= x + \\epsilon \\, \\delta x + o(\\epsilon), \\\\\n  \\tilde{y}(\\epsilon) &= y + \\epsilon \\, \\delta y + o(\\epsilon),\n\\end{aligned}\\] and similarly for other related quantities. If \\(x\\) and \\(y\\) have types that can be added, scaled, and multiplied together, we also have the standard rules \\[\\begin{aligned}\n\\delta (x+y) &= \\delta x + \\delta y \\\\\n\\delta (\\alpha x) &= \\alpha \\, \\delta x \\\\\n\\delta (xy) &= \\delta x \\, y + x \\, \\delta y.\n\\end{aligned}\\] This last is derived in the way that we usually do: \\[\n  (x + \\epsilon \\, \\delta x + o(\\epsilon))\n  (y + \\epsilon \\, \\delta y + o(\\epsilon)) =\n  xy + \\epsilon(\\delta x \\, y + x \\, \\delta y) + o(\\epsilon).\n\\] We note that this is true even for things like linear maps of the right types, which can be multiplied but do not commute.\nVariational notation is sometimes tidier than other ways of writing derivative relationships. For example, suppose \\(A \\in L(\\mathcal{V}, \\mathcal{V})\\) is an invertible linear map and we are interested in the variation in \\(B = A^{-1}\\) with respect to the variation in \\(A\\). We can see \\(B\\) as a differentiable function of \\(A\\) by the implicit function theorem on the equation \\[\n  AB-I = 0.\n\\] Taking variations of this relationship, we have \\[\n  (\\delta A) B + A (\\delta B) = 0,\n\\] which we can rearrange to \\[\n  \\delta B = -A^{-1} (\\delta A) A^{-1}.\n\\] The formula for taking variations of \\(A^{-1}\\) generalizes the 1D formula \\((x^{-1})' = -x^{-2}\\), which is often used as a first example of implicit differentiation in introductory calculus courses. A short computation verifies that our formula agrees with a finite difference estimate for a small example.\n\nlet\n    A =  [ 94.0  29.0  26.0 ;\n           65.0  25.0  66.0 ;\n           85.0  92.0  72.0 ]\n    δA = [ 57.0  15.0  46.0 ;\n           56.0  44.0  79.0 ;\n           42.0  45.0  48.0 ]\n    δB = -A\\δA/A\n    δB_fd = finite_diff(inv, A, δA)\n    isapprox(δB, δB_fd, rtol=1e-6)\nend\n\ntrue\n\n\nWe could work out the same formula with the notation introduced in the previous sections, but the version involving variational notation is arguably easier to read and certainly shorter to write.\n\n\n5.4.7 Adjoints\nConsider a map \\(\\mu\\) taking an vector input \\(x\\) to an scalar output \\(y\\) via a vector intermediates \\(u\\) and \\(v\\), written in terms of the relations \\[\\begin{aligned}\n  u &= f(x) \\\\\n  v &= g(x, u) \\\\\n  y &= h(x, u, v)\n\\end{aligned}\\] Then we have the derivative relationships \\[\\begin{aligned}\n  \\delta u &= D_1 f \\, \\delta x \\\\\n  \\delta v &= D_1 g \\, \\delta x + D_2 g \\, \\delta u \\\\\n  \\delta y &= D_1 h \\, \\delta y + D_2 g \\, \\delta u + D_3 g \\, \\delta v\n\\end{aligned}\\] We can rearrange this into the (quasi)matrix form \\[\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n-D_1 f & 1 & 0 & 0 \\\\\n-D_1 g &  -D_2 g & 1 & 0 \\\\\n-D_1 h &  -D_2 h & -D_3 h & 1\n\\end{bmatrix}\n\\begin{bmatrix} \\delta x \\\\ \\delta u \\\\ \\delta v \\\\ \\delta y \\end{bmatrix} =\n\\begin{bmatrix} I \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\delta x.\n\\] That is, we the computation of \\(\\delta y\\) and then \\(\\delta z\\) from \\(\\delta x\\) as an example of forward substitution for a 4-by-4 linear system.\nIf we truly only care about the relationship between \\(x\\) and \\(y\\), there is no reason why we should explicitly compute the intermediate \\(\\delta y\\). An equally good way to solve the problem is to solve the adjoint equation discussed in Section 5.4.5: \\[\n\\begin{bmatrix} \\bar{x}^* & \\bar{u}^* & \\bar{v}^* & \\bar{y}^* \\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n-D_1 f & 1 & 0 & 0 \\\\\n-D_1 g &  -D_2 g & 1 & 0 \\\\\n-D_1 h &  -D_2 h & -D_3 h & 1\n\\end{bmatrix} =\n\\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}.\n\\] via backward substitution on the 4-by-4 linear system: \\[\\begin{aligned}\n  \\bar{y}^* &= 1 \\\\\n  \\bar{v}^* &= \\bar{y}^* D_3 h \\\\\n  \\bar{u}^* &= \\bar{v}^* D_2 g + \\bar{y}^* D_2 h \\\\\n  \\bar{x}^* &= \\bar{u}^* D_1 h + \\bar{v}^* D_1 g + \\bar{y}^* D_1 h.\n\\end{aligned}\\] We observe that \\[\n  \\delta y = \\bar{x}^* \\, \\delta x;\n\\] that is, \\(\\bar{x}^*\\) is the derivative of the map from \\(x\\) to \\(z\\).\nWhile the variations \\(\\delta y\\) and \\(\\delta z\\) lie in the same space as \\(y\\) and \\(z\\), the dual variables \\(\\bar{u}^*, \\bar{v}^*, \\bar{y}^*\\) (also called adjoint variables) live in the associated dual spaces. The dual variables can also be interpreted as the sensitivity of the output \\(y\\) to the associated primary variable, assuming earlier variables were held constant.\nThe idea of using dual variables to compute derivatives of a function with many intermediates (rather using variations as intermediate quantities) is the basis of adjoint mode or reverse mode automatic differentiation, which we will discuss further in Chapter 11.\n\n\n5.4.8 Higher derivatives\n\n5.4.8.1 Maps from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\)\nA main reason — if not the main reason — why we care about higher derivatives is because of their role in approximation.\nAs a particular starting point, consider \\(g : \\Omega \\subset \\mathbb{R}\n\\rightarrow \\mathbb{R}\\) on an interval \\(\\Omega\\). Assuming \\(g\\) has at least \\(k+1\\) continuous derivatives (i.e. \\(g \\in C^{k+1}\\)), then Taylor’s theorem with integral remainder gives \\[\n  g(t) = \\sum_{j=0}^{k} \\frac{1}{j!} g^{(j)}(t) + r_{k+1}(t),\n\\] where \\[\n  r_{k+1}(t) = \\int_0^t \\frac{1}{k!} (t-s)^{k} g^{(k+1)}(s) \\, ds.\n\\] This formula can be verified with integration by parts. By the mean value theorem, there is some \\(\\xi \\in (0,t)\\) such that \\[\n  r_{k+1}(t) = \\frac{t^{k+1}}{(k+1)!} g^{(k+1)}(\\xi).\n\\] This is the mean value form of the remainder.\nWe will sometimes work with slightly less regular functions, e.g. \\(g : \\mathbb{R}\\rightarrow \\mathbb{R}\\) with \\(k\\) continuous derivatives and \\(g^{(k)}\\) Lipschitz with constant \\(C\\) (which implies absolute continuity of \\(g^{(k)}\\)). In this case, we do not have the mean value form of the remainder, but can still use the integral form to get the bound \\[\n  |r_{k+1}(t)| \\leq \\frac{C|t|^{k+1}}{(k+1)!}.\n\\] Even for \\(C^{k+1}\\) functions, these types of bounds are sometimes easier to work with than the mean value form of the remainder.\n\n\n5.4.8.2 Maps from \\(\\mathcal{V}\\) to \\(\\mathbb{R}\\)\nIf \\(f : \\mathcal{V}\\rightarrow \\mathbb{R}\\) is differentiable in an open set \\(\\Omega\\) containing \\(x\\), the Frechet derivative is a function \\[\n  f' : \\mathcal{V}\\rightarrow \\mathcal{V}^* = L(\\mathcal{V}, \\mathbb{R})\n\\] where \\(f'(x) \\in \\mathcal{V}^*\\) maps directions to directional derivatives. If \\(f'\\) is differentiable, then \\[\n  f'' : \\mathcal{V}\\rightarrow L(\\mathcal{V}, \\mathcal{V}^*).\n\\] The mappings \\(f''(x)\\) from \\(\\mathcal{V}\\) to \\(\\mathcal{V}^*\\) can also be thought of as a bilinear form from two vectors in \\(\\mathcal{V}\\) to \\(\\mathbb{R}\\), i.e. \\((u, v) \\mapsto (f''(x) u) v\\). Hence, we write \\[\n  f'' : \\mathcal{V}\\rightarrow L(\\mathcal{V}\\otimes \\mathcal{V}, \\mathbb{R}).\n\\] When \\(f''\\) is continuously defined in some neighborhood of \\(x\\), the bilinear form is also guaranteed to be symmetric, i.e. \\[\n  f''(x) \\, (u \\otimes v) = f''(x) (v \\otimes u).\n\\] As discussed in Chapter 4, there is a 1-1 correspondence between symmetric bilinear forms and quadratic forms, and we will mostly be interested in \\(f''(x)\\) as representing the quadratic \\(u \\mapsto f''(x) \\, (u \\otimes u)\\).\nThe matrix representation of \\(f''(x)\\) with respect to a particular basis \\(V\\) is called the Hessian matrix, and is sometimes written \\(H_f(x)\\). The entries of this matrix are \\((H_f(x))_{ij} = f''(x) (v_i \\otimes v_j)\\), so that we write evaluation of the Hessian on a pair of vectors as \\[\n  f''(x) (Vc \\otimes Vd) = d^* H_f(x) c.\n\\] In most cases, we are interested in evaluating the quadratic form associated with \\(f''(x)\\), i.e. \\[\n  f''(x) (Vc \\otimes Vc) = c^* H_f(x) c.\n\\] Hessian matrices will play a central role in our discussion of numerical optimization methods.\nIn thinking about first derivatives, we started by considering differentiation along a straight ray. We will do the same thing when thinking about using second derivatives. Suppose \\(c_{x,u}(s) = x+su\\) and \\(c_{x,u}([0,t]) \\in \\Omega\\), and let \\(g = f \\circ c_{x,u}\\). Suppose \\(f \\in C^2(\\Omega)\\) where \\(f''\\) is Lipschitz with constant \\(M\\) (with respect to an induced norm); that is: \\[\\begin{aligned}\n  \\|f''(x)\\| &= \\max_{\\|w\\|=1} |f'(x) (w \\otimes w)| \\\\\n  \\|f''(x)-f''(y)\\| &\\leq M \\|x-y\\|.\n\\end{aligned}\\] Then Taylor’s theorem with remainder gives \\[\n  g(t) = g(0) + g'(0) t + \\frac{1}{2} g''(0) t^2 + r(t),\n  \\quad |r(t)| \\leq \\frac{M \\|u\\|^3}{6} t^3.\n\\] where \\[\\begin{aligned}\n  g'(0) &= f'(0) u \\\\\n  g''(0) &= f''(0) \\, (u \\otimes u).\n\\end{aligned}\\] We usually skip the intermediate function \\(g\\), and write that when \\(f''\\) is Lipschitz and \\(\\Omega\\) is convex (so that the line segment connecting \\(x, u \\in \\Omega\\) will lie entirely within \\(\\Omega\\)) then \\[\n  f(x+u) = f(x) + f'(x) u + \\frac{1}{2} f''(x) (u \\otimes u) + O(\\|u\\|^3),\n\\] where we are only dealing with the asymptotics of the third-order term. When dealing with a concrete space \\(\\mathbb{R}^n\\) (or when working in terms of a basis for \\(\\mathcal{V}\\)), we usually write \\[\n  f(x+u) = f(x) + f'(x) u + \\frac{1}{2} u^* H_f(x) u + O(\\|u\\|^3).\n\\]\nOne can continue to take higher derivatives in a similar manner — e.g., if \\(f''\\) is differentiable, then \\(f'''(x)\\) is a trilinear form in \\(L(\\mathcal{V}\\otimes \\mathcal{V}\\otimes \\mathcal{V}, \\mathbb{R})\\), and we represent it with respect to a basis \\(V\\) as a concrete tensor with entries that we usually write as \\(f_{,ijk} = f'''(x) \\, (v_i \\otimes v_j \\otimes v_k)\\). In the rare cases when we use more than second derivatives, we often revert to indicial notation for computing with functions on concrete spaces; with the summation convention in effect, the Taylor series becomes \\[\n  f(x + u) = f(x) + f_{,i}(x) u_i + \\frac{1}{2} f_{,ij}(x) u_i u_j +\n  \\frac{1}{6} f_{,ijk}(x) u_i u_j u_k + \\ldots.\n\\] The size and notational complexity goes up significantly as we move beyond second derivatives, enough so that we avoid higher derivatives if we can.\n\n\n5.4.8.3 General maps\nWe now consider the case of \\(f : \\mathcal{V}\\rightarrow \\mathcal{W}\\). If \\(f\\) is \\(C^2\\) on \\(\\Omega\\), then the second derivative is \\[\n  f'' : \\mathcal{V}\\rightarrow\n  L(\\mathcal{V}, L(\\mathcal{V}, \\mathcal{W})) \\equiv L(\\mathcal{V}\\otimes \\mathcal{V}, \\mathcal{W}),\n\\] and the bilinear map \\(f''(x)\\) is symmetric in the arguments, i.e. \\[\n  f''(x) \\, (u \\otimes v) = f''(x) \\, (v \\otimes u).\n\\] Expanding to second order with remainder again looks like \\[\n  f(x+u) = f(x) + f'(x) u + \\frac{1}{2} f''(x) (u \\otimes u) + O(\\|u^3\\|).\n\\] But when we want to compute with concrete quantities, we need three indices even to keep track of the second derivative term: with respect to bases \\(V\\) and \\(W\\), we write the concrete tensor \\[\n  f_{i,jk} = \\left[ W^{-1} f(x) (v_j \\otimes v_k) \\right]_i\n\\] or, equivalently (using indices and the summation convention), \\[\n  f(x) (Vc \\otimes Vd) = \\mathbf{w}_i f_{i,jk} c_j d_k.\n\\]\n\n\n\n5.4.9 Analyticity\nSo far, we have considered functions on real vector spaces. But for some of what we have to say about matrix calculus later, it will be useful to also consider functions on the complex plane.\nWe begin with an algebra fact. Suppose that \\(z = x+iy\\) and \\(c = a+ib\\) are complex numbers written in rectangular form; then \\[\n  cz = \\begin{bmatrix} 1 & i \\end{bmatrix}\n       \\begin{bmatrix} a & -b \\\\ b & a \\end{bmatrix}\n       \\begin{bmatrix} x \\\\ y \\end{bmatrix}.\n\\] That is, the matrix mapping the real and imaginary components of \\(z\\) to the real and imaginary components of \\(cz\\) has the form \\(aI + bJ\\) where \\(I\\) is the identity and \\(J\\) is the 2-by-2 skew matrix \\[\n  J = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}.\n\\] Hence, we can interpret complex multiplication as a 2-by-2 matrix on the rectangular coordinate parameterization of the reals — but it is a matrix of a very specific type.\nNow let \\(f : \\mathbb{C}\\rightarrow \\mathbb{C}\\). We would like to say that \\(f\\) is differentiable at \\(z\\) if \\[\n  f(z+w) = f(z) + f'(z) w + o(|w|).\n\\] Phrased in terms of rectangular coordinates (\\(f = g+ih\\), \\(z = x+iy\\), \\(w = u+iv\\)), we have that \\[\n  \\begin{bmatrix}\n    D_1 g(x,y) & D_2 g(x,y) \\\\\n    D_1 h(x,y) & D_2 h(x,y)\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    a(x,y) & -b(x,y) \\\\\n    b(x,y) &  a(x,y)\n  \\end{bmatrix}\n\\] i.e. the partial derivatives of the real and imaginary components of \\(f\\) must satisfy the Cauchy-Riemann equations \\[\n  D_1 g = D_2 h, \\quad D_2 g = -D_1 h.\n\\] A function that is complex differentiable on an open set \\(\\Omega\\) is called holomorphic (or (complex) analytic) on that set.\nHolomorphic functions are about as nice as functions can be. Among other nice properties:\n\nThey are automatically infinitely (complex) differentiable,\nThey have convergent Taylor series about any point in the domain \\(\\Omega\\),\nThe real and imaginary components are harmonic functions (i.e. \\(D_1^2\ng + D_2^2 g = 0\\), and similarly for \\(h\\)), and\nContour integration around any loop \\(\\Gamma\\) whose interior is wholly within \\(\\Omega\\) gives \\(\\int_\\Gamma f(z) \\, dz = 0\\).\n\nWe will heavily rely on this last fact (and related facts) when dealing with the contour integral view of functions of matrices in Section 5.7.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#series",
    "href": "00-Background/04-Calculus.html#series",
    "title": "5  Calculus and analysis",
    "section": "5.5 Series",
    "text": "5.5 Series\nA series is an infinite sequence of terms \\(v_1, v_2, \\ldots\\) that are (formally) added together. When students first learn about series in introductory calculus courses, the terms \\(v_j\\) are typically real or complex numbers. We will consider the more general case where the \\(v_j\\) are drawn from some normed vector space \\(\\mathcal{V}\\), which we will often assume is complete (a Banach space). The partial sums are \\[\n  S_n = \\sum_{j=1}^n v_j;\n\\] and when \\(\\mathcal{V}\\) is a Banach space and the partial sums form a Cauchy sequence, we say the series converges to \\[\n  S = \\sum_{j=1}^\\infty v_j = \\lim_{n \\rightarrow \\infty} S_n.\n\\] Series that do not converge (divergent series) are still useful in many situations; we simply need to understand that they are to be treated formally, and we might not be able to make sense of the limiting value.\n\n5.5.1 Convergence tests\nMany of the standard convergence tests for series in \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) generalize to series in a Banach space. The comparison test is of particular use:\n\nTheorem 5.2 (Comparison test) Suppose \\(\\{v_j\\}_{j=1}^\\infty\\) is a sequence in a Banach space \\(\\mathcal{V}\\), where the terms are bounded by \\(\\|v_j\\| \\leq a_j\\). Suppose \\(\\sum_{j=1}^\\infty a_j\\) converges. Then \\(\\sum_{j=1}^\\infty v_j\\) also converges absolutely; moreover, \\[\n  \\left\\| \\sum_{j=1}^\\infty v_j \\right\\| \\leq \\sum_{j=1}^\\infty a_j.\n\\]\n\n\nProof. Let \\(A_n\\) and \\(A\\) denote the \\(n\\)th partial sum and the limit for the series \\(\\sum_{j=1}^\\infty a_j\\), and let \\(S_n\\) denote the \\(n\\)th partial sum for the series \\(\\sum_{j=1}^\\infty v_j\\). By the triangle inequality, for any \\(i \\geq j\\), we have \\[\n  \\|S_j-S_i\\| = \\left\\| \\sum_{k=i+1}^j v_k \\right\\| \\leq\n  \\sum_{k=i+1}^j \\|v_k\\| \\leq A_j-A_i.\n\\] The fact that the partial sums \\(A_n\\) form a Cauchy sequence means that for any \\(\\epsilon &gt; 0\\), there exists \\(N\\) such that if \\(i \\geq j \\geq N\\), \\[\n  |A_j-A_i| &lt; \\epsilon;\n\\] and because \\(\\|S_j-S_i\\| \\leq |A_j-A_i|\\), the partial sums \\(S_n\\) also form a Cauchy sequence, and therefore converge to a limit \\(S\\).\nThe final bound comes from applying the triangle inequality to show \\[\n  \\|S_n\\|\n  = \\left\\| \\sum_{j=1}^n v_j \\right\\|\n  \\leq \\sum_{j=1}^n \\|v_j\\|\n  \\leq A_n,\n\\] and from there arguing that the limit \\(\\|S\\|\\) is bounded by the limit \\(A\\).\n\nMany other convergence tests similarly generalize to series in Banach spaces, often using the comparison test as a building block. An example is the ratio test:\n\nTheorem 5.3 (Ratio test) Suppose \\(\\{v_j\\}_{j=1}^\\infty\\) is a sequence in a Banach space \\(\\mathcal{V}\\), where \\(\\lim_{n\\rightarrow \\infty} \\|v_{n+1}\\|/\\|v_n\\| = r &lt; 1\\). Then \\(\\sum_{j=1}^\\infty v_j\\) converges absolutely.\n\n\nProof. Let \\(r &lt; r' &lt; 1\\) (e.g. choose \\(r' = (r+1)/2\\)). Then by the hypothesis, there exists some \\(N\\) such that for all \\(n \\geq N\\), \\(\\|v_{n+1}\\|/\\|v_n\\| \\leq r'\\). Hence, for \\(n \\geq N\\) we have \\(\\|v_n\\| \\leq (r')^{n-N} \\|v_N\\|\\), and by the comparison test and convergence of the geometric series, the series \\(\\sum_j v_j\\) converges, and moreover we have the bound \\[\\begin{aligned}\n  \\left\\| \\sum_{n=1}^\\infty v_n \\right\\|\n  &\\leq \\left\\| \\sum_{n=1}^{N-1} v_n \\right\\| + \\sum_{n=N}^\\infty\n  (r')^{n-N} \\|v_N\\| \\\\\n  &= \\left\\| \\sum_{n=1}^{N-1} v_n \\right\\| + \\frac{\\|v_N\\|}{1-r'}.\n\\end{aligned}\\]\n\n\n\n5.5.2 Function series\nFrequently, we are interested in series where the terms are functions of some variable, e.g. \\(v_j : \\Omega \\subset \\mathbb{C}\\rightarrow \\mathcal{V}\\) for some Banach space \\(\\mathcal{V}\\). Often the functional dependence of the terms on \\(v_j\\) is fairly simple, e.g. \\(v_j(z) = \\bar{v}_j z^{p_j}\\) where the exponents \\(p_j\\) may be just positive integers (a power series), positive and negative integers (a Laurent series), or fractions with a fixed denominator (a Puiseaux series). We also often see terms of the form \\(v_n(z) = \\bar{v}_n \\exp(inz)\\) (a Fourier series).\nWhen the terms themselves belong to some Banach space, we can analyze the convergence or divergence of the series as a whole just as we would analyze the convergence or divergence of any other series over a Banach space. For example, if \\(\\Omega\\) is compact, then the set of continuous functions \\(C(\\Omega, \\mathcal{V})\\) (which are automatically uniformly continuous) is a Banach space. Convergence in this Banach space corresponds to uniform convergence of the functions, and uniform convergence of uniformly continuous functions gives a uniformly continuous limit, as noted before.\nHowever, more generally sometimes the terms may not belong to a Banach space; or if they do belong to a Banach space, they might diverge. In this case, we might care about the convergence or divergence of the series \\(\\sum_{j=1}^\\infty v_j(z)\\) for specific values of \\(z\\); and, if we have pointwise convergence over some subset \\(\\Omega' \\subset\n\\Omega\\), we care what properties the limiting function \\(S : \\Omega'\n\\rightarrow \\mathcal{V}\\) might inherit from the terms. This issue can be subtle: for example, a Fourier series can easily converge at almost all points in some interval, but converge to a discontinuous function despite the fact that all the terms are infinitely differentiable.\nFunction series can also be extremely useful even where they diverge, both for formal manipulation and for actual calculations where the approximation properties of finite sums are more important than the limiting behavior of the series.\n\n\n5.5.3 Formal series\nA formal power series is a series of the form \\[\n  S(z) = \\sum_{j=0}^\\infty a_j z^j\n\\] where the coefficients \\(a_j\\) belong to some appropriate vector space \\(\\mathcal{V}\\). In some circumstances, it also makes sense to include negative powers, in which case we sometimes refer to this as a formal Laurent series. Formal power series appear in a variety of applications. We will mostly see them in signal processing, where such a formal power series is also called the \\(z\\)-transform of the sequence of coefficients \\(a_j\\). In that setting, is it particularly useful not only to add and scale formal power series, but also to multiply them together using the rule \\[\n  \\left( \\sum_{j=0}^\\infty a_j z^j \\right)\n  \\left( \\sum_{j=0}^\\infty b_j z^j \\right) =\n  \\sum_{k=0}^\\infty \\left( \\sum_{i+j=k} a_i b_j \\right) z^k;\n\\] that is, the coefficients in the product are the convolution of the coefficient sequences of the multiplicand series. Hence, the power series can be seen as a convenient way of dealing with the book-keeping of tracking sequences and their convolutions, independent of whether things converge. Formal power series (along with other formal function series) also play a key role in combinatorics and statistics, often under the name of “generating functions” (see Wilf (2006)).\nIf \\(\\mathcal{V}\\) is a Banach space and \\(\\log \\|a_n\\| = O(n)\\), the ordinary formal power series associated with the coefficient sequence \\(a_j\\) will converge for values of \\(z \\in \\mathbb{C}\\) close enough to zero. Under such assumptions (which often hold), we can treat the formal power series as defining a function, and can bring the machinery of analytic function theory to bear. Nonetheless, it is helpful to distinguish the formal power series — essentially a trick for indexing elements of a series — from the associated function to which it converges. Only the former is needed for tracking convolutions and the like.\n\n\n5.5.4 Asymptotic series\nAn asymptotic series for a function \\(f\\) is a formal function series with terms \\(v_n(z)\\) such that \\[\n  f(z) - \\sum_{n=0}^{N-1} v_n(z) = o(\\|\\phi_{N-1}(z)\\|)\n\\] for some limiting behavior in \\(z\\) (usually \\(z \\rightarrow 0\\) or \\(z\n\\rightarrow \\infty\\)). Asymptotic series often do not converge, but are nonetheless useful for approximation.\nA standard example of an asymptotic series is Stirling’s approximation for the gamma function (generalized factorial function): \\[\n  \\Gamma(z+1) \\sim\n  \\sqrt{2\\pi z} \\left( \\frac{z}{e} \\right)^z \\left( 1 + \\frac{1}{12}\n  z^{-1} + \\frac{1}{288} z^{-2} - \\frac{139}{51840} z^{-3} - O(z^{-4}) \\right).\n\\] Here the symbol \\(\\sim\\) is used to denote that the series is asymptotic to the function, but does not necessarily converge to the function in the limit of an infinite number of terms. Another standard example is the asymptotic expansion for the tail of the standard normal cdf: \\[\n  1 - \\Phi(z) = \\phi(z) z^{-1} \\left( 1 - z^{-2} + 3z^{-4} + O(z^{-6}) \\right).\n\\] Both these methods are derived via Laplace’s method, which is also very useful for approximating integrals arising in Bayesian inference. These types of asymptotic approximations are typically complementary to numerical methods, providing very accurate estimate precisely in the regions where more conventional numerical approaches have the most difficulty.\n\n\n5.5.5 Analytic functions\nWhile formal series and asymptotic approximation are nice, even nicer things can happen for power series that converge. When a series \\[\n  f(z) = \\sum_{j=0}^\\infty c_j (z-z_0)^j\n\\] converges in a neighborhood of \\(z_0\\) (in \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)), we say the function \\(f\\) is analytic at \\(z_0\\). Such a function is automatically infinitely differentiable in a neighborhood of \\(z_0\\). However, there are (useful) examples of non-analytic functions on \\(\\mathbb{R}\\) that are infinitely differentiable, e.g. \\[\n  g(x) =\n  \\begin{cases}\n    \\exp(-1/x), & x \\geq 0 \\\\\n    0 & \\mbox{otherwise}\n  \\end{cases}\n\\]\nComplex analyticity around a point \\(z_0 \\in \\mathbb{C}\\) (existence of a convergent Taylor series) and the property of being holomorphic around a point (having complex derivatives that satisfy the Cauchy-Riemann equations) are equivalent to each other. Functions that are real analytic in the neighborhood of a point in \\(\\mathbb{R}\\) are also complex analytic in a neighborhood. However, a function that is real analytic over all of \\(\\mathbb{R}\\) may not extend to a function that is complex analytic over all of \\(\\mathbb{C}\\).\nThe property of analyticity at a point \\(z_0\\) is closed under addition, scaling, multiplication of functions, integration, differentiation, and composition. It is also closed under division, assuming that the denominator function is nonzero at \\(z_0\\).\n\n\n5.5.6 Operator functions\nJust as we can compute powers of \\(z \\in \\mathbb{C}\\), we can compute powers of \\(A \\in L(\\mathcal{V}, \\mathcal{V})\\). Indeed, as we noted in ?sec-tbd, polynomials of an operator are enormously useful both as a theoretical tool (e.g. for studying eigenvalues) and as a numerical tool (e.g. in the design of Krylov subspace methods, which we will see later in this book). Unsurprisingly, taking limits of sequences of polynomials is also useful, and power series expressions for analytic functions on \\(\\mathbb{C}\\) work equally well for describing analytic functions on operator spaces.\nFor our purposes, we will mostly consider \\(\\mathcal{V}\\) to be a finite-dimensional normed space, but the concept works equally well for general Banach spaces. In either case, the space of bounded operators \\(L(\\mathcal{V}, \\mathcal{V})\\) is itself a Banach space, and our earlier comments about convergent series in general Banach spaces holds. For example, using a consistent family of norms (e.g. the operator norm), consider \\[\n  \\exp(A) = \\sum_{k=0}^\\infty \\frac{1}{k!} A^k;\n\\] by comparison to the series for \\(\\exp(\\|A\\|)\\), this series converges absolutely, and \\[\n  \\|\\exp(A)\\| \\leq \\exp(\\|A\\|).\n\\] We observe that if we have an eigenpair \\(Av = v \\lambda\\), then \\[\n  \\exp(A) v\n  = \\sum_{k=0}^\\infty \\frac{1}{k!} A^k v\n  = \\sum_{k=0}^\\infty \\frac{1}{k!} v \\lambda^k\n  = v \\exp(\\lambda).\n\\] When \\(A\\) is diagonalizable (i.e. \\(A = V \\Lambda V^{-1}\\)), then we can completely characterize \\(\\exp(A)\\) as \\[\n  \\exp(A) = V \\exp(\\Lambda) V^{-1},\n\\] where \\(\\exp(\\Lambda)\\) is the diagonal matrix of exponentials of eigenvalues.\nWhat works for the exponential tends to work for more general analytic functions \\(f\\). Things get a little more complicated for operators with nontrivial Jordan blocks (and certainly for the more elaborate spectra that can occur in the infinite-dimensional case), but the fundamental picture remains the same: an analytic function \\(f\\) makes sense as a function of an operator when the spectrum is inside the domain of analyticity; and \\(f(A)\\) will have the same eigenvector structure as \\(A\\) while eigenvalues are transformed by the mapping \\(\\lambda \\mapsto f(\\lambda)\\).\n\n\n5.5.7 Neumann series\nWhile the exponential provides a nice example of an operator power series, we will see far more of the Neumann series, which is the generalization of the geometric series. When \\(\\|A\\| &lt; 1\\) under any consistent norm, we have that the Neumann series \\[\n  (I-A)^{-1} = \\sum_{k=0}^\\infty A^k\n\\] converges by comparison with a geometric series in \\(\\|A\\|\\); and (courtesy that same comparison), that \\[\n  \\|(I-A)^{-1}\\| \\leq \\sum_{k=0}^\\infty \\|A\\|^k = \\frac{1}{1-\\|A\\|}.\n\\] With a little supplementary algebra, this Neumann series bound is the basis for many inequalities that will prove useful later. As an example left as an exercise for the student, we observe that when \\(A\\) is invertible and \\(\\|A^{-1} E\\| &lt; 1\\), we have \\[\n\\|(A+E)^{-1}\\| \\leq \\frac{\\|A^{-1}\\|}{1-\\|A^{-1} E\\|}.\n\\]",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#integration",
    "href": "00-Background/04-Calculus.html#integration",
    "title": "5  Calculus and analysis",
    "section": "5.6 Integration",
    "text": "5.6 Integration\n\n5.6.1 Measure and integration\n\n\n5.6.2 Standard inequalities\n\n\n5.6.3 Change of variables",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#sec-calculus-contour-int",
    "href": "00-Background/04-Calculus.html#sec-calculus-contour-int",
    "title": "5  Calculus and analysis",
    "section": "5.7 Contour integrals",
    "text": "5.7 Contour integrals\n\n5.7.1 Poles and residues\n\n\n5.7.2 Resolvent calculus",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#function-spaces",
    "href": "00-Background/04-Calculus.html#function-spaces",
    "title": "5  Calculus and analysis",
    "section": "5.8 Function spaces",
    "text": "5.8 Function spaces\n\n\n\n\nKeynes, John Maynard. 1929. A Tract on Monetary Reform. The Macmillan Company. https://archive.org/details/tractonmonetaryr0000keyn/mode/2up.\n\n\nKline, M. 1990. Mathematical Thought from Ancient to Modern Times. Oxford University Press.\n\n\nRosenthal, Arthur. 1951. “The History of Calculus.” The American Mathematical Monthly 58 (2): 75–86. https://doi.org/10.2307.2308368.\n\n\nWilf, Herbert S. 2006. Generatingfunctionology. Third. AK Peters, Ltd. https://www2.math.upenn.edu/~wilf/DownldGF.html.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/04-Calculus.html#footnotes",
    "href": "00-Background/04-Calculus.html#footnotes",
    "title": "5  Calculus and analysis",
    "section": "",
    "text": "“I turn away with fright and horror from this lamentable evil of functions which do not have derivatives.” – Hermite (according to Kline 1990, 973)↩︎\n“But this long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task if in tempestuous seasons they can only tell us that when the storm is long past the ocean will be flat again.” — John Maynard Keynes, (from p.80, Keynes 1929)↩︎\nA topological space is a set \\(X\\) and a collection of sets (called the open sets) that include the empty set and all of \\(X\\) and are closed under unions and finite intersections.↩︎\nIn general topological spaces, compactness and sequential compactness are not the same thing. Fortunately, it’s all the same in metric space (assuming the axiom of choice).↩︎",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Calculus and analysis</span>"
    ]
  },
  {
    "objectID": "00-Background/05-Optimization.html",
    "href": "00-Background/05-Optimization.html",
    "title": "6  Optimization theory",
    "section": "",
    "text": "6.1 Optimality conditions\nWe will frequently phrase model fitting problems in terms of optimization, which means we need to remember how optimality conditions work. This includes not only remembering first-order and second-order optimality conditions for derivatives, but also knowing a little about optimality conditions for non-differentiable functions. The key concept behind optimality conditions is to locally approximate a function in terms of a local polynomial approximation, where it usually suffices to go out to first or second order.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimization theory</span>"
    ]
  },
  {
    "objectID": "00-Background/05-Optimization.html#optimality-conditions",
    "href": "00-Background/05-Optimization.html#optimality-conditions",
    "title": "6  Optimization theory",
    "section": "",
    "text": "6.1.1 Minima and maxima\nLet \\(f : \\Omega \\subset \\mathcal{V}\\rightarrow \\mathbb{R}\\) be a continuous function. The set \\(\\Omega\\) is the feasible set. We say the problem of minimizing or maximizing \\(f\\) is an unconstrained optimization problem when \\(\\Omega = \\mathcal{V}\\) (all points in the space are feasible). Otherwise, it is a constrained optimization problem.\nWe say \\(x \\in \\Omega\\) is a local minimizer if any \\(y \\in\n\\Omega\\) close enough to \\(x\\) satisfies \\(f(y) \\geq f(x)\\) (more precisely: there is some \\(\\epsilon &gt; 0\\) so that for all \\(y\\) within \\(\\epsilon\\) of \\(x\\), \\(f(y) \\geq f(x)\\)). The value \\(f(x)\\) is the local minimum value. For a strong local minimizer, the inequality is strict – any \\(y \\neq x\\) in a neighborhood of \\(x\\) satisfies \\(f(y) &gt;\nf(x)\\). We say \\(x\\) is a global minimizer if \\(f(x) \\leq f(y)\\) for all \\(y \\in \\Omega\\), and a strong global minimizer if \\(f(x) &lt; f(y)\\) for all \\(y \\in \\Omega\\) other than \\(x\\).\nWe can define a local maximizer or a strong local maximizer analogously, but we usually focus on the case of minimization rather than maximization (and in any event, maximizing \\(f\\) is the same as minimizing \\(-f\\)).\nWhen the feasible set \\(\\Omega\\) is compact (for finite-dimensional vector spaces, this means closed and bounded), we are guaranteed that there is a global minimizer in \\(\\Omega\\). In other cases, \\(f\\) may not be bounded below, or it may be bounded below but with no point where the greatest lower bound (the infimum) is achieved.\n\n\n6.1.2 Derivative and gradient\nIf \\(f : \\mathcal{V}\\rightarrow \\mathbb{R}\\) is differentiable, a stationary point is a point \\(x\\) such that \\(f'(x) = 0\\). At a non-stationary point, there is guaranteed to be some direction \\(u\\) such that \\(f'(x) u &gt; 0\\) (and \\(f'(x) (-u) &lt; 0\\)), so that \\(f\\) can neither attain an (unconstrained) minimum nor maximum at that point. Stationary points can be minima, maxima, or saddles; we usually classify them as such by the second derivative test.\nIn an inner product space, the gradient \\(\\nabla f(x)\\) (the Riesz map of \\(f'(x)\\)) gives us the “direction of steepest ascent,” and the negative gradient gives us the direction of steepest descent. It is important to realize that this direction depends on the inner product used! Moreover, the concept of steepest ascent or descent generalizes to other normed linear spaces where we do not assume an inner product: all we need is the notion of the change in the function value relative to the distance from a starting point. For example, if \\(\\mathcal{V}\\) is \\(\\mathbb{R}^n\\) with the \\(\\ell^\\infty\\) norm, then the direction of steepest descent (or a direction, if \\(f'(x)\\) has some zero components) is given by a vector of \\(s\\) where \\(s_j = \\operatorname{sign}(f'(x)_j)\\); here \\(s\\) is the vector of unit \\(l^\\infty\\) norm such that \\(f'(x) s\\) is maximal.\n\n\n6.1.3 Second derivative test\nWhen \\(f: \\mathcal{V}\\rightarrow \\mathbb{R}\\) is twice differentiable, we can characterize the function near a stationary point \\(x\\) by the second derivative: \\[\n  f(x+u) = f(x) \\frac{1}{2} f''(x) (u \\otimes u) + o(\\|u\\|^2).\n\\] The point \\(x\\) is a local minimizer if the quadratic form given by \\(f''(x)\\) is positive definite and a local maximizer if \\(f''(x)\\) is negative definite. If the Hessian is semidefinite, we need to look at higher derivatives to classify \\(x\\). If \\(f''(x)\\) is strongly indefinite (i.e. the inertia has nonzero positive and negative components), then \\(x\\) is a saddle point.\nFor computation, of course, we usually express \\(f''(x)\\) with respect to a basis. In this case, we describe the second derivative test in terms of the inertia of the Hessian matrix \\(H_f(x)\\).\n\n\n6.1.4 Constraints and cones\nWe have described the first and second derivative tests in the context of unconstrained optimization, but the general approach is equally sensible for constrained optimization problems. We generally define the feasible domain \\(\\Omega\\) in terms of a set of constraints, equations and inequalities involving functions \\(c_i : \\mathcal{V}\\rightarrow \\mathbb{R}\\) that (for our purposes) are at least continuously differentiable: \\[\\begin{aligned}\n  c_i(x) &= 0, & i &\\in \\mathcal{E}, \\\\\n  c_i(x) & \\leq 0, & i &\\in \\mathcal{I}.\n\\end{aligned}\\] We say the \\(i\\)th inequality constraint is active at \\(x\\) if \\(c_i(x) = 0\\). We write the active set at \\(x\\) as \\[\n  \\mathcal{A}(x) = \\{ i \\in \\mathcal{I} : c_i(x) = 0 \\}.\n\\] We do not worry about trying to classify the equality constraints, though in a sense they are always active.\n\n6.1.4.1 Tangent cone\nThe first derivative test tells us that if \\(f\\) is locally minimized at \\(x\\), then a local linear model should not predict a nearby point with smaller function values. That is, there should not be \\(f'(x) u &lt; 0\\) and a (differentiable) feasible path \\(\\gamma : [0,\\tau) \\rightarrow\n\\mathcal{V}\\) with \\(\\gamma(0) = x\\) and \\(\\gamma'(0) = u\\), or we will have \\[\n  f(\\gamma(\\epsilon)) = f(x) + \\epsilon f'(x) u + o(\\epsilon) &lt; f(x).\n\\] for all sufficiently small \\(\\epsilon &gt; 0\\). The set of directions \\(u\\) that are tangent to some feasible path at \\(x\\) is known as the tangent cone at \\(x\\). A function \\(f\\) satisfies a (first order) geometric optimality condition at \\(x\\) if \\[\n  \\forall u \\in T(x), f'(x) u \\geq 0.\n\\] where \\(T(x)\\) denotes the tangent cone. Equivalently, \\[\n  -f'(x) \\in T(x)^o,\n\\] where \\(T(x)^o\\) is the polar cone of \\(T(x)\\), i.e. \\[\n  T(x)^o = \\{ w^* \\in \\mathcal{V}^* : \\forall u \\in T(x), w^* u \\leq 0 \\}.\n\\] A local minimizer must satisfy this condition, though not all points that satisfy this condition need be local minimizers.\n\n\n6.1.4.2 Linearized cone\nThe trouble with the geometric optimality condition is that we somehow need to characterize the tangent cone at each feasible point. Because we are characterizing the feasible set in terms of differentiable constraint functions, it is tempting to try to characterize feasible directions via the derivatives of the constraints by looking at the linearized cone \\[\\begin{aligned}\nL(x) =\n\\left\\{ u \\in \\mathcal{V}: \\right. \\quad &\n\\left( \\forall i \\in \\mathcal{E}, c_i'(x) u = 0 \\right)\n\\wedge \\\\\n& \\left. \\left( \\forall i \\in \\mathcal{A}(x), c_i'(x) u \\leq 0 \\right) \\right\\}.\n\\end{aligned}\\] It is true that \\(T(x) \\subseteq L(x)\\), and the polars satisfy \\(L(x)^o \\subseteq T(x)^o\\). Unfortunately, the linearized cone can be strictly bigger than the tangent cone. This is true even in 1D. For example, consider the domain \\(x \\geq 0\\) written as \\(c(x) = -x^3 \\leq 0\\). The tangent cone at \\(x = 0\\) is \\(T(0) = \\{ u \\in \\mathbb{R}: u \\geq 0 \\}\\), but the linearized cone is \\(L(0) = \\mathbb{R}\\). Hence, if we seek to minimize \\(f(x) = x\\) subject to \\(-x^3 \\leq 0\\), the point \\(x = 0\\) satisfies the geometric optimality condition, but the condition is not satisfied if we replace the tangent cone with the linearized cone.\n\n\n6.1.4.3 Constraint qualification\nA constraint qualification condition is a hypothesis on the constraints that guarantees that \\(L^o(x) = T^o(x)\\), so that we can use the linearized cone in lieu of the tangent cone in the geometric optimality condition. Two common examples are the linearly independent constraint qualification (LICQ), which holds at \\(x\\) if \\[\n  \\{ c_i'(x) : i \\in \\mathcal{A}(x) \\cup \\mathcal{E} \\}\n  \\mbox{ is linearly independent.}\n\\] and the Mangasarian-Fromovitz constraint qualification (MFCQ), which holds at \\(x\\) if \\[\\begin{aligned}\n  & \\{ c_i'(x) : i \\in \\mathcal{E} \\} \\mbox{ is linearly independent and} \\\\\n  & \\exists u \\in \\mathcal{V} : c_i'(x) u &lt; 0 \\mbox{ for } i \\in\n  \\mathcal{A}(x) \\mbox{ and } c_i'(x) u = 0 \\mbox{ for } i \\in \\mathcal{E}.\n\\end{aligned}\\] Note that our example of the constraint \\(-x^3 \\leq 0\\) satisfies neither LICQ nor MFCQ at \\(0\\).\n\n\n\n6.1.5 Multipliers and KKT\nWe have already seen one theorem of alternatives in our discussion of linear algebra — the Fredholm alternative theorem, which deals with solvability of linear equations. There are many other theorems of alternatives for dealing with inequalities, associated with mathematicians like Motzkin, Kuhn, and Farkas. One such theorem is Farkas’ lemma: if \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(b \\in \\mathbb{R}^m\\), then exactly one of the two statements is true:\n\nThere exists \\(x \\in \\mathbb{R}^n\\) such that \\(Ax = b\\) and \\(x \\geq 0\\)\nThere exists \\(y \\in \\mathbb{R}^m\\) such that \\(y^* A \\geq 0\\) and \\(y^* b &lt; 0\\)\n\nHere the inequalities are taken elementwise.\nUsing Farkas’s lemma, one can rewrite the polar of the linearized cone as \\[\n  L(x)^o =\n  \\left\\{\n  w^* \\in \\mathcal{V}^* :\n  w^* =\n  \\sum_{i \\in \\mathcal{E}} \\lambda_i c_i'(x) +\n  \\sum_{i \\in \\mathcal{A}(x)} \\mu_i c_i'(x), \\mu_i \\geq 0\n  \\right\\}.\n\\] It is usually convenient to define \\(\\mu_i = 0\\) for inequality constraints that are inactive; in this case, we can rewrite \\[\\begin{aligned}\n  L(x)^o = \\{ w^* \\in \\mathcal{V}^* : \\quad\n  & w^* = \\lambda^* c_{\\mathcal{E}}'(x) + \\mu^* c_{\\mathcal{I}}'(x), \\\\\n  &  \\mu^* c_{\\mathcal{I}}(x) = 0, \\mu^* \\geq 0 \\},\n\\end{aligned}\\] where \\(c_{\\mathcal{E}}(x)\\) and \\(c_{\\mathcal{I}}(x)\\) are (column) vectors of equality and inequality constraint functions and \\(\\lambda^*\\) and \\(\\mu^*\\) are concrete row vectors. The statement that \\(\\mu^* c_{\\mathcal{I}}(x) = 0\\), typically called a complementary slackness condition is equivalent to saying that inactive inequality constraints (where \\(c_i(x) &lt; 0\\)) must be associated with zero multipliers.\nWith this machinery in place, we can now rewrite the condition \\(-f'(x) \\in L(x)^o\\) in the form that we usually use. We define the Lagrangian function \\[\n  \\mathcal{L}(x, \\mu^*, \\lambda^*) =\n  f(x) + \\lambda^* c_{\\mathcal{E}}(x) + \\mu^* c_{\\mathcal{I}}(x)\n\\] The variables \\(\\lambda^*\\) and \\(\\mu^*\\) that multiply the constraints are known as Lagrange multipliers. The Karush-Kuhn-Tucker (KKT) conditions at \\(x_*\\), equivalent to \\(-f'(x_*) \\in L(x^*)^o\\) are \\[\\begin{aligned}\n  D_1 \\mathcal{L}(x_*, \\lambda, \\mu) &= 0\n  & \\mbox{constrained stationarity}\\\\\n  c_{\\mathcal{E}}(x_*) &= 0,\n  & \\mbox{primal feasibility (equality)}\\\\\n  c_{\\mathcal{I}}(x_*) & \\leq 0,\n  & \\mbox{primal feasibility (inequality)}\\\\\n  \\mu & \\geq 0,\n  & \\mbox{dual feasibility}\\\\\n  \\mu^* c_{\\mathcal{I}}(x_*) &= 0,\n  & \\mbox{complementary slackness.}\n\\end{aligned}\\] When a constraint qualification condition holds, the KKT conditions are necessary first-order conditions for optimality at \\(x_*\\).\n\n\n6.1.6 Multipliers and adjoints\nNow suppose that we wanted to find a minimum (or maximum or other stationary point) of \\(y\\) subject to the equations we saw in Section 5.4.7 \\[\\begin{aligned}\n  u-f(x) &= 0 \\\\\n  v-g(x,u) &= 0 \\\\\n  y-h(x,u,v) &= 0\n\\end{aligned}\\] The Lagrangian for this system is \\[\n  \\mathcal{L} =\n  y -\n  \\bar{u}^* (u-f(x)) -\n  \\bar{v}^* (v-g(x,u)) -\n  \\bar{y}^* (y-h(x,u,v))\n\\] where \\(x, u, v, y\\) are the primal variables and \\(\\bar{u}^*, \\bar{v}^*,\n\\bar{y}^*\\) are multipliers1. Stationarity gives \\[\n  \\begin{bmatrix} \\bar{u}^* & \\bar{v}^* & \\bar{y}^* \\end{bmatrix}\n  \\begin{bmatrix}\n  -D_1 f & 1 & 0 & 0 \\\\\n  -D_1 g & -D_2 g & 1 & 0 \\\\\n  -D_1 h & -D_2 h & -D_3 h & 1\n  \\end{bmatrix} =\n  \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}.\n\\] We can write this equivalently as \\(\\bar{x}^* = 0\\) where \\[\n  \\begin{bmatrix} \\bar{x}^* & \\bar{u}^* & \\bar{v}^* & \\bar{y}^* \\end{bmatrix}\n  \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\n  -D_1 f & 1 & 0 & 0 \\\\\n  -D_1 g & -D_2 g & 1 & 0 \\\\\n  -D_1 h & -D_2 h & -D_3 h & 1\n  \\end{bmatrix} =\n  \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}.\n\\] This is precisely the set of equations that we saw in Section 5.4.7, except that where we said “dual variables” in Section 5.4.7, here we are saying (negative) “Lagrange multipliers.”\n\n\n6.1.7 Mechanical analogies\n\n\n6.1.8 Constrained second derivatives\nThe second-order geometric optimality condition for a twice-differentiable \\(f\\) at \\(x\\) is a straightforward extension of the first-order geometric optimality condition. If \\(x\\) satisfies the first-order condition \\[\n  \\forall u \\in T(x), f'(x) u \\geq 0,\n\\] and we satisfy that \\[\n  \\forall 0 \\neq u \\in T(x) \\mbox{ s.t. } f'(x) u = 0,\n  f''(x) (u \\otimes u) &gt; 0,\n\\] then \\(x\\) is a constrained local minimum. If there are nonzero directions \\(u\n\\in T(x)\\) where \\(f'(x) u = 0\\) and \\(f'(x) (u \\otimes u) &lt; 0\\), then \\(x\\) is not a constrained local minimum. Otherwise, we cannot determine minimality without looking at higher derivatives.\nAs with the geometric first-order condition, the geometric second-order condition is complicated by the need to get a handle on the tangent cone. Assuming that we satisfy the linearly independent constraint qualification condition at \\(x\\), we have a more straightforward version of the second derivative test. Let \\[\n  \\mathcal{J} = \\mathcal{E} \\cup \\{ i \\in \\mathcal{I} : \\mu_i &gt; 0 \\}.\n\\] Then a sufficient condition for \\(x\\) to be a strong constrained local minimizer is if \\[\n  \\forall \\mbox{ nonzero } u \\in \\mathcal{N}(c'_{\\mathcal{J}}(x)), \\quad\n  f''(x) (u \\otimes u) &gt; 0.\n\\] That is, the Hessian should be positive definite in all directions that are not already “uphill” at first order. Such a condition is known as conditional positive definiteness, since \\(f''(x)\\) is positive definite conditioned on some constraints on the directions considered. If there are such directions for which \\(f''(x) (u \\otimes\nu) &lt; 0\\), then we do not have a local minimizer; otherwise, we may need to consider higher derivatives to make a diagnosis.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimization theory</span>"
    ]
  },
  {
    "objectID": "00-Background/05-Optimization.html#vector-optimization",
    "href": "00-Background/05-Optimization.html#vector-optimization",
    "title": "6  Optimization theory",
    "section": "6.2 Vector optimization",
    "text": "6.2 Vector optimization",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimization theory</span>"
    ]
  },
  {
    "objectID": "00-Background/05-Optimization.html#convexity",
    "href": "00-Background/05-Optimization.html#convexity",
    "title": "6  Optimization theory",
    "section": "6.3 Convexity",
    "text": "6.3 Convexity\nA set \\(\\Omega \\subset \\mathcal{V}\\) is convex if every line segment between points in \\(\\Omega\\) also lies in \\(\\Omega\\), i.e. \\[\n  \\forall x, y \\in \\Omega, \\forall s \\in [0,1], (1-s)x + s y \\in \\Omega.\n\\] A function \\(f : \\Omega \\subset \\mathcal{V}\\rightarrow \\mathbb{R}\\) is convex if the graph of \\(f\\) on a line segment in \\(\\Omega\\) always lies below the secant connecting the endpoint values: \\[\n  \\forall x, y \\in \\Omega, \\forall s \\in [0,1],\n  f((1-s) x + sy) \\leq (1-s) f(x) + s f(y).\n\\] We say \\(f\\) is strictly convex if the inequality is strict on the interior of the segment connecting \\(x\\) and \\(y\\): \\[\n  \\forall x \\neq y \\in \\Omega, \\forall s \\in (0,1),\n  f((1-s) x + sy) &lt; (1-s) f(x) + s f(y).\n\\] If \\(f\\) is twice differentiable, convexity corresponds to positive semi-definiteness of the Hessian, and strict convexity corresponds to positive definiteness of the Hessian. However, functions can be convex even if they are not twice differentiable.\nA convex function on a closed, bounded set in finite dimensions is guaranteed to take on its minimum value \\(f_{\\min}\\) at some point \\(x\n\\in \\Omega\\). There are no local minimizers, only global minimizers. The set of global minimizers \\(\\{x \\in \\Omega : f(x) = f_{\\min}\\}\\) is itself a convex set. If \\(f\\) is strictly convex, then the global minimizer is unique.\n\n6.3.1 Subderivatives\nA subderivative of a convex \\(f\\) at \\(x \\in \\Omega\\) is the set of functionals \\(w^* \\in \\mathcal{V}\\) such that \\(f(x+u) \\geq f(x) + w^* u\\) for all \\(x + u \\in \\Omega\\); that is, the subderivative corresponds to the set of “supporting hyperplanes” that agree with \\(f(x)\\) and lie below \\(f\\) elsewhere on \\(\\Omega\\). When \\(\\mathcal{V}\\) is an inner product space, we say \\(w \\in \\mathcal{V}\\) is in the subgradient at \\(x \\in \\Omega\\) if \\(f(x+u)\n\\geq f(x) + \\langle u, w \\rangle\\) whenever \\(x + u \\in \\Omega\\).\nWhen \\(f\\) is differentiable at \\(x\\), the only element of the subderivative is the derivative (and the only element of the subgradient is the gradient). However, the concept of a subderivative continues to make sense even for nonsmooth functions. For example, the absolute value function \\(x \\mapsto |x|\\) on \\(\\mathbb{R}\\) is not differentiable at 0, but has a subgradient \\([-1,1]\\).\nThe notion of a subderivative allows us to generalize the usual notion of stationary points for differentiable functions: a point \\(x \\in \\Omega\\) is a stationary point for a convex \\(f\\) if the subderivative of \\(f\\) at \\(x\\) contains the \\(0 \\in \\mathcal{V}^*\\), and a minimizer must be a stationary point.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimization theory</span>"
    ]
  },
  {
    "objectID": "00-Background/05-Optimization.html#footnotes",
    "href": "00-Background/05-Optimization.html#footnotes",
    "title": "6  Optimization theory",
    "section": "",
    "text": "For equality constraints, the signs of the Lagrange multipliers are unimportant.↩︎",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimization theory</span>"
    ]
  },
  {
    "objectID": "00-Background/06-Probability.html",
    "href": "00-Background/06-Probability.html",
    "title": "7  Probability",
    "section": "",
    "text": "I assume that you have seen some probability theory before, and that this is just a reminder. If you need a more thorough refresher, the book by Ross (Ross 2014) is a popular introductory text that covers discrete and continuous problems, but not more general probability measures. Another good undergraduate text by Chung and AitSahlia (Chung and AitSahlia 2003) includes a little bit of measure theory. Good graduate texts include the books by Billingsley (Billingsley 1995) and by Breiman (Breiman 1992). If you want a reminder that is more thorough than the one we give here, but less than a full textbook, the treatment in (Deisenroth, Faisal, and Ong 2020) is a good starting point.\n\nAxiomatic probability, counting, and measure\nConditional and marginal probabilities\nBayesian and frequentist perspectives\nRandom variables\nThe wonder of Gaussians (and not all RVs are Gaussian!)\nMoments (+ linearity of expectations, variance and precision,heteroscedasticity, etc) and tails\nIndependence, conditional independence, factorization of probabilities, graphical models\nStandard statistics (incl variance and precision)\nCentral limit theorem / LLN / sums\nParameter estimation\nBayes and updating\nConjugate priors\nUninformative priors\nMarkov chains\nMartingales\nStochastic processes\nStandard inequalities and concentration of measure\n\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. Third. Wiley Series in Probability and Mathematical Statistics. Wiley.\n\n\nBreiman, Leo. 1992. Probability. SIAM. https://doi.org/10.1137/1.9781611971286.\n\n\nChung, Kai Lai, and Farid AitSahlia. 2003. Elementary Probability Theory. Undergraduate Texts in Mathematics. Springer. https://doi.org/10.1007/978-0-387-21548-8.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge University Press. https://mml-book.com.\n\n\nRoss, Sheldon. 2014. A First Course in Probability. Ninth. Pearson.",
    "crumbs": [
      "Background Plus a Bit",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/01-Error.html",
    "href": "01-Fund1d/01-Error.html",
    "title": "8  Notions of Error",
    "section": "",
    "text": "Error, uncertainty, etc - known unknowns vs unknown unknowns\nSources of error\nModels of error (worst case, statistical, etc)\nRelative and absolute errors\nMixed error\nPropagation of error\nSensitivity and conditioning\nForward, backward, and residual error\nApplications in data analysis\n\n“Explanatory, not exculpatory” Floating point is not everything!\nTesting and debugging numerical codes - Testing vs debugging (draw a picture) - Support for approx, testing framework in the standard library - Random tests and hard tests - Testing from the ground up - It’s not all floating point! - Saving random seeds",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Notions of Error</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/02-Arithmetic.html",
    "href": "01-Fund1d/02-Arithmetic.html",
    "title": "9  Floating Point",
    "section": "",
    "text": "Fundamentals of floating point (and fixed point!)\nFloating point formats\nBasic arithmetic operations, rules for IEEE\nExceptional conditions\nError analysis\nDesign principles\nMultiple precision computations\nSimulated extra precision",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Floating Point</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/03-Approximation.html",
    "href": "01-Fund1d/03-Approximation.html",
    "title": "10  Approximation",
    "section": "",
    "text": "Inputs: function values, sampling oracle (values, derivs, integrals, etc)\nOutput: Approximating function, often used for other tasks. Plus some notion of error!\nQ to ask: what data is available? Noise? What is assumed about fn (smoothness, derivatives, etc)?\nWhat is needed from approximator (derivatives?). How fast is fitting? Evaluation? Convergence?\nDiscuss general theory and influence of noise later. Focus here on piecewise polynomials and interpolation thereof.\nPolynomial interpolation and regression\nDifferent forms of the interpolation problem\nChoice of basis and numerical issues, computational speed\nError analysis\nPiecewise polynomials: splines and more\nKernel-based approximation\nWhich space to use?\nRational approximation\nApproximation of densities",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Approximation</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/04-AutoDiff.html",
    "href": "01-Fund1d/04-AutoDiff.html",
    "title": "11  Automatic Differentiation",
    "section": "",
    "text": "11.1 Dual numbers\nComputing with variations is not only useful for pen-and-paper computations; it is also the basis for automatic differentiation with so-called dual numbers. We describe some of the fundamentals of working with dual numbers below. A more full-featured implementation of the dual numbers is given in the ForwardDiff.jl package.",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/04-AutoDiff.html#dual-numbers",
    "href": "01-Fund1d/04-AutoDiff.html#dual-numbers",
    "title": "11  Automatic Differentiation",
    "section": "",
    "text": "11.1.1 Scalar computations\nA dual number is a pair \\((x, \\delta x)\\) consisting of a value \\(x\\) and a variation \\(\\delta x\\). As above, above, we can think of this as a function \\(\\tilde{x}(\\epsilon) = x + \\epsilon \\, \\delta x + o(\\epsilon)\\). For ordinary scalar numbers, we represent this with a Julia structure.\n\nstruct Dual{T&lt;:Number} &lt;: Number\n    val :: T  # Value\n    δ :: T    # Variation\nend\n\nvalue(x :: Dual) = x.val\nvariation(x :: Dual) = x.δ\n\nWe want to allow constants, which are dual numbers with a zero variation. We can construct these directly or convert them from other numbers.\n\nDual{T}(x :: T) where {T} = Dual{T}(x, zero(T))\nBase.convert(::Type{Dual{T}}, x :: S) where {T,S&lt;:Number} =\n    Dual{T}(x, zero(T))\n\nWe also want to be able to convert between types of Julia dual numbers, and we want promotion rules for doing arithmetic that involves dual numbers together with other types of numbers.\n\nBase.convert(::Type{Dual{T}}, x :: Dual{S}) where {T,S} =\n    Dual{T}(x.val, x.δ)\nBase.promote_rule(::Type{Dual{T}}, ::Type{Dual{S}}) where {T,S} =\n    Dual{promote_type(T,S)}\nBase.promote_rule(::Type{Dual{T}}, ::Type{S}) where {T,S&lt;:Number} =\n    Dual{promote_type(T,S)}\n\nWe would like to define a variety of standard operations (unary and binary) for dual numbers. Rather than writing the same boilerplate code for every function, we write macros @dual_unary and @dual_binary that take the name of an operation and the formula for the derivative in terms of \\(x, \\delta x, y, \\delta y\\), where \\(x\\) and \\(y\\) are the first and second arguments, respectively.\n\nmacro dual_unary(op, formula)\n    :(function $op(x :: Dual)\n          x, δx = value(x), variation(x)\n          Dual($op(x), $formula)\n      end)\nend\n\nmacro dual_binary(op, formula)\n    :(function $op(x :: Dual, y :: Dual)\n          x, δx = value(x), variation(x)\n          y, δy = value(y), variation(y)\n          Dual($op(x, y), $formula)\n      end)\nend\n\nWe overload the +, -, and * operators to work with dual numbers, using the usual rules of differentiation. We also overload both the left and right division operations and the exponentiation operation.\n\n@dual_binary(Base.:+, δx + δy)\n@dual_binary(Base.:-, δx - δy)\n@dual_unary(Base.:-, -δx)\n@dual_binary(Base.:*, δx*y + x*δy)\n@dual_binary(Base.:/, (δx*y - x*δy)/y^2)\n@dual_binary(Base.:\\, (δy*x - y*δx)/x^2)\n@dual_binary(Base.:^, x^y*(y*δx/x + log(x)*δy))\n\nWe provide a second version of the power function for when the exponent is a constant integer (as is often the case); this allows us to deal with negative values of \\(x\\) gracefully.\n\nBase.:^(x :: Dual, n :: Integer) = Dual(x.val^n, n*x.val^(n-1)*x.δ)\n\nFor comparisons, we will only consider the value and ignore the variation.\n\nBase.:(==)(x :: Dual, y :: Dual)  = x.val == y.val\nBase.isless(x :: Dual, y :: Dual) = x.val &lt; y.val\n\nFor convenience, we also write a handful of the standard functions that one learns to differentiate in a first calculus course.\n\n@dual_unary(Base.abs,  δx*sign(x))\n@dual_unary(Base.sqrt, δx/sqrt(x)) \n@dual_unary(Base.exp,  exp(x)*δx)\n@dual_unary(Base.log,  δx/x) \n@dual_unary(Base.sin,  cos(x)*δx)\n@dual_unary(Base.cos, -sin(x)*δx) \n@dual_unary(Base.asin, δx/sqrt(1-x^2)) \n@dual_unary(Base.acos,-δx/sqrt(1-x^2)) \n\nWith these definitions in place, we can automatically differentiate through a variety of functions without writing any special code. For example, the following code differentiates the Haaland approximation to the Darcy-Weisbach friction factor in pipe flow and compares to a finite difference approximation:\n\nlet\n    fhaaland(ϵ, D, Re) = 1.0/(-1.8*log( (ϵ/D/3.7)^1.11 + 6.9/Re ))^2\n    δy = fhaaland(0.01, 1.0, Dual(3000, 1)).δ\n    δy_fd = finite_diff(Re-&gt;fhaaland(0.01, 1.0, Re), 3000, h=1e-3)\n    isapprox(δy, δy_fd, rtol=1e-6)\n end\n\ntrue\n\n\n\n\n11.1.2 Matrix computations\nWe can also use dual numbers inside of some of the linear algebra routines provided by Julia. For example, consider \\(x = A^{-1} b\\) where \\(b\\) is treated as constant; the following code automatically computes both \\(A^{-1} b\\) and \\(-A^{-1} (\\delta A) A^{-1} b\\).\n\nfunction test_dual_solve()\n    Aval = [1.0 2.0; 3.0 4.0]\n    δA   = [1.0 0.0; 0.0 0.0]\n    b = [3.0; 4.0]\n    A = Dual.(Aval, δA)\n    x = A\\b\n\n    # Compare ordinary and variational parts to formulas\n    value.(x) ≈ Aval\\b &&\n    variation.(x) ≈ -Aval\\(δA*(Aval\\b))\nend\n\ntest_dual_solve()\n\ntrue\n\n\nWhile this type of automatic differentiation through a matrix operation works, it is relatively inefficient. We can also define operations that act at a matrix level for relatively expensive operations like matrix multiplication and linear solves. We give as an example code in Julia for more efficient linear solves with matrices of dual numbers, using LU factorization (Chapter 16) as the basis for applying \\(A^{-1}\\) to a matrix or vector:\n\nfunction Base.:\\(AA :: AbstractMatrix{Dual{T}},\n                 BB :: AbstractVecOrMat{Dual{S}}) where {T,S}\n    A, δA = value.(AA), variation.(AA)\n    B, δB = value.(BB), variation.(BB)\n    F = lu(A)\n    X = F\\B\n    Dual.(X, F\\(δB-δA*X))\nend\n\nfunction Base.:\\(AA :: AbstractMatrix{Dual{T}},\n                 B :: AbstractVecOrMat{S}) where {T,S}\n    A, δA = value.(AA), variation.(AA)\n    F = lu(A)\n    X = F\\B\n    Dual.(X, F\\(-δA*X))\nend\n\nfunction Base.:\\(A :: AbstractMatrix{T},\n                 BB :: AbstractVecOrMat{Dual{S}}) where {T,S}\n    B, δB = value.(BB), variation.(BB)\n    F = lu(A)\n    Dual.(F\\B, F\\δB)\nend\n\ntest_dual_solve()\n\ntrue\n\n\n\n\n11.1.3 Special cases\nThere are other cases as well where automatic differentiation using dual numbers needs a little help. For example, consider the thin-plate spline function, which has a removable singularity at zero:\n\nϕtps(r) = r == 0.0 ? 0.0 : r^2*log(abs(r))\n\nIf we treat \\(r\\) as a dual number, the output for \\(r = 0\\) will be an ordinary floating point number, while the output for every other value of \\(r\\) will be a dual number. However, we can deal with this by writing a specialized version of the function for dual numbers.\n\nϕtps(r :: Dual) = r == 0.0 ? Dual(0.0, 0.0) : r^2*log(r)\n\nWith this version, the function works correctly at both zero and nonzero dual number arguments:\n\nϕtps(Dual(0.0, 1.0)), ϕtps(Dual(1.0, 1.0))\n\n(Dual{Float64}(0.0, 0.0), Dual{Float64}(0.0, 1.0))\n\n\nIn addition to difficulties with removable singularities, automatic differentiation systems may lose accuracy do to floating point effects even for functions that are well-behaved. We return to this in Chapter 9.",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/04-AutoDiff.html#forward-and-backward",
    "href": "01-Fund1d/04-AutoDiff.html#forward-and-backward",
    "title": "11  Automatic Differentiation",
    "section": "11.2 Forward and backward",
    "text": "11.2 Forward and backward\nIn the previous section, we described automatic differentiation by tracking variations (dual numbers) through a computation, often known as forward mode differentiation. An alternate approach known as backward mode (or adjoint mode) differentiation involves tracking a different set of variables (dual variables)\n\n11.2.1 An example\nWe consider again the sample function given in the previous section: \\[\n  f(x_1, x_2) =\n  \\left( -1.8 \\log (x_1/3.7)^{1.11} + 6.9/x_2 \\right)^{-2}.\n\\] We would usually write this concisely as\n\nfhaaland0(x1, x2) = (-1.8*log( (x1/3.7)^1.11 + 6.9/x2 ))^-2\n\nWhen we compute or manipulate such somewhat messy expressions (e.g. for differentiation), it is useful to split them into simpler subexpressions (as in the single static assignment (SSA) style introduced in Chapter 2). For example, we can rewrite \\(f(x_1, y_1)\\) in terms of seven intermediate expressions, and compute variations of each intermediate to get a variation of the final result. In code, this is\n\nfunction fhaaland1(x1, x2)\n    y1 = x1/3.7\n    y2 = y1^1.11\n    y3 = 6.9/x2\n    y4 = y2+y3\n    y5 = log(y4)\n    y6 = -1.8*y5\n    y7 = y6^-2\n    function Df(δx1, δx2)\n        δy1 = δx1/3.7\n        δy2 = 1.11*y1^0.11 * δy1\n        δy3 = -6.9/x2^2 * δx2\n        δy4 = δy2+δy3\n        δy5 = δy4/y4\n        δy6 = -1.8*δy5\n        δy7 = -2*y6^(-3)*δy6\n    end\n    y7, Df\nend\n\nThe function Df inside fhaaland1 computes derivatives similarly to using dual numbers as in the previous section.\nAnother way to think of this computation is that we have solved for the intermediate \\(y\\) variables as a function of \\(x\\) from the relationship \\[\n  G(x, y) = h(x,y) - y = 0,\n\\] where the rows of \\(h\\) are the seven equations above, e.g. \\(h_1(x,y) = x_1/3.7\\). Differentiating this relation gives us \\[\n  D_1 h(x,y) \\, \\delta x + \\left( D_2 h(x,y) - I \\right) \\, \\delta y = 0.\n\\] The formula for the variations in the \\(y\\) variables can be thought of as coming from using forward substitution to solve the linear system \\[\n  (D_2 h(x,y) -I) \\delta y = -D_1 h(x,y) \\delta x.\n\\] That is, we compute the components of \\(\\delta y\\) in order by observing that \\(h_i(x,y)\\) depends only on \\(y_1, \\ldots, y_{i-1}\\) and writing \\[\n  \\delta y_i\n  = D_1 h_i(x,y) \\, \\delta x +\n    \\sum_{j=1}^{i=1} \\left( D_2 h_i(x,y) \\right)_j \\, \\delta y_j.\n\\]\nA key observation is that we do not then use all of \\(\\delta y\\); we only care about \\[\n  f'(x) \\delta x = w^* \\delta y,\n\\] where \\(w^*\\) is a functional that selects the desired output (here \\(w^* = e_7^*\\)). Putting the steps of the previous computation together, we have \\[\n  f'(x) \\, \\delta x =\n  w^* \\left[ \\left( D_2 h(x,y) - I)^{-1} \\left( -D_1 h(x_y) \\, \\delta x\n  \\right) \\right) \\right].\n\\] But associativity, we could also write this as \\[\n  f'(x) \\, \\delta x =\n  \\left[ \\left( -w^* (D_2 h(x,y) - I)^{-1} \\right) D_1 h(x,y) \\right]\n  \\, \\delta x.\n\\] Giving names to the parenthesized pieces of this latter equation, we have \\[\\begin{aligned}\n  \\bar{y}^* &= -w^* (D_2 h(x,y) - I)^{-1} \\\\\n  f'(x) &= \\bar{y}^* D_1 h(x,y).\n\\end{aligned}\\] Where we solved the system for the variations \\(\\delta y\\) by forward substitution, we solve the system dual variables \\(\\bar{y}^*\\) by backward substitution, applying the formula \\[\n  \\bar{y}_j^* = w_j^* + \\sum_{i=j+1}^n \\bar{y}_i^* (D_2 h_i(x,y))_j\n\\] for \\(j = m, m-1, \\ldots, 1\\). Because this computation can be written in terms of a solve with the adjoint matrix \\((D_2 h(x,y)-I)^*\\) using backward substitution, this method of computing derivatives is sometimes called adjoint mode or backward mode differentiation.\nTranslating these ideas into code for our example function, we have\n\nfunction fhaaland2(x1, x2)\n    y1 = x1/3.7\n    y2 = y1^1.11\n    y3 = 6.9/x2\n    y4 = y2+y3\n    y5 = log(y4)\n    y6 = -1.8*y5\n    y7 = y6^-2\n    \n    ȳ7 = 1\n    ȳ6 = ȳ7 * (-2*y6^-3)\n    ȳ5 = ȳ6 * (-1.8)\n    ȳ4 = ȳ5/y4\n    ȳ3 = ȳ4\n    ȳ2 = ȳ4\n    ȳ1 = ȳ2 * (1.11*y1^0.11)\n\n    Df = [ȳ1/3.7  ȳ3*(-6.9/x2^2)]\n    y7, Df\nend\n\nWe now have three different methods for computing the derivative of \\(f\\), which we illustrate below (as well as checking that our computations agree with each other).\n\nlet\n    # Compute f and the derivative using dual numbers\n    f0a = fhaaland0(Dual(0.01, 1.0), 3000)\n    f0b = fhaaland0(0.01, Dual(3000, 1))\n    Df0 = [variation(f0a) variation(f0b)]\n\n    # Compute using the SSA-based forward mode code\n    f1, Df1fun = fhaaland1(0.01, 3000)\n    Df1 = [Df1fun(1,0) Df1fun(0,1)]\n\n    # Compute using the SSA-based backward mode code\n    f2, Df2 = fhaaland2(0.01, 3000)\n\n    # Compare to see that all give the same results\n    f1 == f2 && f1 ≈ value(f0a) && f1 ≈ value(f0b) &&\n    Df1[1] ≈ Df2[1] && Df0[1] ≈ Df2[1] &&\n    Df1[2] ≈ Df2[2] && Df0[2] ≈ Df2[2]\nend\n\ntrue\n\n\nWe note that we only need to compute the \\(\\bar{y}\\) variables once to get the derivative vector, where for forward mode we need to compute the \\(\\delta y\\) variables twice.\nThe cost of computing the derivative in one direction \\((\\delta x_1, \\delta y_2)\\) is only about as great as the cost of evaluating \\(f\\). But if we wanted the derivative with respect to both \\(x_1\\) and \\(x_2\\), we would need to run at least the code in Df twice (and with the dual number implementation from the previous section, we would also run the evaluation of \\(f\\) twice).",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/04-AutoDiff.html#a-derivative-example",
    "href": "01-Fund1d/04-AutoDiff.html#a-derivative-example",
    "title": "11  Automatic Differentiation",
    "section": "11.3 A derivative example",
    "text": "11.3 A derivative example\nBeyond writing language utilities, Julia macros are useful for writing embedded domain-specific languages (DSLs) for accomplishing particular tasks. In this setting, we are really writing a language interpreter embedded in Julia, using the Julia parse tree as an input and producing Julia code as output.\nOne example has to do with automatic differentiation of simple code expressions, where we are giving another interpretation to simple Julia expressions. This is a more ambitious example, and can be safely skipped over. However, it is also a useful example of a variety of features in Julia. Some types of automatic differentiation can be done in arguably simpler and more powerful ways without writing macros, and we will return to this in later chapters.\n\n11.3.1 Normalization\nWe define a “simple” expression as one that only involves only:\n\nLiteral nodes (including symbols),\nExpr nodes of call type, including arithmetic and function calls,\nAssignment statements,\nTuple constructors,\nbegin/end blocks.\n\nWe can test for simple expressions by recursing through an expression tree\n\n# Simple expression nodes\nis_simple(e :: Expr) =\n    e.head in [:call, :(=), :tuple, :block] &&\n    all(is_simple.(e.args))\n\n# Other nodes are literal, always simple\nis_simple(e) = true\n\nIn addition to insisting that expressions are simple, we also want to simplify some peculiarities in Julia’s parsing of addition and multiplication. In many languages, an expression like 1 + 2 + 3 is parsed as two operations: (1 + 2) + 3. In Julia, this results in a single node. It will simplify our life to convert these types of nodes to binary nodes, assuming left associativity of the addition operation. We do this by recursively rewriting the expression tree to replace a particular type of operation node (calling op) with a left-associated version of that same node:\n\nleftassoc(op :: Symbol, e) = e\nfunction leftassoc(op :: Symbol, e :: Expr)\n    args = leftassoc.(op, e.args)\n    if e.head == :call && args[1] == op\n        foldl((x,y) -&gt; Expr(:call, op, x, y), args[2:end])\n    else\n        Expr(e.head, args...)\n    end\nend\n\nWe are particularly interested in left associativity of addition and multiplication, so we write a single function for those operations\n\nleftassoc(e) = leftassoc(:+, leftassoc(:*, e))\n\nWe can see the effect by printing the parse of a particular example:\n\nlet\n    e = :(a*b*c + 2 + 3)\n    println(\"Before: $e\")\n    println(\"After:  $(leftassoc(e))\")\nend\n\nBefore: a * b * c + 2 + 3\nAfter:  ((a * b) * c + 2) + 3\n\n\nFinally, the Julia parser adds LineNumberNodes to blocks in order to aid with debugging. We will dispose of those here so that we can focus solely on processing expressions.\n\nfilter_line(e) = e\nfilter_line(e :: Expr) =\n    Expr(e.head,\n         filter(x-&gt;!(x isa LineNumberNode),\n                filter_line.(e.args))...)\n\nPutting everything together, we will normalize simple expressions by eliminating line numbers and re-associating addition and multiplication.\n\nnormalize_expr(e) = e\nfunction normalize_expr(e :: Expr)\n    @assert is_simple(e) \"Expression must be simple\"\n    leftassoc(filter_line(e))\nend\n\n\n\n11.3.2 SSA\nSimple expressions of the type that we have described can be converted to static single assignment (SSA) form, where each intermediate subexpression is assigned a unique local name (which we produce with a call to gensym). We represent a program in SSA form as a vector of (symbol, expression) pairs to be interpreted as “evaluate the expression and assign it to the symbol.” We emit SSA terms by appending them to the end of a list.\n\nfunction ssa_generator()\n    result = []\n\n    # Add a single term to the SSA result\n    function emit!(s, e)\n        push!(result, (s,e))\n        s\n    end\n\n    # Add several terms to the SSA result;\n    #  s is the designated \"final result\"\n    function emit!(s, l :: Vector)\n        append!(result, l)\n        s\n    end\n\n    # Emit a term with a new local name\n    emit!(e) = emit!(gensym(), e)\n\n    result, emit!\nend\n\nThe to_ssa function returns a final result in SSA form Each symbol is only assigned once. Each arithmetic, function call, and tuple constructor results in a new symbol. For assignments, we record a binding of the left hand side symbol to the result computed for the right hand side.\n\nfunction to_ssa(e :: Expr)\n    \n    # Check and normalize the expression\n    e = normalize_expr(e)\n\n    # Set up results list and symbol table\n    elist, emit! = ssa_generator()\n    symbol_table = Dict{Symbol,Any}()\n\n    # Functions for recursive processing\n    process(e) = e\n    process(e :: Symbol) = get(symbol_table, e, e)\n    function process(e :: Expr)\n        args = process.(e.args)\n        if e.head == :block\n            args[end]\n        elseif e.head == :(=)\n            symbol_table[e.args[1]] = args[2]\n            args[2]\n        else\n            emit!(Expr(e.head, args...))\n        end\n    end\n\n    process(e), elist\nend\n\nIn case we want to convert a single leaf to SSA form, we define a second method:\n\nfunction to_ssa(e)\n    s = gensym()\n    s, [(s,e)]\nend\n\nWe also want a utility to compute from SSA back to ordinary Julia code\n\nfrom_ssa(sym, elist) =\n    Expr(:block,\n         [Expr(:(=), s, e) for (s,e) in elist]...,\n         sym)\n\nfrom_ssa (generic function with 1 method)\n\n\nAs is often the case, an example is useful to illustrate the code.\n\nlet\n    s, elist = to_ssa(\n        quote\n            x = 10\n            x = 10+x+1\n            y = x*x\n        end)\n    from_ssa(s, elist)\nend\n\nquote\n    var\"##230\" = 10 + 10\n    var\"##231\" = var\"##230\" + 1\n    var\"##232\" = var\"##231\" * var\"##231\"\n    var\"##232\"\nend\n\n\n\n\n11.3.3 Derivative function\nTo set up our differentiation function, we will need some differentiation rules. We store this in a dictionary where the keys are different types of operations and functions, and the arguments are the arguments to those functions and their derivatives. Each rule takes expressions for the value of the function, the arguments, and the derivatives of the arguments, and then returns the derivative of the function. In some cases, we may want different methods associated with the same function, e.g. to distinguish between negation and subtraction or to provide a special case of the power function where the exponent is an integer constant.\n\nderiv_minus(f,x,dx) = :(-$dx)\nderiv_minus(f,x,y,dx,dy) = :($dx-$dy)\nderiv_pow(f,x,n :: Int, dx, dn) = :($n*$x^$(n-1)*$dx)\nderiv_pow(f,x,y,dx,dy) = :($f*($dy/$x*$dx + $dy*log($x)))\n\nderiv_rules = Dict(\n    :+   =&gt; (f,x,y,dx,dy) -&gt; :($dx+$dy),\n    :*   =&gt; (f,x,y,dx,dy) -&gt; :($dx*$y + $x*$dy),\n    :/   =&gt; (f,x,y,dx,dy) -&gt; :(($dx - $f*$dy)/$y),\n    :-   =&gt; deriv_minus,\n    :^   =&gt; deriv_pow,\n    :log =&gt; (f,x,dx)    -&gt; :($dx/$x),\n    :exp =&gt; (f,x,dx)    -&gt; :($f*$dx)\n)\n\nNow we are in a position to write code to simultaneously evaluate the expression and the derivative (with respect to some symbol s). To do this, it is helpful to give a (locally defined) name to each subexpression and its derivative, and to produce code that is a sequence of assignments to those names. We accumulate those assignments into a list (elist). An internal process function with different methods for different types of objects is used to generate the assignments for the subexpression values and the associated derivatives.\n\nfunction derivative(s :: Symbol, e :: Expr)\n\n    # Convert input to SSA form, set up generator\n    sym, elist = to_ssa(e)\n    eresult, emit! = ssa_generator()\n    \n    # Derivatives of leaves, init with ds/ds = 1.\n    deriv_table = Dict{Symbol,Any}()\n    deriv_table[s] = 1\n    deriv(e :: Symbol) = get(deriv_table, e, 0)\n    deriv(e) = 0\n\n    # Rules to generate differentiation code\n    function derivcode(s, e :: Expr)\n        if e.head == :call\n            rule = deriv_rules[e.args[1]]\n            dexpr = rule(s, e.args[2:end]...,\n                         deriv.(e.args[2:end])...)\n            emit!(to_ssa(dexpr)...)\n        elseif e.head == :tuple\n            emit!(Expr(:tuple, deriv.(e.args)...))\n        else\n            error(\"Unexpected expression of type $(e.head)\")\n        end\n    end\n    \n    derivcode(s, e) = emit!(gensym(), deriv(e))\n    \n    # Produce code for results and derivatives\n    for (s,e) in elist\n        emit!(s,e)\n        deriv_table[s] = derivcode(s, e)\n    end\n\n    # Add a tuple for return at the end (function + deriv)\n    emit!(Expr(:tuple, sym, deriv_table[sym])), eresult\nend\n\nAs an example, consider computing the derivative of \\(mx + b\\) with respect to \\(x\\)\n\nfrom_ssa(derivative(:x, :(m*x+b))...)\n\nquote\n    var\"##233\" = m * x\n    var\"##235\" = 0 * x\n    var\"##236\" = m * 1\n    var\"##237\" = var\"##235\" + var\"##236\"\n    var\"##234\" = var\"##233\" + b\n    var\"##238\" = var\"##237\" + 0\n    var\"##239\" = (var\"##234\", var\"##238\")\n    var\"##239\"\nend\n\n\nGiving more comprehensible variable names, this is equivalent to\ny1 = m * x\ndy1a = 0 * x\ndy1b = m * 1\ndy1 = dy1a + dy1b\ny2 = y1 + b\ndy2 = dy1 + 0\nresult = (y2, dy2)\nresult\nThis is correct, but it is also a very complicated-looking way to compute \\(m\\)! We therefore consider putting in some standard simplifications.\n\n\n11.3.4 Simplification\nThere are many possible algebraic relationships that we can use to simplify derivatives. Some of these are relationships, like multiplication by zero, are things that we believe are safe but the compiler cannot safely do on our behalf1. Others are things that the compiler could probably do on our behalf, but we might want to do on our own to understand how these things work.\nWe will again use the matching rules on multiple dispatch to write our simplification rules as different methods under a common name. For example, the simplification rules for \\(x + y\\) (where the prior expression is saved as \\(e\\)) are:\n\n# Rules to simplify e = x+y\nsimplify_add(e, x :: Number, y :: Number) = x+y\nsimplify_add(e, x :: Number, y :: Symbol) = x == 0 ? y : e\nsimplify_add(e, x :: Symbol, y :: Number) = y == 0 ? x : e\nsimplify_add(e, x, y) = e\n\nThe rules for minus (unary and binary) are similar in flavor. We will give the unary and binary versions both the same name, and distinguish between the two cases based on the number of arguments that we see.\n\n# Rules to simplify e = -x\nsimplify_sub(e, x :: Number) = -x\nsimplify_sub(e, x) = e\n\n# Rules to simplify e = x-y\nsimplify_sub(e, x :: Number, y :: Symbol) = x == 0 ? :(-$y) : e\nsimplify_sub(e, x :: Symbol, y :: Number) = y == 0 ? y : e\nsimplify_sub(e, x, y) = e\n\nWith multiplication, we will use the ordinary observation that anything times zero should be zero, ignoring the peculiarities of floating point.\n\n# Rules to simplify e = x*y\nsimplify_mul(e, x :: Number, y :: Number) = x*y\nsimplify_mul(e, x :: Number, y :: Symbol) =\n    if     x == 0  0\n    elseif x == 1  y\n    else           e\n    end\nsimplify_mul(e, x :: Symbol, y :: Number) =\n    if     y == 0  0\n    elseif y == 1  x\n    else           e\n    end\nsimplify_mul(e, x, y) = e\n\nFinally, we define rules for simplifying quotients and powers.\n\n# Rules to simplify e = x/y\nsimplify_div(e, x :: Number, y :: Number) = x/y\nsimplify_div(e, x :: Symbol, y :: Number) = y == 1 ? x : e\nsimplify_div(e, x :: Number, y :: Symbol) = x == 0 ? 0 : e\nsimplify_div(e, x, y) = e\n\n# Simplify powers\nsimplify_pow(e, x :: Symbol, n :: Number) =\n    if     n == 1  x\n    elseif n == 0  1\n    else           e\n    end\nsimplify_pow(e, x, n) = e\n\nTo simplify a line in an SSA assignment, we look up the appropriate rule function in a table (as we did with differentiation rules) and apply it.\n\nsimplify_rules = Dict(\n    :+ =&gt; simplify_add,\n    :- =&gt; simplify_sub,\n    :* =&gt; simplify_mul,\n    :/ =&gt; simplify_div,\n    :^ =&gt; simplify_pow)\n\nsimplify_null(e, args...) = e\nsimplify(e) = e\nsimplify(e :: Expr) =\n    if e.head == :call\n        rule = get(simplify_rules, e.args[1], simplify_null)\n        rule(e, e.args[2:end]...)\n    else\n        e\n    end\n\nsimplify (generic function with 2 methods)\n\n\nPutting everything together, our rule for simplifying code in SSA form is:\n\nLook up whether a previous simplification replaced any arguments with constants or with other symbols. If a replacement was also replaced earlier, reply the rule recursively.\nApply the appropriate simplification rule.\nIf the expression now looks like (symbol, x) where x is a leaf (a constant or another symbol), make a record in the lookup table to replace future references to symbol with references to x.\n\nThe following code carries out this task.\n\nfunction simplify_ssa(sym, elist)\n\n    # Set up and add to replacement table\n    replacements = Dict{Symbol,Any}()\n    function replacement(s,e)\n        replacements[s] = e\n    end\n\n    # Apply a replacement policy\n    replace(e) = e\n    replace(s :: Symbol) =\n        if s in keys(replacements)\n            replace(replacements[s])\n        else\n            s\n        end\n\n    # Simplify each term\n    eresult, emit! = ssa_generator()\n    for (s,e) in elist\n        e = e isa Expr ? Expr(e.head, replace.(e.args)...) : e\n        enew = simplify(e)\n        if enew isa Number || enew isa Symbol\n            replacement(s, enew)\n        end\n        emit!(s, enew)\n    end\n    replace(sym), eresult\nend\n\nFinally, we do dead code elimination, keeping only “live” computations on which the final result depends. Liveness is computed recursively: the final result is live, and anything that a live variable depends on is live. Because in SSA each expression depends only on previous results, we can compute liveness by traversing the list from back to front and updating what we know to be live as we go.\n\nfunction prune_ssa(sym, elist)\n\n    # The end symbol is live\n    live = Set([sym])\n\n    # Propogate liveness\n    mark_live(s :: Symbol) = push!(live, s)\n    mark_live(e :: Expr) = mark_live.(e.args)\n    mark_live(e) = nothing\n    \n    # If the RHS is live, so is anything it depends on\n    for (s,e) in reverse(elist)\n        if s in live\n            mark_live(e)\n        end\n    end\n\n    # Return only the live expressions\n    sym, filter!(p -&gt; p[1] in live, elist)\nend\n\nRepeating our experiment with differentiating \\(mx + b\\) but including simplification and pruning, we get a much terser generated code.\n\nfrom_ssa(\n    prune_ssa(\n        simplify_ssa(\n            derivative(:x, :(m*x+b))...)...)...)\n\nquote\n    var\"##240\" = m * x\n    var\"##241\" = var\"##240\" + b\n    var\"##246\" = (var\"##241\", m)\n    var\"##246\"\nend\n\n\n\n\n11.3.5 The macro\nFinally, we package the derivatives into a macro. The macro gets the derivative information from the derivative function and packages it into a block that at the end returns the final expression value and derivative as a tuple. We note that the namespace for macros is different from the namespace for functions, so it is fine to use the same name (derivative) for both our main function and for the macro that calls it. Note that we escape the full output in this case – we want to be able to use the external names, and we only write to unique local symbols.\n\nmacro derivative(s :: Symbol, e :: Expr)\n    sym, elist = derivative(s, e)\n    sym, elist = simplify_ssa(sym, elist)\n    sym, elist = prune_ssa(sym, elist)\n    esc(from_ssa(sym, elist))\nend\n\nWe will test out the macro on a moderately messy expression that is slightly tedious to compute by hand, and compare our result to the (slightly tedious) hand computation.\n\nlet\n    # An expression and a hand-computed derivative\n    f(x) = -log(x^2 + 2*exp(x) + (x+1)/x)\n    df(x) =\n        -(2*x + 2*exp(x) -1/x^2)/\n        (x^2 + 2*exp(x) + (x+1)/x)\n\n    # Autodiff of the same expression\n    g(x) = @derivative(x, -log(x^2 + 2*exp(x) + (x+1)/x))\n\n    # Check that the results agree up to roundoff effects\n    xtest = 2.3\n    gx, dgx = g(xtest)\n    gx ≈ f(xtest) && dgx ≈ df(xtest)\nend\n\ntrue",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/04-AutoDiff.html#footnotes",
    "href": "01-Fund1d/04-AutoDiff.html#footnotes",
    "title": "11  Automatic Differentiation",
    "section": "",
    "text": "In IEEE floating point, 0 * Inf is a NaN, not a zero. The compiler does not know that we don’t expect infinities in the code, and so cannot safely eliminate products with 0.↩︎",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/05-NumDiff.html",
    "href": "01-Fund1d/05-NumDiff.html",
    "title": "12  Numerical Differentiation",
    "section": "",
    "text": "Finite differencing (real and complex)\nDifferentiation via models\nDifferentiation matrices",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Numerical Differentiation</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/06-Quadrature.html",
    "href": "01-Fund1d/06-Quadrature.html",
    "title": "13  Quadrature",
    "section": "",
    "text": "Numerical indefinite and definite integration\nLow-dimensional quadrature rules\nGaussian quadrature (Gauss-Legendre, Gauss-Hermite, etc)\nSpecialized quadrature, transforms, etc\nMatrices, moments, and quadrature\nSome example statistical computations (being Bayesian, for example)\nBayesian quadrature (a pointer ahead)\nMonte Carlo quadrature (a pointer ahead)",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quadrature</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/07-Roots-Opt.html",
    "href": "01-Fund1d/07-Roots-Opt.html",
    "title": "14  Root Finding and Optimization",
    "section": "",
    "text": "Global and local search Hard and easy problems, initial guesses, bounding intervals Bisection Fixed point iteration and analysis Newton’s method and analysis Beyond Newton’s method – polynomial proxies, etc (fwd pointer) Secant method Safeguards Comments re multiple parameters, implicit differentiation (look ahead) Connections: parameter estimation",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Root Finding and Optimization</span>"
    ]
  },
  {
    "objectID": "01-Fund1d/08-Random.html",
    "href": "01-Fund1d/08-Random.html",
    "title": "15  Computing with Randomness",
    "section": "",
    "text": "NB: Maybe this is just section 4\nRandomized methods - Monte Carlo and Las Vegas Variational inference (??) Probabilistic numerics (including BQ) Uncertainty quantification",
    "crumbs": [
      "Fundamentals in 1D",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Computing with Randomness</span>"
    ]
  },
  {
    "objectID": "02-NLA/01-Systems.html",
    "href": "02-NLA/01-Systems.html",
    "title": "16  Linear Systems",
    "section": "",
    "text": "Many applications of linear systems (including optimization)\nThe usual intro to LU Role of pivoting Cholesky (+ other areas where pivoting is not needed) Block factorization, role of level 3 BLAS\nGeneral meaning of Schur complements SMW and auxiliary variable formulations Schur complements, covariance, precision, and conditioning\nCondition estimation\nIterative refinement\nSparsity and dependencies Data sparse matrices Direct sparse solver ideas When do direct sparse solvers make sense? Fast and superfast solvers (fwd ref: signals)\nExamples with various Markov chain computations, PageRank?",
    "crumbs": [
      "Numerical Linear Algebra",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Linear Systems</span>"
    ]
  },
  {
    "objectID": "02-NLA/02-Least-Squares.html",
    "href": "02-NLA/02-Least-Squares.html",
    "title": "17  Least Squares",
    "section": "",
    "text": "Underdetermined and overdetermined systems Standard approaches (QR, NE, SVD) Augmented system formulation Alternate norms, continuous least squares and associated factorizations Regularization (Tikhonov, truncated SVD, implicit regularization via iteration) Sketching Leverage scores Sparsity vs incoherence Bayesian least squares Cross-validation, GCV, etc",
    "crumbs": [
      "Numerical Linear Algebra",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "02-NLA/03-Eigenvalues.html",
    "href": "02-NLA/03-Eigenvalues.html",
    "title": "18  Eigenvalue Problems and the SVD",
    "section": "",
    "text": "Reminder of the canonical forms; difference between the SEP, NEP, SVD Reminder: application of eigenvalue problems. What is actually needed? Power method (+ PageRank as an example)\nSubspace iteration Randomized range finder\nFrom subspace iteration to QR Hessenberg matrices (+ control?)\nSymmetric eigenvalue problem and Rayleigh quotients Tridiagonalization",
    "crumbs": [
      "Numerical Linear Algebra",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Eigenvalue Problems and the SVD</span>"
    ]
  },
  {
    "objectID": "02-NLA/04-Signals.html",
    "href": "02-NLA/04-Signals.html",
    "title": "19  Signals and Transforms",
    "section": "",
    "text": "Linear ODEs, linear difference equations, etc Convolutions for differential and difference equations Convolutions in probability Auto-regressive models Fast Fourier transforms Toeplitz and Hankel matrices Displacement rank and superfast solvers Chebyshev-Fourier connection",
    "crumbs": [
      "Numerical Linear Algebra",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Signals and Transforms</span>"
    ]
  },
  {
    "objectID": "02-NLA/05-Stationary.html",
    "href": "02-NLA/05-Stationary.html",
    "title": "20  Stationary Iterations",
    "section": "",
    "text": "Standard splittings and such PageRank, Markov chains, etc Stochastic iterations? (look ahead) Regularization via iteration",
    "crumbs": [
      "Numerical Linear Algebra",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Stationary Iterations</span>"
    ]
  },
  {
    "objectID": "03-Nonlinear/01-Calculus.html",
    "href": "03-Nonlinear/01-Calculus.html",
    "title": "22  Calculus Revisited",
    "section": "",
    "text": "Finite differences and sparsity Use and abuse of automatic differentiation Backpropagation and neural networks (SIREV, Neidinger 2010)",
    "crumbs": [
      "Nonlinear Equations and Optimization",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Calculus Revisited</span>"
    ]
  },
  {
    "objectID": "03-Nonlinear/02-Nonlinear.html",
    "href": "03-Nonlinear/02-Nonlinear.html",
    "title": "23  Nonlinear Equations and Unconstrained Optimization",
    "section": "",
    "text": "Global vs local search Zeroth order methods",
    "crumbs": [
      "Nonlinear Equations and Optimization",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Nonlinear Equations and Unconstrained Optimization</span>"
    ]
  },
  {
    "objectID": "03-Nonlinear/04-Constrained.html",
    "href": "03-Nonlinear/04-Constrained.html",
    "title": "25  Constrained Optimization",
    "section": "",
    "text": "Equality vs inequality constraints Simple non-negativity constraints Penalties and barriers Projected gradients and SPG Interior point methods Augmented Lagrangian methods",
    "crumbs": [
      "Nonlinear Equations and Optimization",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Constrained Optimization</span>"
    ]
  },
  {
    "objectID": "05-Reduction/01-Matrix.html",
    "href": "05-Reduction/01-Matrix.html",
    "title": "31  Latent Factors and Matrix Factorization",
    "section": "",
    "text": "Matrices as data vs matrices in linear algebra - stacked/multimodal vs multiway data SVD, PCA, etc ID and CUR NMF and parts decompositions",
    "crumbs": [
      "Dimension Reduction and Latent Factor Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Latent Factors and Matrix Factorization</span>"
    ]
  },
  {
    "objectID": "05-Reduction/02-Tensor.html",
    "href": "05-Reduction/02-Tensor.html",
    "title": "32  From Matrices to Tensors",
    "section": "",
    "text": "Basic vocabulary Why are tensors harder CP Tucker Tensor train",
    "crumbs": [
      "Dimension Reduction and Latent Factor Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>From Matrices to Tensors</span>"
    ]
  },
  {
    "objectID": "06-Approximation/01-Concepts.html",
    "href": "06-Approximation/01-Concepts.html",
    "title": "34  Fundamental Concepts",
    "section": "",
    "text": "Space of functions / parameterization (NB: “nonparametric”) Consistency, stability, bias/variance decomposition, curse of dimensionality Interpolation or other fitting process, stability in map from data to fns (noise and numerics)",
    "crumbs": [
      "Function Approximation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Fundamental Concepts</span>"
    ]
  },
  {
    "objectID": "06-Approximation/02-Low-Dimension.html",
    "href": "06-Approximation/02-Low-Dimension.html",
    "title": "35  Low-Dimensional Structure",
    "section": "",
    "text": "On input: active subspace, sloppy models, random embed for optimization On output: POD, DEIM, etc; approximation from a subspace",
    "crumbs": [
      "Function Approximation",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Low-Dimensional Structure</span>"
    ]
  },
  {
    "objectID": "06-Approximation/03-Kernels.html",
    "href": "06-Approximation/03-Kernels.html",
    "title": "36  Kernels and RBFs",
    "section": "",
    "text": "Interpretations Error analysis Computing for modest n Computing for large n",
    "crumbs": [
      "Function Approximation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Kernels and RBFs</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Axler, Sheldon. 2024. Linear Algebra Done Right. Fourth.\nSpringer.\n\n\nBailey, David H. 1991. “Twelve Ways to Fool the Masses When Giving\nPerformance Results on Parallel Comptuers.” Supercomputing\nReview, August. https://www.davidhbailey.com/dhbpapers/twelve-ways.pdf.\n\n\nBailey, David H., Robert F. Lucas, and Samuel Williams, eds. 2010.\nPerformance Tuning of Scientific Applications. CRC Press. https://doi.org/10.1201/b10509.\n\n\nBillingsley, Patrick. 1995. Probability and Measure. Third.\nWiley Series in Probability and Mathematical Statistics. Wiley.\n\n\nBreiman, Leo. 1992. Probability. SIAM. https://doi.org/10.1137/1.9781611971286.\n\n\nChung, Kai Lai, and Farid AitSahlia. 2003. Elementary Probability\nTheory. Undergraduate Texts in Mathematics. Springer. https://doi.org/10.1007/978-0-387-21548-8.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020.\nMathematics for Machine Learning. Cambridge University Press.\nhttps://mml-book.com.\n\n\nGraham, Paul. 1993. On Lisp. Prentice Hall. https://paulgraham.com/onlisp.html.\n\n\nHennessey, John L., and David A. Patterson. 2017. Computer\nArchitecture: A Quantitative Approach. Sixth. Elsevier.\n\n\nHunt, Andy, and David Thomas. 1999. The Pragmatic Programmer: From\nJourneyman to Master. Addison Wesley.\n\n\nKernighan, Brian W., and Rob Pike. 1999. The Practice of\nProgramming. Addison Wesley.\n\n\nKernighan, Brian W., and P. J. Plauger. 1978. The Elements of\nProgramming Style. Second. McGraw-Hill.\n\n\nKeynes, John Maynard. 1929. A Tract on Monetary Reform. The\nMacmillan Company. https://archive.org/details/tractonmonetaryr0000keyn/mode/2up.\n\n\nKline, M. 1990. Mathematical Thought from Ancient to Modern\nTimes. Oxford University Press.\n\n\nKnuth, Donald E. 1974. “Structured Programming with go to Statements.” Computing\nSurveys 6 (4).\n\n\n———. 1984. “Literate Programming.” Comput. J. 27\n(2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLax, Peter D. 2007. Linear Algebra and Its Applications.\nSecond. Wiley.\n\n\nLay, David C., Steven R. Lay, and Judi J. McDonald. 2015. Linear\nAlgebra and Its Applications. Fifth. Pearson.\n\n\nLeonard, Mary J., Steven T. Kalinowski, and Tessa C. Andrews. 2014.\n“Misconceptions Yesterday, Today, and Tomorrow.” CBE –\nLife Sciences Education 13 (2). https://doi.org/10.1187/cbe.13-12-0244.\n\n\nMuller, Derek A. 2008. “Designing Effective Multimedia for Physics\nEducation.” PhD thesis, University of Sydney.\n\n\nRosenthal, Arthur. 1951. “The History of Calculus.” The\nAmerican Mathematical Monthly 58 (2): 75–86. https://doi.org/10.2307.2308368.\n\n\nRoss, Sheldon. 2014. A First Course in Probability. Ninth.\nPearson.\n\n\nSadler, Philip M., Gerhard Sonnert, Harold P. Coyle, Nancy Cook-Smith,\nand Jaimie L. Miller. 2013. “The Influence of Teachers’ Knowledge\non Student Learning in Middle School Physical Science\nClassrooms.” American Educational Research Journal 50\n(5). https://doi.org/10.3102/0002831213477680.\n\n\nStrang, Gilbert. 2023. Introduction to Linear Algebra. Sixth.\nWellesley-Cambridge Press.\n\n\nVuduc, Richard, Aparna Chandramowlishwaran, Jee Choi, Kenneth\nCzechowski, Marat Guney, Logan Moon, and Aashay Shringarpure. 2010.\n“On the Limits of GPU Acceleration.” In\nHotPar’10: Proceedings of the 2nd USENIX Conference on Hot Topics in\nParallelism. Berkeley, CA. https://doi.org/10.5555/1863086.1863099.\n\n\nWilf, Herbert S. 2006. Generatingfunctionology. Third. AK\nPeters, Ltd. https://www2.math.upenn.edu/~wilf/DownldGF.html.\n\n\nWilliams, Samuel, Andrew Waterman, and David Patterson. 2009.\n“Roofline: An Insightful Visual Performance Model for Multicore\nArchitectures.” Communications of the ACM 52 (4). https://doi.org/10.1145/1498765.1498785.",
    "crumbs": [
      "References"
    ]
  }
]